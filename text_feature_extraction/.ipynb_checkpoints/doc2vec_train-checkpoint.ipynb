{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cheesecake': 1, 'hummus': 13, 'chocolate_cake': 4, 'chocolate_mousse': 5, 'deviled_eggs': 6, 'fried_rice': 9, 'sushi': 23, 'nachos': 17, 'macarons': 16, 'pancakes': 19, 'dumplings': 7, 'tacos': 24, 'onion_rings': 18, 'frozen_yogurt': 10, 'garlic_bread': 11, 'chicken_wings': 3, 'macaroni_and_cheese': 15, 'lasagna': 14, 'pizza': 20, 'sashimi': 21, 'apple_pie': 0, 'hamburger': 12, 'french_toast': 8, 'chicken_curry': 2, 'steak': 22, 'waffles': 26, 'tiramisu': 25}\n",
      "number of recipes in train dataset:  264\n"
     ]
    }
   ],
   "source": [
    "number_of_crossvalidation_run = 1\n",
    "models_folder_name = os.path.join(os.getcwd(),'models','train', str(number_of_crossvalidation_run))\n",
    "summaries_folder_name = os.path.join(os.getcwd(),'summaries','train',str(number_of_crossvalidation_run))\n",
    "\n",
    "path_to_preprocessed_texts = os.path.join(models_folder_name, 'recipes_train_dataset.pkl')\n",
    "\n",
    "\n",
    "\n",
    "df_preprocessed_texts = pd.read_pickle(path_to_preprocessed_texts)\n",
    "preprocessed_texts = df_preprocessed_texts.preprocessed_texts.values.tolist()\n",
    "labels = df_preprocessed_texts['labels'].values.tolist()\n",
    "\n",
    "unique_labels=sorted(set(labels))\n",
    "number_categories=len(unique_labels)\n",
    "categories_indices=np.linspace(0,number_categories-1,number_categories,dtype=int)\n",
    "labels2integers=dict(zip(unique_labels,categories_indices))\n",
    "\n",
    "print(labels2integers)\n",
    "print(\"number of recipes in train dataset: \", df_preprocessed_texts.labels.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "generations = 200000\n",
    "model_learning_rate = 0.0005\n",
    "\n",
    "embedding_size = 27   #word embedding size\n",
    "doc_embedding_size = 27  #document embedding size\n",
    "concatenated_size = embedding_size + doc_embedding_size\n",
    "\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(preprocessed_texts):\n",
    "    words=[w for words_in_recipe in preprocessed_texts for w in words_in_recipe]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words))\n",
    "    count=sorted(count)\n",
    "    word_dict = {}\n",
    "    for word in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return (word_dict)\n",
    "\n",
    "#replace each word in texts with integer value\n",
    "def text_to_numbers(preprocessed_texts, word_dict):\n",
    "    data = []\n",
    "    for prepr_text in preprocessed_texts:\n",
    "        text_data = []\n",
    "        for word in prepr_text:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            text_data.append(word_ix)\n",
    "        data.append(text_data)\n",
    "    return (data)\n",
    "\n",
    "\n",
    "def create_batch_data(text_with_words_conv_to_numbers, batch_size=batch_size):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    rand_text_ix = int(np.random.choice(len(text_with_words_conv_to_numbers), size=1))\n",
    "    rand_text = text_with_words_conv_to_numbers[rand_text_ix]\n",
    "    word_to_predict_label=np.random.choice(list(set(rand_text)), size=1,replace=False)\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        item_in_batch=[]        \n",
    "        \n",
    "        label_words=np.random.choice(rand_text, size=1,replace=False)\n",
    "\n",
    "        item_in_batch.extend(word_to_predict_label)\n",
    "        item_in_batch.append(rand_text_ix)     \n",
    "        label_data.extend(label_words)\n",
    "        batch_data.append(item_in_batch)\n",
    "\n",
    "        \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array(label_data))\n",
    "\n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run': 340, 'oven': 271, 'enjoy': 145, 'tongs': 418, 'filet': 152, 'coat': 93, 'egg': 142, 'cumin': 121, 'instruction': 201, 'pressure': 314, 'food': 163, 'oat': 266, 'cardamom': 70, 'heart': 193, 'sandwich': 346, 'skillet': 367, 'chill': 82, 'moisture': 252, 'pot': 307, 'mustard': 257, 'ground': 186, 'block': 41, 'stuffed': 400, 'taco': 405, 'tahini': 406, 'strip': 399, 'boil': 44, 'fryer': 169, 'crispy': 117, 'layer': 211, 'season': 353, 'coffee': 98, 'dry': 139, 'mash': 235, 'chopstick': 86, 'jam': 205, 'ball': 18, 'bundt': 61, 'lemon': 214, 'bit': 36, 'scallion': 351, 'aquafaba': 7, 'drop': 138, 'oregano': 270, 'rice': 333, 'mango': 228, 'mat': 236, 'ladyfinger': 209, 'cooky': 106, 'pico': 292, 'coriander': 109, 'naan': 258, 'buffalo': 59, 'recipe': 330, 'seed': 356, 'cocoa': 95, 'spray': 382, 'wire': 449, 'milliliter': 247, 'foil': 162, 'sugar': 401, 'flip': 159, 'sprig': 384, 'gelatin': 175, 'clove': 92, 'seaweed': 355, 'masala': 233, 'bell': 33, 'basil': 23, 'smith': 371, 'mushroom': 256, 'face': 150, 'blackberry': 37, 'rest': 332, 'marinara': 231, 'appetizer': 5, 'sake': 342, 'salmon': 343, 'beaten': 29, 'packet': 272, 'cider': 87, 'batter': 25, 'speed': 378, 'garbanzo': 172, 'cinnamon': 89, 'curry': 123, 'iron': 202, 'blender': 40, 'herb': 194, 'cranberry': 115, 'yeast': 456, 'meat': 240, 'marinade': 230, 'floret': 160, 'bottom': 47, 'mac': 223, 'round': 338, 'leaf': 212, 'none': 261, 'saucepan': 349, 'cream': 116, 'soup': 374, 'coconut': 96, 'peanut': 287, 'base': 22, 'pizza': 300, 'form': 164, 'hamburger': 190, 'rare': 327, 'mozzarella': 254, 'sesame': 357, 'puree': 321, 'ramekin': 325, 'sea': 352, 'surface': 402, 'hummus': 197, 'frosting': 167, 'ice': 198, 'sirloin': 364, 'squash': 388, 'raspberry': 328, 'zucchini': 460, 'dente': 130, 'milk': 246, 'pesto': 291, 'towel': 425, 'everything': 147, 'meatball': 241, 'cling': 91, 'onion': 268, 'cherry': 78, 'area': 8, 'jalapeño': 204, 'well': 443, 'pepper': 290, 'breast': 52, 'turmeric': 430, 'paprika': 276, 'mascarpone': 234, 'dipping': 133, 'gouda': 178, 'mesh': 243, 'pecan': 288, 'preheat': 312, 'cut': 124, 'spring': 385, 'dunk': 141, 'grate': 183, 'middle': 245, 'tin': 414, 'dessert': 131, 'avocado': 11, 'sauce': 348, 'running': 341, 'lime': 217, 'lid': 216, 'spatula': 377, 'juice': 207, 'ranch': 326, 'spoonful': 381, 'coloring': 99, 'caramel': 69, 'cucumber': 120, 'flesh': 158, 'berry': 34, 'spicy': 379, 'hole': 195, 'panko': 274, 'rectangle': 331, 'popsicle': 303, 'seasoning': 354, 'cheesecake': 77, 'baguette': 14, 'macaroni': 225, 'salsa': 344, 'texture': 410, 'thigh': 412, 'toss': 422, 'pork': 304, 'baking': 17, 'topping': 419, 'dome': 134, 'brush': 57, 'crumb': 118, 'eggplant': 143, 'pancake': 273, 'whip': 444, 'nori': 263, 'ravioli': 329, 'loaf': 221, 'stone': 396, 'spread': 383, 'bamboo': 19, 'pumpkin': 320, 'camembert': 68, 'butter': 63, 'bag': 13, 'part': 279, 'macaron': 224, 'skin': 368, 'hazelnut': 192, 'roll': 337, 'puck': 317, 'grain': 180, 'daikon': 127, 'imitation': 199, 'mayonnaise': 238, 'sheet': 358, 'liner': 218, 'jack': 203, 'bakery': 16, 'slice': 370, 'crust': 119, 'amp': 3, 'breakfast': 51, 'pipe': 298, 'cornstarch': 112, 'garam': 171, 'inside': 200, 'shrimp': 361, 'wrapper': 455, 'pinch': 295, 'drain': 136, 'stock': 395, 'steam': 392, 'chickpea': 80, 'tofu': 416, 'leg': 213, 'nacho': 259, 'eye': 149, 'fish': 156, 'rack': 323, 'mousse': 253, 'pie': 293, 'sit': 365, 'muffin': 255, 'sashimi': 347, 'cracker': 114, 'pine': 296, 'garlic': 173, 'lettuce': 215, 'cool': 107, 'melt': 242, 'pudding': 318, 'granny': 181, 'roe': 336, 'freezer': 166, 'cilantro': 88, 'mixture': 251, 'circle': 90, 'fry': 168, 'wok': 450, 'paste': 281, 'vinegar': 435, 'sriracha': 389, 'syrup': 404, 'meal': 239, 'biscuit': 35, 'beer': 31, 'brownie': 56, 'dinner': 132, 'space': 376, 'extract': 148, 'confectioner': 102, 'nibble': 260, 'choice': 85, 'espresso': 146, 'combine': 100, 'border': 46, 'mix': 249, 'honey': 196, 'cheddar': 75, 'bread': 49, 'fill': 153, 'cake': 67, 'carrot': 71, 'powder': 310, 'spinach': 380, 'wedge': 442, 'broccoli': 53, 'pasta': 280, 'fusion': 170, 'piece': 294, 'press': 313, 'piping': 299, 'gyoza': 188, 'granola': 182, 'yolk': 458, 'veggie': 434, 'bean': 27, 'consistency': 103, 'springform': 386, 'corner': 111, 'steak': 391, 'sieve': 362, 'box': 48, 'puff': 319, 'curl': 122, 'dough': 135, 'sauté': 350, 'mini': 248, 'total': 423, 'tender': 408, 'skinless': 369, 'breadcrumb': 50, 'tray': 426, 'banana': 20, 'ring': 335, 'crab': 113, 'skewer': 366, 'worm': 453, 'patty': 284, 'arrange': 9, 'marshmallow': 232, 'bacon': 12, 'quantity': 322, 'buttercream': 64, 'work': 452, 'soy': 375, 'cheese': 76, 'poke': 302, 'peel': 289, 'corn': 110, 'sushi': 403, 'coating': 94, 'handful': 191, 'paper': 275, 'blueberry': 42, 'apple': 6, 'pod': 301, 'vegetable': 433, 'cone': 101, 'portion': 305, 'blend': 39, 'virgin': 436, 'tortilla': 421, 'almond': 1, 'sprinkle': 387, 'starch': 390, 'potato': 308, 'griddle': 184, 'pastry': 282, 'glaze': 177, 'truffle': 427, 'chicken': 79, 'brown': 55, 'air': 0, 'zip': 459, 'chocolate': 84, 'garnish': 174, 'peak': 286, 'shiitake': 360, 'finger': 155, 'jasmine': 206, 'freeze': 165, 'burger': 62, 'strainer': 397, 'pea': 285, 'elbow': 144, 'teriyaki': 409, 'thickness': 411, 'beet': 32, 'flake': 157, 'cooker': 104, 'kosher': 208, 'bubble': 58, 'toast': 415, 'nut': 264, 'cookie': 105, 'graham': 179, 'facing': 151, 'asparagus': 10, 'row': 339, 'beat': 28, 'worcestershire': 451, 'bake': 15, 'liquid': 220, 'amount': 2, 'wrap': 454, 'dripping': 137, 'lunch': 222, 'oil': 267, 'beef': 30, 'grill': 185, 'sodium': 373, 'wing': 448, 'cashew': 72, 'provolone': 316, 'macarons': 226, 'processor': 315, 'tuna': 428, 'wasabi': 439, 'cabbage': 65, 'metal': 244, 'mixer': 250, 'rainbow': 324, 'filling': 154, 'parmesan': 277, 'position': 306, 'boneless': 45, 'salt': 345, 'guacamole': 187, 'stir': 394, 'noodle': 262, 'dumpling': 140, 'decker': 129, 'bbq': 26, 'wine': 447, 'barbecue': 21, 'cayenne': 74, '½': 461, 'simmer': 363, 'pour': 309, 'baste': 24, 'chili': 81, 'boat': 43, 'dark': 128, 'mandu': 227, 'cylinder': 126, 'thumb': 413, 'flour': 161, 'orange': 269, 'water': 441, 'strawberry': 398, 'tomato': 417, 'bun': 60, 'chive': 83, 'stick': 393, 'cajun': 66, 'ham': 189, 'ginger': 176, 'yogurt': 457, 'cod': 97, 'core': 108, 'blade': 38, 'wash': 440, 'torch': 420, 'nutmeg': 265, 'parsley': 278, 'wafer': 437, 'pineapple': 297, 'tempura': 407, 'waffle': 438, 'pat': 283, 'liqueur': 219, 'whisk': 445, 'cauliflower': 73, 'maple': 229, 'broth': 54, 'ricotta': 334, 'white': 446, 'touch': 424, 'cutting': 125, 'upwards': 431, 'turkey': 429, 'lasagna': 210, 'prawn': 311, 'shell': 359, 'matchstick': 237, 'snack': 372, 'angle': 4, 'vanilla': 432}\n",
      "462\n"
     ]
    }
   ],
   "source": [
    "word_dictionary=build_dictionary(preprocessed_texts)\n",
    "vocabulary_size=len(word_dictionary)\n",
    "print(word_dictionary)\n",
    "print(vocabulary_size)\n",
    "\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 293, 131, 6, 310, 63, 401, 89, 345, 282, 142, 275, 6, 63, 401, 89, 265, 345, 358, 319, 282, 142, 142, 275, 289, 6, 441, 310, 310, 441, 251, 63, 401, 89, 265, 345, 445, 251, 6, 394, 282, 312, 282, 331, 6, 251, 331, 441, 331, 6, 331, 331, 154, 445, 142, 293, 358, 293, 15, 55, 331, 275, 331, 6, 293, 145], [6, 293, 131, 319, 282, 63, 401, 6, 89, 186, 358, 282, 63, 401, 6, 249, 319, 282, 370, 6, 63, 282, 401, 282, 249, 6, 282, 145], [69, 6, 293, 330, 131, 181, 371, 6, 207, 161, 401, 186, 89, 186, 293, 119, 142, 116, 55, 401, 63, 181, 371, 6, 207, 401, 186, 89, 186, 293, 119, 142, 116, 63, 345, 312, 271, 6, 370, 207, 161, 401, 89, 6, 293, 119, 293, 6, 370, 47, 119, 293, 119, 6, 370, 135, 119, 293, 142, 293, 119, 55, 282, 349, 116, 401, 401, 445, 63, 44, 69, 69, 348, 107, 293, 345, 370, 145], [104, 115, 6, 293, 330, 131, 115, 348, 6, 293, 154, 67, 249, 63, 432, 198, 116, 115, 6, 293, 67, 249, 63, 432, 198, 116, 115, 348, 6, 293, 104, 47, 67, 249, 104, 6, 115, 370, 63, 370, 249, 432, 198, 116, 116, 145], [6, 293, 330, 131, 161, 401, 345, 63, 198, 441, 181, 371, 6, 401, 214, 112, 214, 267, 345, 161, 401, 345, 63, 441, 181, 371, 6, 401, 207, 267, 345, 119, 161, 401, 345, 63, 161, 294, 198, 441, 249, 135, 135, 271, 154, 289, 108, 6, 370, 401, 214, 112, 207, 267, 345, 6, 402, 337, 135, 338, 293, 47, 135, 47, 293, 119, 6, 119, 119, 135, 119, 119, 267, 401, 89, 119, 15, 119, 55, 107, 370, 145], [6, 293, 49, 330, 131, 49, 142, 246, 432, 148, 89, 371, 6, 401, 112, 432, 198, 116, 69, 348, 370, 142, 246, 432, 89, 181, 371, 6, 401, 432, 198, 116, 69, 312, 49, 142, 246, 432, 89, 49, 394, 49, 246, 251, 49, 338, 6, 401, 112, 6, 49, 15, 107, 370, 432, 198, 116, 69, 348, 145], [6, 293, 330, 16, 432, 198, 116, 161, 345, 63, 198, 441, 181, 371, 6, 401, 161, 345, 89, 214, 142, 432, 198, 116, 345, 63, 441, 371, 6, 401, 345, 89, 265, 214, 142, 401, 161, 345, 249, 63, 161, 251, 441, 135, 441, 135, 135, 135, 402, 6, 108, 370, 6, 401, 161, 345, 89, 265, 207, 214, 249, 6, 271, 402, 293, 135, 337, 338, 135, 135, 6, 154, 251, 135, 135, 293, 142, 401, 293, 293, 119, 55, 282, 116, 145], [6, 293, 131, 179, 114, 401, 89, 63, 6, 207, 89, 175, 116, 186, 179, 114, 401, 89, 63, 6, 207, 310, 116, 89, 312, 271, 179, 114, 401, 89, 63, 179, 114, 251, 47, 15, 63, 179, 114, 119, 349, 6, 207, 89, 175, 175, 179, 114, 175, 251, 175, 116, 89, 145], [6, 293, 330, 131, 181, 371, 6, 401, 112, 89, 345, 293, 6, 6, 401, 345, 119, 289, 108, 6, 6, 401, 112, 89, 345, 275, 293, 119, 293, 119, 358, 6, 251, 119, 293, 119, 399, 119, 15, 116, 145], [6, 293, 140, 330, 16, 6, 401, 89, 63, 112, 319, 282, 142, 6, 87, 441, 401, 89, 63, 207, 6, 401, 89, 63, 112, 358, 282, 142, 6, 87, 441, 401, 89, 214, 207, 214, 289, 6, 6, 294, 108, 47, 108, 6, 294, 6, 6, 294, 401, 89, 112, 63, 249, 6, 294, 282, 402, 282, 358, 338, 348, 282, 358, 108, 6, 6, 294, 282, 142, 282, 6, 282, 140, 142, 140, 140, 15, 271, 282, 55, 154, 87, 348, 140, 441, 6, 87, 401, 89, 349, 44, 44, 348, 445, 63, 207, 348, 140, 145], [69, 6, 293, 330, 16, 6, 214, 401, 401, 89, 293, 135, 116, 6, 401, 55, 401, 89, 265, 293, 116, 6, 441, 207, 441, 6, 6, 108, 108, 401, 401, 89, 265, 394, 6, 394, 293, 135, 135, 6, 89, 401, 251, 6, 69, 348, 271, 6, 370, 293, 135, 370, 6, 370, 15, 15, 55, 271, 107, 349, 89, 401, 44, 116, 394, 69, 348, 6, 293, 145], [89, 337, 6, 293, 330, 16, 371, 6, 89, 337, 401, 89, 181, 371, 6, 337, 401, 89, 112, 312, 267, 289, 6, 370, 401, 89, 112, 6, 394, 89, 337, 161, 47, 89, 337, 119, 6, 337, 119, 15, 15, 293, 145], [330, 131, 63, 179, 114, 116, 76, 401, 432, 148, 63, 179, 114, 116, 76, 63, 325, 325, 179, 114, 118, 325, 63, 249, 119, 47, 116, 76, 401, 432, 154, 179, 114, 119, 77, 325, 82, 77, 145], [15, 78, 77, 330, 131, 179, 114, 118, 401, 63, 345, 116, 76, 432, 148, 116, 401, 310, 345, 179, 114, 401, 63, 345, 116, 401, 310, 345, 154, 179, 114, 118, 401, 345, 63, 386, 82, 116, 76, 28, 116, 401, 432, 345, 28, 310, 116, 251, 28, 119, 165, 154, 77, 145], [48, 56, 77, 330, 56, 249, 116, 76, 401, 432, 148, 441, 175, 116, 48, 56, 249, 432, 441, 116, 312, 271, 56, 249, 48, 309, 414, 15, 271, 77, 154, 116, 76, 401, 432, 148, 175, 441, 249, 77, 154, 56, 22, 82, 116, 145], [77, 330, 131, 249, 116, 76, 401, 432, 148, 246, 310, 116, 387, 48, 48, 76, 401, 432, 246, 175, 310, 116, 387, 312, 249, 48, 25, 386, 15, 116, 76, 401, 432, 249, 246, 445, 175, 310, 175, 251, 116, 251, 445, 77, 251, 82, 116, 387, 145], [78, 77, 330, 131, 131, 78, 77, 77, 78, 154, 28, 330, 401, 441, 78, 84, 63, 116, 76, 432, 116, 401, 445, 394, 441, 401, 78, 394, 148, 386, 78, 165, 63, 251, 386, 22, 165, 116, 76, 401, 432, 148, 116, 28, 77, 251, 119, 165, 78, 154, 77, 77, 25, 82, 145], [248, 77, 131, 35, 63, 116, 76, 116, 401, 142, 432, 148, 35, 63, 116, 401, 142, 432, 148, 401, 77, 312, 271, 13, 118, 309, 63, 35, 22, 426, 426, 35, 22, 15, 77, 154, 116, 76, 116, 445, 401, 142, 432, 426, 35, 22, 154, 15, 401, 77, 401, 401, 401, 145], [248, 15, 77, 131, 63, 246, 246, 116, 76, 401, 116, 179, 114, 63, 246, 246, 116, 76, 401, 179, 114, 118, 114, 63, 394, 246, 246, 251, 251, 251, 116, 76, 63, 401, 116, 249, 251, 394, 179, 114, 63, 414, 218, 119, 47, 218, 251, 165, 414, 218, 145], [248, 287, 63, 77, 114, 116, 76, 401, 287, 63, 116, 432, 114, 401, 287, 63, 116, 432, 142, 287, 63, 114, 47, 414, 28, 116, 76, 401, 287, 63, 116, 432, 142, 28, 251, 287, 63, 114, 15, 145], [260, 56, 131, 401, 63, 84, 142, 246, 84, 76, 401, 142, 432, 148, 246, 84, 84, 13, 246, 84, 260, 116, 76, 142, 13, 260, 271, 401, 63, 394, 84, 394, 142, 394, 13, 260, 251, 56, 426, 15, 77, 445, 116, 76, 401, 445, 142, 432, 148, 251, 56, 13, 260, 15, 145], [79, 123, 330, 132, 345, 186, 290, 186, 121, 186, 430, 109, 186, 70, 74, 45, 369, 79, 412, 457, 176, 267, 71, 268, 308, 417, 281, 417, 79, 54, 333, 88, 442, 441, 310, 345, 457, 246, 267, 63, 186, 121, 186, 109, 186, 70, 74, 45, 369, 79, 412, 457, 92, 176, 267, 71, 268, 308, 281, 417, 206, 333, 88, 442, 441, 310, 345, 457, 246, 267, 393, 63, 345, 290, 121, 109, 70, 74, 394, 79, 412, 457, 92, 173, 176, 79, 441, 310, 457, 246, 394, 394, 135, 135, 267, 267, 307, 79, 267, 307, 71, 268, 308, 92, 173, 394, 394, 417, 281, 417, 54, 394, 363, 79, 394, 363, 308, 79, 394, 457, 258, 135, 135, 135, 258, 135, 258, 63, 258, 79, 123, 333, 457, 88, 442, 145], [79, 123, 330, 132, 267, 79, 345, 290, 268, 71, 308, 441, 285, 281, 267, 79, 345, 268, 71, 308, 441, 281, 267, 79, 353, 345, 290, 79, 307, 267, 268, 71, 308, 79, 441, 44, 363, 216, 285, 123, 281, 333, 145], [79, 123, 132, 96, 267, 89, 393, 70, 301, 333, 441, 345, 267, 268, 176, 281, 186, 430, 186, 121, 186, 109, 74, 457, 45, 369, 79, 412, 345, 290, 441, 442, 88, 96, 267, 89, 393, 301, 333, 441, 345, 96, 267, 268, 176, 92, 417, 281, 186, 186, 186, 109, 74, 457, 45, 369, 79, 412, 345, 290, 441, 88, 333, 96, 267, 307, 89, 393, 70, 301, 333, 441, 345, 333, 333, 89, 393, 70, 301, 96, 267, 268, 268, 176, 173, 417, 281, 430, 121, 109, 74, 394, 457, 79, 412, 353, 345, 290, 79, 441, 216, 79, 123, 348, 79, 348, 333, 442, 88, 145], [79, 123, 330, 132, 45, 369, 79, 52, 310, 186, 121, 171, 233, 186, 176, 267, 81, 417, 441, 345, 290, 333, 109, 45, 369, 79, 81, 310, 121, 171, 233, 186, 176, 267, 81, 417, 441, 345, 290, 333, 109, 79, 353, 79, 310, 121, 171, 233, 176, 267, 307, 79, 81, 417, 441, 345, 290, 44, 363, 348, 333, 109, 145], [79, 123, 330, 132, 308, 441, 267, 123, 310, 45, 369, 79, 52, 268, 285, 33, 290, 71, 290, 345, 206, 333, 278, 308, 267, 123, 310, 45, 369, 79, 52, 268, 285, 33, 290, 71, 290, 345, 206, 333, 278, 308, 441, 308, 308, 308, 441, 307, 267, 123, 310, 79, 268, 307, 79, 285, 33, 290, 71, 290, 308, 441, 308, 307, 345, 290, 123, 310, 123, 44, 123, 206, 333, 278, 145], [63, 330, 52, 345, 290, 123, 310, 171, 233, 81, 310, 268, 92, 457, 96, 246, 63, 333, 109, 79, 52, 345, 290, 123, 310, 171, 233, 81, 310, 268, 92, 457, 246, 63, 333, 109, 216, 333, 109, 145], [96, 123, 330, 267, 45, 369, 79, 52, 345, 290, 54, 268, 173, 123, 281, 96, 246, 186, 430, 81, 310, 186, 109, 186, 176, 345, 290, 33, 290, 160, 160, 88, 267, 45, 369, 79, 52, 345, 290, 79, 268, 92, 123, 281, 246, 186, 81, 310, 186, 109, 186, 176, 345, 290, 33, 290, 160, 160, 88, 307, 267, 267, 52, 353, 345, 290, 79, 79, 79, 54, 268, 173, 307, 268, 123, 281, 394, 96, 246, 310, 109, 176, 345, 290, 394, 79, 33, 290, 394, 44, 216, 363, 123, 88, 145], [448, 5, 79, 448, 310, 345, 348, 448, 310, 348, 312, 252, 448, 275, 425, 394, 310, 345, 79, 17, 323, 55, 117, 323, 358, 275, 162, 137, 367, 59, 348, 79, 448, 348, 145], [196, 26, 79, 448, 330, 161, 81, 310, 345, 290, 310, 448, 26, 348, 196, 26, 81, 310, 345, 290, 310, 79, 448, 348, 196, 312, 161, 81, 310, 345, 290, 310, 448, 161, 448, 275, 358, 211, 15, 55, 26, 348, 196, 448, 348, 448, 358, 211, 348, 145], [196, 173, 448, 330, 5, 161, 310, 290, 345, 267, 63, 310, 375, 348, 196, 401, 357, 356, 161, 310, 290, 345, 267, 267, 63, 375, 348, 401, 357, 356, 161, 310, 290, 345, 79, 211, 251, 267, 79, 448, 267, 275, 425, 63, 348, 196, 401, 448, 348, 394, 448, 348, 357, 356, 145], [290, 448, 330, 5, 79, 448, 310, 345, 63, 207, 290, 448, 310, 345, 63, 207, 290, 345, 312, 448, 283, 139, 17, 310, 345, 448, 394, 17, 323, 382, 358, 448, 261, 150, 448, 448, 271, 348, 63, 207, 290, 345, 394, 448, 394, 145], [375, 79, 448, 330, 5, 79, 448, 345, 290, 390, 267, 375, 348, 401, 357, 356, 448, 345, 290, 390, 267, 401, 173, 357, 312, 267, 79, 448, 345, 290, 390, 448, 267, 448, 55, 275, 425, 448, 357, 145], [448, 330, 5, 79, 448, 310, 345, 375, 348, 401, 196, 357, 356, 448, 310, 345, 375, 348, 357, 356, 448, 283, 139, 17, 310, 345, 448, 394, 17, 323, 382, 358, 448, 261, 150, 448, 448, 271, 348, 375, 348, 394, 401, 196, 356, 394, 448, 394, 145], [117, 59, 448, 330, 5, 112, 448, 161, 310, 290, 310, 345, 290, 267, 59, 348, 112, 448, 283, 161, 310, 310, 290, 267, 59, 348, 112, 79, 448, 323, 112, 161, 310, 310, 345, 290, 267, 79, 448, 448, 267, 55, 79, 448, 323, 275, 425, 358, 79, 59, 348, 145], [194, 448, 330, 79, 448, 310, 345, 267, 173, 278, 345, 448, 310, 345, 267, 278, 345, 312, 252, 448, 275, 425, 394, 310, 345, 79, 17, 323, 55, 117, 323, 358, 275, 162, 137, 367, 267, 173, 394, 194, 448, 394, 348, 145], [196, 257, 448, 330, 5, 79, 448, 310, 345, 196, 257, 345, 448, 310, 345, 196, 257, 345, 290, 312, 252, 448, 275, 425, 394, 310, 345, 79, 17, 323, 55, 117, 323, 358, 275, 162, 137, 367, 196, 257, 348, 394, 79, 448, 348, 145], [345, 448, 5, 79, 448, 267, 112, 290, 448, 267, 290, 271, 358, 275, 79, 448, 267, 390, 290, 345, 448, 358, 15, 117, 145], [448, 79, 448, 310, 345, 375, 348, 196, 401, 357, 356, 448, 310, 345, 375, 348, 401, 357, 356, 252, 448, 275, 425, 394, 310, 345, 79, 17, 323, 55, 117, 323, 358, 275, 162, 137, 367, 375, 348, 196, 401, 356, 348, 394, 79, 448, 348, 145], [66, 26, 448, 330, 5, 79, 448, 310, 345, 26, 348, 66, 448, 310, 345, 26, 348, 66, 312, 448, 283, 139, 17, 310, 345, 448, 394, 17, 323, 382, 358, 448, 261, 150, 448, 448, 271, 348, 26, 348, 66, 448, 394, 448, 145], [194, 448, 330, 5, 79, 448, 310, 345, 63, 194, 39, 278, 448, 310, 345, 63, 173, 194, 39, 278, 312, 448, 283, 139, 17, 310, 345, 448, 394, 17, 323, 382, 358, 448, 261, 150, 448, 448, 271, 348, 63, 394, 173, 394, 194, 39, 345, 278, 448, 394, 145], [196, 448, 330, 448, 112, 345, 310, 290, 207, 196, 26, 79, 448, 112, 345, 310, 290, 348, 207, 196, 283, 79, 112, 310, 345, 79, 211, 358, 323, 207, 196, 345, 312, 448, 448, 448, 348, 145], [84, 330, 131, 401, 95, 310, 310, 246, 267, 432, 148, 84, 192, 383, 310, 310, 267, 432, 84, 192, 383, 84, 192, 383, 84, 192, 383, 25, 84, 192, 383, 401, 145], [453, 302, 48, 67, 330, 131, 67, 249, 267, 142, 249, 246, 116, 105, 453, 84, 48, 84, 67, 249, 267, 142, 249, 246, 116, 106, 453, 271, 84, 67, 249, 267, 142, 445, 25, 249, 246, 445, 116, 106, 67, 195, 67, 453, 67, 84, 116, 105, 251, 67, 67, 453, 106, 145], [84, 287, 63, 330, 131, 67, 249, 116, 76, 287, 63, 246, 246, 419, 287, 63, 48, 84, 67, 249, 76, 287, 63, 246, 246, 84, 67, 25, 15, 116, 76, 287, 63, 246, 246, 195, 67, 195, 287, 63, 251, 67, 67, 251, 251, 67, 419, 67, 67, 63, 145], [84, 67, 330, 84, 67, 84, 167, 84, 69, 84, 84, 302, 195, 67, 195, 67, 195, 246, 84, 67, 84, 167, 69, 67], [106, 116, 302, 67, 330, 84, 67, 106, 116, 318, 249, 246, 84, 106, 246, 272, 116, 246, 84, 106, 246, 302, 195, 67, 195, 272, 106, 116, 318, 249, 246, 445, 419, 84, 106, 394, 246, 67, 195, 251, 318, 251, 251, 106, 67], [106, 61, 330, 131, 116, 76, 401, 432, 148, 67, 249, 84, 105, 84, 116, 116, 76, 432, 148, 48, 84, 67, 249, 201, 106, 84, 95, 116, 271, 445, 116, 76, 401, 432, 148, 67, 25, 61, 67, 116, 116, 251, 67, 25, 67, 84, 105, 116, 76, 67, 25, 106, 67, 177, 84, 116, 177, 177, 67, 177, 67], [84, 269, 67, 330, 131, 84, 63, 142, 269, 142, 401, 116, 269, 148, 84, 63, 142, 142, 401, 116, 269, 84, 63, 394, 84, 142, 269, 84, 251, 394, 142, 401, 142, 251, 84, 251, 142, 251, 142, 394, 15, 271, 67, 269, 116, 116, 269, 148, 401, 116, 269, 145], [84, 48, 67, 330, 131, 67, 249, 116, 84, 95, 310, 48, 84, 67, 249, 116, 84, 95, 310, 271, 67, 249, 201, 25, 61, 15, 67, 116, 84, 116, 249, 84, 61, 67, 84, 195, 67, 67, 95, 310, 145], [84, 84, 330, 131, 441, 128, 84, 198, 116, 84, 441, 198, 128, 84, 198, 116, 84, 84, 441, 198, 441, 198, 441, 84, 251, 198, 84, 251, 116, 116, 84, 145], [84, 253, 330, 131, 116, 401, 84, 116, 105, 116, 401, 84, 116, 106, 116, 401, 250, 286, 164, 116, 84, 116, 251, 116, 84, 116, 251, 116, 84, 105, 145], [106, 116, 253, 330, 131, 84, 346, 105, 116, 84, 346, 106, 116, 401, 116, 84, 346, 106, 106, 13, 116, 106, 116, 84, 346, 106, 116, 250, 116, 106, 401, 28, 286, 164, 84, 346, 105, 13, 13, 84, 346, 105, 84, 346, 105, 145], [84, 96, 116, 330, 131, 96, 345, 404, 96, 116, 95, 310, 96, 246, 404, 345, 96, 246, 404, 432, 148, 96, 157, 96, 345, 96, 116, 95, 310, 96, 246, 345, 96, 246, 432, 148, 96, 157, 96, 157, 345, 96, 404, 251, 96, 116, 95, 310, 96, 246, 404, 345, 251, 96, 116, 96, 246, 250, 96, 246, 148, 404, 28, 96, 157, 145], [84, 253, 330, 131, 401, 432, 148, 84, 398, 7, 401, 432, 148, 128, 84, 398, 7, 250, 401, 286, 164, 432, 286, 164, 84, 253, 398, 145], [84, 253, 330, 131, 84, 96, 116, 96, 246, 432, 84, 96, 116, 96, 246, 432, 148, 84, 84, 96, 116, 96, 246, 432, 84, 145], [84, 253, 330, 131, 84, 346, 105, 175, 441, 401, 84, 310, 116, 432, 148, 116, 84, 346, 106, 175, 441, 401, 128, 84, 95, 310, 116, 432, 148, 116, 106, 13, 175, 441, 175, 401, 95, 116, 432, 250, 28, 251, 28, 106, 253, 253, 116, 286, 164, 401, 286, 164, 116, 253, 95, 310, 145], [84, 253, 330, 131, 441, 84, 401, 432, 441, 7, 128, 84, 401, 432, 28, 7, 286, 164, 84, 401, 432, 253, 398, 145], [11, 142, 330, 5, 142, 11, 457, 257, 207, 345, 290, 142, 11, 457, 257, 207, 345, 290, 124, 142, 458, 458, 446, 11, 457, 257, 207, 345, 290, 142, 458, 299, 13, 13, 111, 142, 446, 276, 145], [66, 142, 330, 5, 142, 238, 257, 66, 290, 348, 268, 142, 238, 257, 66, 348, 268, 142, 307, 153, 441, 142, 307, 44, 365, 142, 198, 441, 142, 458, 142, 446, 238, 257, 66, 290, 348, 458, 299, 13, 459, 13, 111, 124, 251, 142, 276, 268, 145], [142, 330, 142, 238, 257, 345, 290, 278, 142, 238, 257, 345, 290, 276, 278, 142, 307, 153, 441, 142, 307, 44, 365, 142, 198, 441, 142, 458, 142, 446, 238, 257, 345, 290, 458, 299, 13, 459, 13, 111, 124, 251, 142, 446, 276, 278, 145], [142, 330, 5, 142, 11, 207, 345, 290, 417, 268, 88, 204, 421, 142, 207, 345, 290, 417, 268, 204, 142, 307, 441, 44, 365, 441, 198, 441, 142, 458, 446, 458, 251, 142, 446, 421, 88, 145], [142, 330, 5, 142, 11, 88, 204, 268, 417, 207, 345, 421, 88, 142, 11, 88, 417, 207, 345, 88, 142, 307, 153, 441, 142, 307, 44, 365, 142, 198, 441, 142, 458, 142, 446, 11, 88, 204, 268, 417, 207, 345, 458, 299, 13, 459, 13, 111, 124, 251, 142, 88, 421, 145], [142, 330, 5, 142, 257, 238, 345, 290, 83, 238, 345, 290, 83, 142, 458, 257, 238, 345, 290, 458, 299, 13, 458, 251, 142, 446, 276, 145], [142, 330, 5, 142, 12, 83, 76, 345, 290, 12, 83, 142, 12, 83, 76, 345, 290, 12, 83, 142, 307, 153, 441, 142, 307, 44, 365, 142, 198, 441, 142, 458, 142, 446, 12, 83, 76, 345, 290, 458, 299, 13, 459, 13, 111, 124, 251, 142, 12, 83, 145], [255, 414, 142, 330, 5, 142, 457, 348, 345, 290, 278, 162, 142, 457, 348, 345, 290, 278, 142, 458, 142, 446, 255, 414, 458, 142, 255, 414, 414, 255, 414, 142, 162, 162, 142, 162, 441, 255, 414, 162, 142, 142, 446, 458, 162, 255, 414, 142, 458, 142, 446, 414, 142, 142, 458, 457, 348, 345, 290, 251, 299, 13, 458, 251, 142, 446, 276, 278, 145], [79, 140, 267, 79, 268, 71, 92, 345, 63, 161, 79, 116, 161, 310, 345, 116, 267, 268, 71, 92, 345, 63, 116, 310, 345, 116, 267, 79, 294, 71, 268, 394, 63, 161, 249, 116, 374, 161, 310, 345, 290, 116, 394, 251, 135, 116, 135, 338, 135, 251, 140, 374, 140, 140, 145], [227, 330, 227, 251, 416, 304, 351, 227, 455, 140, 186, 304, 186, 351, 268, 92, 176, 357, 267, 375, 348, 345, 441, 455, 348, 375, 348, 357, 267, 333, 447, 435, 290, 416, 275, 275, 441, 252, 416, 416, 294, 304, 351, 268, 176, 267, 375, 348, 345, 290, 249, 441, 245, 455, 155, 455, 441, 251, 455, 227, 455, 441, 227, 227, 441, 140, 348, 145], [374, 140, 330, 140, 455, 441, 441, 175, 310, 375, 348, 395, 186, 304, 268, 360, 256, 176, 267, 455, 247, 441, 175, 310, 375, 348, 186, 304, 268, 186, 256, 186, 294, 176, 92, 173, 357, 267, 342, 374, 441, 175, 310, 375, 348, 395, 394, 374, 154, 186, 304, 268, 256, 176, 267, 249, 381, 245, 455, 381, 374, 140, 140, 455, 294, 275, 140, 247, 441, 392, 145], [140, 304, 342, 345, 401, 173, 176, 375, 348, 267, 308, 390, 65, 83, 188, 368, 441, 308, 390, 267, 375, 348, 435, 267, 188, 348, 304, 345, 401, 92, 173, 176, 375, 348, 357, 267, 308, 390, 65, 83, 247, 441, 390, 357, 267, 375, 348, 435, 81, 267, 188, 348, 304, 342, 345, 401, 249, 173, 176, 375, 348, 267, 308, 390, 65, 381, 245, 188, 368, 441, 368, 155, 267, 188, 188, 216, 392, 216, 267, 145], [140, 330, 161, 345, 441, 65, 268, 176, 375, 348, 267, 186, 304, 290, 256, 71, 361, 375, 348, 333, 447, 435, 267, 290, 345, 441, 65, 268, 92, 176, 267, 186, 304, 290, 256, 71, 361, 375, 348, 333, 447, 435, 357, 267, 290, 161, 345, 441, 249, 135, 135, 294, 135, 294, 140, 135, 294, 294, 140, 455, 294, 275, 135, 65, 268, 173, 176, 375, 348, 267, 249, 304, 154, 186, 304, 345, 290, 65, 251, 394, 154, 256, 71, 65, 251, 394, 361, 154, 361, 65, 251, 394, 140, 455, 155, 455, 441, 455, 154, 155, 154, 455, 267, 140, 47, 140, 441, 216, 392, 140, 441, 140, 275, 252, 375, 348, 333, 435, 267, 290, 394, 140, 348, 145], [330, 65, 345, 186, 304, 360, 256, 351, 267, 176, 290, 338, 455, 267, 441, 348, 375, 348, 267, 333, 447, 65, 345, 186, 304, 360, 256, 351, 267, 290, 338, 455, 267, 441, 348, 375, 348, 357, 267, 333, 447, 435, 65, 345, 252, 65, 252, 65, 441, 65, 186, 304, 360, 351, 267, 176, 345, 290, 249, 245, 455, 155, 441, 455, 140, 455, 267, 140, 47, 441, 154, 140, 348, 145], [140, 361, 186, 304, 342, 345, 375, 348, 176, 83, 401, 455, 188, 368, 441, 267, 333, 435, 375, 348, 267, 186, 304, 345, 176, 92, 83, 401, 455, 247, 441, 357, 267, 267, 361, 186, 304, 342, 345, 375, 348, 176, 173, 401, 83, 249, 155, 455, 441, 455, 455, 381, 245, 455, 455, 455, 140, 140, 455, 154, 140, 267, 140, 47, 441, 216, 392, 216, 267, 348, 333, 435, 375, 348, 267, 140, 348, 145], [330, 186, 79, 268, 351, 88, 176, 81, 310, 267, 455, 65, 267, 417, 81, 176, 290, 345, 357, 356, 88, 186, 79, 268, 351, 81, 310, 63, 267, 140, 455, 65, 275, 267, 417, 290, 356, 88, 154, 186, 79, 268, 351, 88, 176, 310, 267, 249, 245, 455, 155, 441, 455, 155, 455, 441, 65, 392, 455, 348, 267, 417, 173, 176, 290, 345, 417, 417, 251, 357, 356, 88, 145], [415, 51, 49, 142, 63, 116, 84, 170, 370, 49, 142, 63, 116, 84, 118, 49, 370, 294, 142, 142, 118, 63, 116, 84, 145], [415, 330, 51, 142, 401, 432, 148, 345, 49, 63, 401, 186, 89, 186, 345, 404, 142, 432, 295, 345, 49, 63, 401, 186, 295, 186, 295, 345, 229, 404, 142, 401, 89, 295, 345, 445, 49, 142, 251, 49, 162, 63, 401, 89, 345, 162, 415, 162, 162, 404, 145], [334, 84, 415, 404, 49, 246, 334, 76, 84, 142, 246, 401, 398, 401, 370, 246, 334, 248, 84, 246, 295, 398, 401, 207, 142, 246, 401, 49, 370, 334, 76, 49, 370, 84, 84, 334, 142, 251, 367, 63, 49, 142, 404, 251, 404, 145], [415, 330, 51, 49, 63, 142, 246, 432, 89, 84, 232, 84, 404, 84, 170, 370, 63, 246, 432, 84, 248, 232, 84, 404, 248, 84, 445, 142, 89, 432, 246, 142, 440, 118, 141, 49, 142, 440, 118, 242, 63, 367, 49, 118, 49, 370, 84, 313, 248, 232, 84, 404, 145], [415, 330, 51, 49, 142, 432, 246, 89, 116, 76, 432, 148, 116, 398, 401, 102, 404, 170, 432, 246, 89, 116, 76, 432, 207, 398, 229, 404, 445, 142, 89, 432, 246, 142, 440, 249, 116, 76, 432, 401, 207, 116, 370, 49, 251, 313, 398, 370, 313, 141, 142, 440, 242, 63, 367, 49, 118, 49, 398, 102, 401, 404, 145], [415, 330, 51, 328, 37, 404, 328, 219, 116, 76, 49, 246, 142, 328, 219, 345, 63, 116, 404, 328, 37, 404, 328, 219, 76, 370, 49, 246, 142, 345, 116, 229, 404, 328, 37, 328, 219, 404, 34, 116, 76, 404, 370, 49, 34, 370, 49, 370, 49, 313, 294, 445, 246, 142, 328, 219, 345, 63, 367, 246, 251, 55, 116, 404, 116, 415, 229, 116, 34, 145], [42, 415, 43, 330, 51, 116, 76, 401, 89, 142, 116, 49, 42, 116, 76, 401, 89, 142, 116, 221, 49, 221, 42, 404, 116, 76, 401, 89, 142, 116, 445, 221, 49, 221, 49, 43, 49, 294, 49, 294, 42, 142, 251, 49, 43, 358, 275, 49, 142, 251, 55, 43, 162, 370, 404, 145], [42, 415, 330, 51, 49, 142, 246, 89, 432, 148, 42, 370, 49, 142, 89, 432, 42, 370, 49, 142, 246, 89, 432, 249, 49, 42, 49, 34, 142, 251, 142, 404, 145], [89, 415, 330, 51, 49, 63, 142, 246, 432, 89, 116, 76, 116, 401, 89, 432, 170, 63, 246, 432, 89, 116, 76, 116, 432, 445, 142, 89, 432, 246, 142, 440, 249, 116, 76, 116, 401, 89, 432, 141, 370, 49, 142, 440, 242, 63, 367, 49, 118, 49, 63, 89, 116, 251, 145], [415, 415, 142, 246, 63, 432, 89, 401, 345, 170, 221, 415, 142, 246, 63, 432, 89, 401, 345, 358, 415, 445, 142, 246, 63, 432, 89, 345, 401, 49, 251, 358, 358, 145], [34, 415, 330, 51, 49, 142, 246, 432, 89, 34, 401, 102, 170, 370, 49, 142, 246, 432, 89, 42, 328, 37, 207, 401, 102, 401, 445, 142, 89, 432, 246, 142, 440, 141, 49, 142, 440, 242, 63, 367, 49, 249, 34, 401, 207, 34, 370, 370, 102, 145], [415, 330, 51, 84, 232, 49, 142, 116, 63, 170, 49, 142, 116, 63, 370, 49, 232, 49, 142, 116, 251, 63, 55, 145], [12, 142, 333, 330, 132, 142, 12, 268, 267, 333, 345, 290, 401, 375, 348, 268, 170, 142, 268, 267, 333, 345, 290, 401, 268, 142, 345, 12, 268, 267, 450, 142, 142, 450, 12, 450, 12, 450, 142, 268, 450, 168, 333, 333, 168, 345, 290, 401, 348, 394, 333, 142, 12, 268, 145], [297, 333, 330, 297, 267, 142, 311, 173, 268, 333, 433, 348, 385, 268, 297, 267, 142, 311, 92, 268, 333, 433, 348, 385, 268, 297, 297, 297, 297, 450, 267, 29, 450, 142, 311, 168, 311, 142, 311, 450, 450, 267, 173, 268, 394, 333, 433, 333, 434, 142, 311, 348, 297, 394, 147, 297, 385, 268, 145], [357, 79, 73, 333, 330, 132, 73, 52, 285, 71, 375, 348, 290, 142, 357, 356, 79, 52, 71, 290, 142, 356, 73, 160, 160, 79, 285, 71, 375, 348, 290, 220, 333, 433, 79, 73, 333, 443, 29, 443, 142, 142, 333, 249, 357, 356, 145], [409, 79, 333, 134, 330, 132, 409, 79, 333, 134, 333, 333, 170, 401, 348, 79, 267, 71, 268, 33, 290, 345, 290, 285, 160, 267, 333, 142, 357, 356, 351, 220, 401, 375, 348, 79, 79, 332, 79, 267, 450, 367, 267, 71, 268, 290, 345, 290, 285, 160, 367, 53, 408, 433, 367, 267, 367, 173, 333, 267, 333, 367, 29, 434, 394, 333, 375, 348, 394, 333, 333, 267, 367, 79, 409, 348, 356, 333, 134, 409, 79, 134, 351, 145], [434, 333, 330, 132, 267, 71, 268, 33, 290, 53, 160, 285, 110, 142, 333, 375, 348, 267, 290, 170, 267, 71, 173, 268, 33, 290, 53, 160, 285, 110, 142, 333, 357, 267, 290, 450, 367, 267, 71, 268, 268, 33, 290, 53, 433, 142, 142, 249, 332, 433, 285, 110, 333, 375, 348, 267, 290, 249, 333, 145], [333, 330, 132, 73, 53, 160, 345, 290, 375, 348, 401, 170, 73, 160, 290, 401, 73, 160, 160, 53, 345, 290, 375, 348, 401, 220, 333, 145], [79, 333, 330, 267, 173, 268, 71, 110, 285, 333, 79, 142, 375, 348, 267, 345, 290, 268, 357, 356, 170, 267, 92, 173, 268, 71, 110, 285, 333, 79, 142, 375, 348, 357, 267, 345, 290, 268, 357, 356, 367, 267, 268, 71, 110, 285, 71, 408, 333, 79, 142, 375, 348, 267, 345, 290, 394, 147, 357, 356, 268, 145], [79, 409, 333, 330, 132, 79, 52, 409, 348, 267, 268, 71, 53, 160, 142, 333, 375, 348, 267, 290, 170, 79, 52, 409, 348, 267, 268, 173, 71, 53, 160, 333, 357, 267, 290, 79, 409, 348, 450, 367, 79, 409, 267, 268, 173, 71, 268, 433, 142, 142, 249, 332, 433, 333, 79, 375, 348, 267, 290, 249, 333, 145], [189, 333, 330, 132, 267, 268, 71, 33, 290, 189, 142, 333, 375, 348, 290, 170, 267, 268, 173, 71, 33, 290, 189, 142, 333, 290, 297, 450, 367, 267, 268, 71, 33, 290, 189, 268, 189, 189, 433, 142, 142, 249, 332, 433, 333, 375, 348, 290, 297, 249, 333, 145], [333, 330, 132, 267, 173, 52, 345, 290, 71, 53, 160, 333, 285, 373, 375, 348, 170, 267, 92, 173, 52, 345, 290, 71, 53, 160, 333, 285, 373, 375, 348, 267, 367, 79, 345, 290, 71, 53, 408, 333, 375, 348, 285, 145], [434, 333, 330, 333, 54, 71, 267, 268, 285, 345, 290, 142, 351, 333, 71, 267, 268, 92, 285, 345, 290, 142, 351, 333, 54, 71, 307, 394, 307, 333, 307, 307, 267, 267, 268, 173, 285, 345, 290, 249, 394, 147, 443, 333, 142, 29, 443, 142, 147, 351, 145], [333, 330, 333, 441, 375, 348, 267, 433, 142, 345, 333, 441, 375, 348, 357, 267, 433, 142, 351, 333, 441, 267, 348, 394, 333, 433, 333, 142, 142, 333, 142, 142, 333, 351, 145], [457, 330, 131, 432, 457, 196, 1, 324, 387, 432, 457, 1, 324, 387, 163, 315, 324, 387, 165, 145], [457, 182, 20, 330, 131, 20, 457, 1, 246, 20, 457, 1, 246, 182, 20, 303, 393, 245, 20, 20, 275, 358, 165, 457, 1, 246, 410, 246, 94, 182, 20, 457, 94, 337, 387, 419, 20, 166, 94, 145], [457, 182, 20, 330, 372, 20, 432, 457, 182, 303, 432, 457, 182, 303, 393, 20, 303, 393, 20, 275, 165, 20, 20, 457, 387, 182, 165, 145], [457, 20, 330, 131, 20, 457, 1, 246, 20, 457, 246, 20, 303, 393, 245, 20, 20, 275, 358, 165, 457, 1, 246, 410, 246, 94, 20, 457, 94, 337, 387, 419, 20, 166, 94, 145], [457, 387, 20, 131, 20, 457, 1, 246, 324, 387, 20, 457, 1, 246, 324, 387, 20, 303, 393, 245, 20, 20, 275, 358, 165, 457, 1, 246, 410, 246, 94, 20, 457, 94, 337, 387, 419, 20, 166, 94, 145], [84, 457, 330, 372, 20, 457, 84, 20, 457, 84, 84, 163, 315, 84, 165, 419, 145], [228, 457, 330, 372, 228, 246, 457, 196, 228, 246, 457, 196, 163, 315, 165, 419, 145], [457, 330, 131, 398, 432, 457, 398, 432, 457, 163, 315, 165, 145], [398, 457, 330, 131, 398, 457, 196, 398, 457, 196, 163, 315, 165, 145], [457, 330, 131, 228, 20, 457, 228, 20, 457, 163, 315, 165, 145], [457, 84, 20, 330, 131, 20, 457, 1, 246, 84, 20, 457, 1, 246, 84, 20, 303, 393, 245, 20, 20, 275, 358, 165, 457, 1, 246, 410, 246, 94, 84, 20, 457, 94, 337, 387, 419, 20, 166, 94, 145], [49, 330, 63, 76, 76, 75, 76, 268, 14, 170, 63, 92, 76, 75, 76, 268, 14, 312, 14, 14, 63, 251, 14, 370, 145], [417, 254, 49, 5, 49, 173, 63, 254, 76, 417, 23, 345, 290, 170, 221, 49, 92, 173, 63, 254, 76, 417, 23, 345, 290, 63, 173, 221, 49, 63, 49, 254, 417, 23, 345, 290, 348, 267, 354, 370, 399, 145], [68, 49, 330, 5, 221, 76, 63, 278, 170, 63, 278, 92, 68, 49, 49, 49, 63, 278, 173, 68, 49, 49, 63, 49, 15, 68, 49, 145], [30, 400, 49, 330, 5, 391, 345, 290, 142, 267, 14, 254, 76, 11, 63, 278, 170, 30, 345, 290, 142, 267, 14, 370, 11, 399, 63, 92, 278, 312, 345, 290, 30, 30, 30, 142, 49, 118, 49, 118, 30, 399, 30, 267, 30, 399, 55, 14, 294, 200, 14, 294, 14, 294, 254, 370, 30, 399, 370, 11, 76, 30, 11, 294, 400, 14, 294, 370, 14, 370, 358, 162, 63, 173, 278, 277, 63, 251, 14, 370, 162, 49, 173, 55, 145], [49, 330, 49, 63, 354, 76, 417, 267, 345, 290, 23, 221, 49, 63, 92, 354, 76, 417, 92, 173, 267, 345, 290, 23, 63, 92, 173, 221, 49, 63, 354, 76, 55, 417, 173, 345, 290, 23, 267, 267, 49, 251, 76, 145], [189, 49, 330, 5, 300, 135, 63, 278, 345, 254, 76, 75, 76, 189, 63, 92, 278, 345, 254, 76, 75, 76, 189, 370, 312, 300, 135, 63, 278, 345, 251, 135, 254, 75, 76, 135, 294, 189, 76, 135, 76, 49, 15, 145], [49, 330, 5, 14, 186, 30, 49, 118, 278, 345, 290, 142, 254, 76, 231, 348, 63, 278, 170, 14, 186, 30, 49, 118, 345, 290, 142, 370, 231, 348, 63, 92, 277, 312, 186, 30, 49, 118, 278, 142, 345, 290, 251, 241, 241, 14, 14, 294, 241, 200, 14, 294, 370, 254, 241, 254, 241, 241, 76, 14, 294, 400, 14, 294, 370, 358, 162, 63, 63, 14, 370, 370, 162, 14, 15, 162, 231, 145], [291, 49, 330, 300, 135, 291, 254, 76, 79, 63, 310, 268, 310, 290, 278, 170, 291, 254, 76, 79, 63, 310, 268, 310, 312, 300, 135, 291, 135, 254, 291, 79, 254, 135, 400, 135, 400, 135, 221, 63, 310, 268, 310, 290, 278, 63, 135, 135, 63, 76, 221, 15, 162, 278, 370, 145], [62, 330, 132, 186, 30, 345, 290, 268, 310, 267, 75, 76, 60, 12, 417, 186, 30, 345, 290, 268, 310, 267, 75, 76, 190, 12, 370, 30, 75, 30, 75, 30, 76, 267, 62, 62, 60, 12, 417, 215, 145], [12, 75, 330, 12, 116, 76, 268, 75, 76, 186, 30, 345, 290, 75, 76, 12, 116, 76, 268, 75, 186, 30, 345, 290, 370, 190, 60, 12, 12, 116, 76, 268, 75, 249, 186, 30, 251, 251, 186, 30, 353, 345, 290, 62, 62, 370, 76, 62, 62, 76, 62, 62, 60, 145], [190, 330, 132, 186, 30, 208, 345, 186, 290, 267, 76, 60, 186, 30, 267, 370, 76, 75, 190, 30, 284, 284, 284, 353, 345, 290, 367, 267, 284, 367, 284, 370, 76, 284, 76, 284, 62, 284, 60, 348, 145], [3, 62, 330, 80, 173, 267, 214, 406, 348, 186, 121, 345, 267, 310, 290, 157, 345, 290, 60, 215, 268, 92, 267, 207, 186, 121, 345, 267, 310, 157, 345, 290, 190, 215, 268, 80, 173, 207, 406, 121, 345, 370, 370, 370, 345, 267, 310, 290, 157, 345, 290, 370, 267, 251, 370, 62, 370, 145], [268, 62, 330, 132, 186, 30, 345, 290, 267, 190, 60, 76, 268, 186, 30, 345, 290, 267, 190, 60, 268, 249, 30, 284, 30, 267, 62, 370, 62, 60, 268, 145], [330, 132, 190, 60, 186, 30, 284, 76, 268, 186, 30, 284, 370, 76, 268, 63, 63, 190, 60, 60, 30, 76, 284, 284, 284, 76, 184, 207, 60, 268, 62, 60, 76, 145], [132, 267, 268, 290, 310, 121, 345, 417, 348, 348, 348, 401, 257, 190, 267, 268, 290, 92, 310, 121, 345, 417, 348, 348, 348, 60, 367, 267, 267, 268, 290, 268, 173, 310, 121, 345, 173, 353, 345, 417, 348, 348, 348, 401, 257, 249, 190, 60, 145], [330, 190, 60, 63, 186, 30, 345, 249, 76, 215, 417, 257, 290, 208, 345, 186, 190, 60, 63, 186, 30, 317, 345, 3, 249, 370, 215, 370, 417, 257, 208, 290, 208, 345, 186, 184, 190, 60, 63, 60, 184, 348, 348, 60, 184, 345, 3, 317, 317, 184, 377, 317, 284, 377, 377, 62, 345, 3, 249, 62, 207, 377, 62, 184, 377, 62, 62, 76, 62, 215, 370, 417, 60, 145], [197, 330, 372, 80, 173, 406, 207, 121, 290, 345, 290, 436, 267, 441, 278, 92, 406, 207, 121, 345, 290, 267, 441, 278, 80, 173, 406, 207, 163, 315, 39, 267, 441, 197, 290, 278, 267, 145], [380, 330, 372, 80, 380, 193, 207, 345, 290, 436, 267, 380, 193, 207, 345, 290, 267, 80, 380, 193, 173, 406, 207, 163, 315, 39, 267, 197, 380, 193, 267, 145], [434, 197, 330, 197, 290, 32, 197, 290, 32, 163, 315, 197, 321, 163, 315, 197, 290, 321, 163, 315, 197, 32, 321, 145], [27, 115, 197, 330, 27, 406, 214, 115, 267, 345, 441, 27, 207, 214, 115, 267, 345, 2, 441, 163, 315, 39, 2, 267, 441, 103, 267, 115, 145], [197, 330, 372, 172, 27, 345, 186, 121, 173, 207, 436, 267, 267, 441, 27, 345, 186, 92, 214, 207, 436, 267, 267, 441, 80, 8, 425, 425, 27, 368, 27, 163, 315, 345, 121, 315, 251, 163, 315, 341, 309, 214, 207, 267, 267, 441, 315, 340, 251, 85, 434, 145], [291, 197, 330, 372, 172, 27, 345, 186, 121, 291, 207, 441, 27, 345, 92, 186, 121, 291, 214, 207, 441, 80, 8, 425, 425, 27, 368, 27, 163, 315, 345, 121, 173, 291, 315, 251, 163, 315, 341, 309, 207, 441, 315, 340, 251, 85, 434, 145], [290, 197, 330, 372, 172, 27, 345, 186, 121, 173, 290, 207, 436, 267, 27, 345, 186, 92, 290, 214, 207, 436, 267, 80, 8, 425, 425, 27, 368, 27, 163, 315, 345, 121, 173, 290, 315, 251, 163, 315, 341, 309, 207, 267, 315, 340, 251, 85, 434, 145], [173, 197, 330, 172, 27, 173, 406, 214, 267, 345, 296, 264, 278, 441, 172, 27, 92, 207, 267, 345, 296, 264, 278, 441, 172, 27, 368, 368, 163, 315, 39, 2, 267, 441, 103, 296, 264, 267, 145], [197, 330, 372, 80, 321, 267, 173, 345, 207, 267, 92, 173, 345, 214, 207, 80, 267, 173, 345, 207, 163, 315, 267, 441, 103, 145], [197, 330, 372, 27, 406, 214, 345, 290, 290, 121, 267, 296, 264, 441, 172, 207, 92, 290, 121, 267, 296, 264, 441, 172, 27, 368, 368, 163, 315, 39, 2, 267, 441, 103, 330, 434, 145], [53, 79, 210, 330, 132, 63, 173, 290, 76, 262, 79, 53, 254, 63, 173, 290, 277, 76, 210, 262, 79, 53, 254, 312, 63, 173, 290, 44, 76, 348, 348, 47, 210, 262, 211, 79, 53, 254, 348, 211, 211, 348, 254, 162, 15, 162, 76, 145], [380, 210, 337, 132, 210, 262, 334, 76, 380, 267, 173, 231, 348, 254, 76, 76, 345, 290, 278, 170, 210, 262, 380, 267, 92, 173, 254, 277, 345, 290, 278, 210, 262, 380, 267, 345, 334, 380, 173, 461, 254, 461, 277, 345, 290, 251, 210, 262, 337, 231, 47, 210, 337, 332, 254, 277, 15, 76, 278, 145], [460, 210, 330, 132, 210, 330, 210, 460, 262, 186, 429, 240, 348, 460, 267, 268, 92, 186, 417, 345, 290, 417, 281, 142, 278, 345, 290, 460, 460, 262, 358, 267, 268, 186, 429, 429, 417, 23, 278, 270, 345, 290, 417, 281, 312, 334, 76, 142, 278, 277, 345, 290, 211, 429, 348, 251, 277, 211, 429, 348, 76, 332, 145], [329, 330, 132, 329, 348, 170, 329, 348, 426, 329, 348, 426, 211, 329, 211, 348, 211, 348, 211, 329, 211, 348, 76, 426, 162, 15, 162, 15, 76, 36, 145], [134, 330, 132, 267, 268, 186, 30, 345, 290, 417, 348, 417, 281, 334, 76, 23, 142, 76, 262, 254, 76, 63, 161, 345, 186, 290, 76, 23, 170, 267, 268, 92, 186, 30, 345, 290, 348, 281, 23, 142, 210, 262, 254, 63, 461, 345, 186, 290, 76, 23, 76, 312, 267, 268, 173, 30, 345, 290, 240, 240, 417, 348, 417, 281, 348, 251, 334, 23, 142, 277, 267, 262, 47, 262, 210, 262, 211, 240, 251, 254, 47, 262, 262, 240, 251, 254, 348, 334, 251, 262, 211, 332, 262, 332, 254, 332, 240, 348, 210, 262, 162, 63, 161, 281, 348, 44, 353, 345, 290, 277, 134, 348, 134, 23, 134, 277, 23, 145], [210, 337, 132, 210, 262, 186, 30, 186, 268, 345, 290, 231, 348, 334, 76, 380, 142, 254, 76, 170, 210, 262, 186, 30, 186, 268, 345, 290, 231, 380, 142, 210, 262, 367, 30, 268, 345, 290, 380, 231, 348, 334, 142, 251, 210, 251, 231, 254, 277, 145], [210, 330, 132, 380, 278, 76, 345, 290, 231, 348, 44, 210, 262, 254, 170, 334, 380, 278, 345, 290, 231, 44, 210, 262, 254, 334, 380, 278, 277, 345, 290, 211, 417, 348, 417, 348, 210, 262, 262, 211, 334, 251, 254, 211, 262, 334, 417, 348, 254, 211, 262, 417, 348, 211, 254, 277, 76, 145], [330, 132, 267, 186, 30, 345, 186, 290, 268, 231, 348, 358, 334, 76, 254, 76, 23, 170, 267, 186, 30, 345, 290, 173, 92, 268, 231, 348, 358, 254, 312, 267, 367, 30, 345, 290, 30, 268, 268, 231, 348, 348, 281, 30, 251, 358, 30, 358, 30, 251, 332, 358, 332, 231, 348, 358, 334, 348, 254, 76, 334, 23, 15, 76, 145], [210, 330, 267, 268, 290, 345, 290, 143, 388, 460, 417, 281, 270, 417, 334, 76, 254, 76, 23, 278, 142, 143, 460, 388, 417, 267, 268, 290, 92, 345, 290, 143, 388, 460, 281, 254, 23, 278, 143, 388, 417, 262, 312, 267, 367, 268, 290, 92, 345, 290, 143, 388, 460, 417, 281, 270, 345, 290, 433, 417, 281, 47, 417, 251, 332, 210, 211, 334, 254, 23, 278, 345, 290, 142, 143, 388, 460, 417, 173, 345, 290, 267, 211, 47, 262, 211, 334, 251, 433, 211, 262, 334, 433, 15, 76, 348, 433, 145], [223, 3, 330, 246, 225, 75, 246, 144, 225, 75, 76, 307, 246, 280, 394, 280, 75, 394, 76, 280, 145], [223, 330, 225, 441, 246, 345, 290, 75, 76, 83, 144, 225, 441, 246, 345, 290, 75, 76, 83, 225, 441, 345, 394, 246, 76, 345, 290, 394, 394, 387, 83, 145], [225, 330, 132, 267, 186, 30, 268, 225, 441, 30, 75, 417, 268, 267, 186, 268, 225, 441, 30, 75, 75, 417, 268, 267, 268, 394, 186, 30, 186, 30, 394, 441, 30, 280, 417, 394, 280, 280, 76, 394, 76, 268, 145], [380, 3, 330, 132, 63, 380, 246, 345, 290, 225, 75, 76, 254, 246, 290, 75, 76, 254, 312, 63, 380, 380, 246, 345, 290, 394, 246, 225, 246, 225, 75, 254, 76, 254, 145], [223, 330, 132, 12, 83, 278, 63, 161, 246, 345, 290, 310, 290, 225, 75, 76, 76, 76, 76, 316, 76, 254, 76, 75, 12, 118, 83, 278, 63, 246, 345, 290, 310, 225, 75, 76, 76, 76, 178, 76, 316, 76, 254, 76, 75, 76, 307, 12, 12, 118, 83, 278, 394, 118, 307, 307, 63, 161, 394, 246, 353, 345, 290, 290, 348, 312, 280, 348, 394, 75, 178, 316, 76, 76, 280, 246, 251, 254, 75, 223, 76, 387, 118, 251, 145], [223, 330, 132, 225, 63, 161, 246, 186, 290, 345, 380, 254, 76, 254, 76, 417, 23, 225, 63, 246, 186, 290, 345, 380, 254, 254, 417, 23, 312, 271, 307, 441, 280, 136, 63, 161, 394, 246, 348, 353, 276, 290, 345, 380, 394, 254, 394, 280, 394, 348, 254, 280, 254, 417, 23, 145], [76, 223, 330, 132, 63, 161, 246, 441, 144, 225, 116, 76, 75, 76, 76, 76, 76, 290, 345, 290, 23, 63, 246, 441, 116, 76, 75, 76, 178, 345, 290, 23, 312, 63, 161, 246, 441, 225, 394, 280, 130, 116, 76, 75, 76, 178, 353, 345, 290, 394, 251, 63, 394, 118, 118, 225, 76, 271, 23, 145], [307, 223, 330, 246, 225, 63, 75, 76, 345, 290, 278, 246, 144, 225, 63, 345, 290, 278, 307, 246, 144, 225, 280, 408, 63, 75, 345, 290, 280, 76, 63, 246, 348, 387, 278, 145], [320, 223, 330, 320, 267, 345, 290, 63, 161, 246, 186, 310, 290, 75, 76, 76, 225, 280, 75, 76, 254, 76, 388, 267, 345, 290, 186, 310, 280, 271, 320, 267, 345, 290, 186, 271, 320, 348, 63, 161, 246, 348, 348, 394, 186, 310, 290, 75, 394, 320, 394, 320, 225, 249, 75, 254, 76, 145], [223, 330, 380, 278, 246, 441, 345, 225, 63, 161, 310, 268, 310, 290, 345, 75, 76, 254, 380, 278, 246, 441, 345, 225, 63, 310, 268, 310, 290, 345, 75, 254, 380, 278, 246, 307, 441, 345, 144, 130, 441, 136, 307, 63, 161, 394, 251, 246, 310, 268, 310, 290, 345, 307, 394, 75, 76, 249, 380, 251, 394, 249, 280, 225, 254, 145], [223, 330, 132, 308, 71, 268, 72, 345, 290, 310, 268, 310, 456, 225, 308, 71, 268, 72, 345, 290, 310, 268, 310, 456, 225, 308, 71, 268, 307, 441, 408, 441, 433, 72, 345, 290, 310, 268, 310, 456, 441, 441, 249, 280, 276, 145], [223, 3, 330, 132, 225, 441, 388, 71, 246, 75, 76, 116, 144, 225, 130, 441, 388, 71, 246, 441, 388, 71, 307, 433, 408, 441, 307, 246, 76, 116, 76, 394, 225, 249, 145], [106, 116, 226, 142, 401, 401, 1, 161, 84, 105, 95, 310, 163, 99, 401, 63, 432, 246, 105, 116, 154, 142, 446, 401, 401, 84, 106, 95, 310, 163, 63, 432, 246, 106, 116, 154, 142, 401, 286, 164, 401, 105, 161, 95, 310, 142, 446, 251, 142, 226, 25, 163, 99, 249, 312, 251, 13, 13, 298, 358, 275, 25, 275, 106, 424, 368, 164, 154, 63, 401, 105, 116, 154, 432, 246, 299, 13, 106, 424, 15, 64, 224, 226, 145], [226, 16, 401, 161, 345, 142, 401, 432, 163, 99, 63, 401, 432, 116, 345, 142, 446, 401, 432, 138, 163, 63, 432, 116, 226, 163, 401, 161, 345, 378, 1, 161, 251, 243, 142, 446, 345, 250, 286, 164, 401, 286, 164, 432, 28, 163, 99, 28, 1, 251, 142, 446, 1, 161, 25, 224, 25, 299, 13, 338, 25, 17, 358, 294, 275, 25, 358, 226, 275, 358, 402, 226, 424, 271, 226, 226, 275, 226, 64, 63, 28, 250, 401, 28, 432, 28, 116, 28, 64, 299, 13, 338, 64, 224, 359, 224, 359, 346, 224, 359, 64, 145], [226, 16, 142, 401, 401, 1, 161, 310, 63, 401, 432, 95, 310, 142, 446, 401, 401, 1, 161, 1, 161, 163, 310, 63, 401, 432, 95, 310, 17, 426, 275, 142, 250, 378, 286, 164, 401, 250, 142, 446, 286, 401, 28, 142, 446, 286, 401, 446, 243, 397, 1, 161, 310, 142, 446, 294, 397, 251, 142, 446, 25, 286, 299, 13, 338, 25, 17, 426, 17, 426, 402, 25, 106, 368, 164, 402, 106, 312, 154, 63, 105, 155, 368, 25, 155, 106, 271, 106, 105, 359, 64, 359, 105, 95, 310, 145], [226, 142, 401, 161, 401, 116, 76, 401, 246, 142, 76, 246, 142, 250, 378, 286, 164, 401, 250, 142, 446, 286, 401, 446, 243, 397, 1, 161, 401, 142, 446, 294, 397, 312, 299, 13, 224, 251, 298, 275, 358, 106, 424, 368, 164, 106, 424, 15, 154, 116, 76, 401, 246, 299, 13, 116, 251, 346, 145], [226, 16, 142, 401, 401, 1, 161, 163, 99, 116, 76, 401, 246, 205, 142, 446, 401, 138, 163, 116, 246, 205, 142, 401, 286, 164, 401, 1, 161, 142, 446, 251, 142, 446, 25, 163, 99, 249, 25, 13, 13, 25, 13, 13, 358, 275, 25, 275, 298, 358, 17, 358, 25, 106, 424, 368, 164, 271, 154, 116, 76, 401, 246, 299, 13, 106, 106, 424, 15, 116, 251, 105, 205, 346, 105, 226, 145], [224, 198, 116, 346, 161, 401, 142, 401, 163, 99, 198, 116, 161, 1, 161, 1, 161, 163, 401, 142, 446, 401, 138, 163, 198, 116, 17, 426, 275, 243, 1, 161, 401, 294, 142, 250, 378, 286, 164, 401, 250, 142, 446, 286, 401, 28, 142, 446, 286, 401, 446, 161, 401, 142, 446, 138, 163, 99, 251, 142, 446, 25, 286, 249, 25, 299, 13, 338, 25, 17, 426, 17, 426, 402, 25, 226, 368, 164, 402, 224, 155, 368, 25, 155, 106, 271, 226, 224, 359, 198, 116, 198, 116, 224, 359, 198, 116, 224, 145], [287, 63, 226, 16, 401, 1, 239, 95, 310, 116, 401, 287, 63, 7, 95, 310, 116, 401, 287, 63, 312, 401, 1, 239, 95, 310, 7, 28, 286, 116, 401, 7, 28, 286, 164, 401, 1, 239, 95, 310, 7, 25, 299, 13, 298, 275, 358, 426, 402, 15, 226, 287, 63, 226, 226, 145], [26, 330, 5, 421, 186, 26, 348, 75, 76, 76, 417, 268, 12, 278, 26, 186, 26, 348, 417, 268, 12, 278, 312, 367, 421, 186, 26, 348, 75, 417, 268, 12, 278, 26, 348, 15, 145], [308, 259, 330, 5, 308, 267, 345, 290, 310, 369, 79, 52, 204, 290, 76, 11, 268, 308, 267, 345, 290, 310, 369, 52, 204, 76, 11, 268, 312, 308, 267, 310, 345, 290, 358, 358, 52, 267, 345, 290, 79, 308, 79, 79, 207, 79, 367, 308, 79, 204, 76, 11, 268, 145], [26, 79, 259, 330, 79, 26, 348, 421, 75, 76, 268, 187, 116, 170, 79, 421, 75, 76, 187, 116, 79, 21, 348, 421, 76, 268, 76, 187, 116, 145], [31, 330, 5, 116, 76, 75, 76, 290, 257, 268, 268, 170, 31, 116, 76, 75, 76, 290, 257, 421, 268, 268, 417, 367, 31, 367, 367, 31, 116, 76, 75, 76, 367, 251, 257, 31, 76, 31, 76, 211, 421, 31, 76, 211, 145], [73, 330, 5, 73, 267, 345, 186, 290, 348, 203, 76, 75, 76, 11, 116, 88, 170, 73, 160, 267, 345, 186, 290, 207, 203, 75, 76, 11, 116, 88, 312, 358, 73, 160, 267, 345, 290, 348, 207, 73, 421, 358, 73, 203, 76, 75, 76, 358, 15, 76, 11, 116, 88, 145], [79, 259, 330, 5, 79, 348, 421, 203, 76, 417, 268, 170, 348, 203, 417, 268, 312, 79, 348, 367, 421, 79, 203, 417, 268, 15, 145], [73, 330, 5, 73, 256, 267, 268, 186, 121, 81, 310, 348, 345, 290, 421, 78, 417, 204, 39, 88, 73, 256, 267, 268, 186, 310, 348, 345, 290, 78, 417, 39, 88, 73, 160, 160, 256, 267, 367, 268, 268, 73, 256, 121, 81, 310, 348, 345, 290, 73, 256, 251, 256, 421, 73, 76, 76, 88, 145], [26, 79, 259, 330, 5, 267, 79, 52, 345, 186, 290, 310, 81, 310, 26, 348, 421, 76, 268, 417, 12, 278, 116, 170, 267, 79, 52, 290, 310, 81, 310, 21, 421, 39, 268, 417, 278, 116, 11, 312, 267, 79, 345, 290, 310, 81, 310, 21, 348, 348, 367, 421, 211, 21, 79, 76, 268, 417, 12, 278, 15, 116, 11, 145], [79, 330, 5, 267, 417, 81, 310, 121, 310, 345, 290, 79, 421, 39, 116, 417, 88, 267, 79, 81, 310, 121, 310, 345, 290, 421, 39, 116, 88, 267, 417, 81, 310, 121, 310, 345, 290, 348, 79, 348, 312, 211, 421, 367, 211, 76, 251, 348, 211, 421, 211, 76, 79, 348, 15, 76, 116, 417, 88, 145], [79, 259, 330, 5, 369, 79, 52, 345, 290, 81, 310, 310, 121, 267, 268, 290, 421, 290, 203, 76, 75, 76, 116, 369, 79, 52, 290, 310, 310, 267, 268, 290, 207, 421, 203, 75, 116, 312, 79, 52, 345, 81, 310, 310, 121, 367, 267, 79, 79, 79, 367, 267, 268, 268, 290, 290, 79, 207, 421, 367, 251, 203, 75, 76, 15, 76, 145], [26, 268, 335, 330, 268, 26, 348, 26, 268, 370, 26, 348, 289, 268, 335, 335, 268, 335, 268, 335, 275, 358, 26, 348, 335, 55, 145], [31, 268, 335, 330, 268, 441, 161, 345, 290, 310, 31, 441, 267, 268, 441, 161, 345, 290, 310, 31, 441, 267, 267, 289, 268, 370, 335, 441, 161, 345, 290, 310, 31, 441, 161, 335, 441, 335, 161, 335, 267, 55, 275, 425, 387, 345, 145], [59, 79, 268, 335, 330, 116, 76, 59, 348, 79, 76, 326, 268, 161, 142, 274, 50, 267, 116, 76, 59, 348, 79, 326, 268, 142, 267, 116, 76, 59, 348, 79, 326, 268, 289, 268, 268, 335, 370, 211, 335, 275, 358, 426, 79, 251, 268, 335, 251, 267, 161, 142, 49, 118, 335, 161, 142, 118, 335, 267, 55, 335, 76, 326, 348, 145], [268, 116, 374, 268, 268, 374, 268, 268, 335, 374, 268, 358, 382, 268, 335, 374, 251, 268, 335, 268, 268, 335, 335, 335, 251, 145], [254, 268, 335, 330, 268, 254, 76, 161, 142, 50, 267, 268, 370, 142, 267, 348, 289, 268, 335, 335, 254, 268, 335, 335, 254, 211, 76, 268, 335, 161, 142, 50, 335, 161, 142, 50, 142, 50, 268, 335, 166, 267, 335, 55, 76, 275, 425, 145], [268, 335, 330, 382, 268, 50, 76, 310, 270, 345, 290, 142, 161, 382, 268, 49, 118, 274, 49, 118, 345, 290, 142, 348, 358, 382, 275, 268, 370, 211, 335, 49, 118, 310, 270, 345, 290, 142, 161, 268, 335, 161, 142, 49, 118, 251, 268, 335, 358, 55, 348, 145], [300, 268, 335, 330, 268, 254, 76, 300, 348, 76, 270, 267, 142, 274, 50, 161, 270, 268, 254, 300, 348, 348, 267, 142, 268, 370, 211, 335, 426, 275, 268, 335, 370, 268, 211, 300, 348, 254, 387, 270, 426, 166, 142, 50, 270, 300, 335, 166, 161, 142, 50, 267, 268, 335, 275, 425, 267, 387, 270, 145], [20, 273, 51, 20, 401, 401, 432, 148, 246, 345, 310, 89, 63, 63, 20, 55, 401, 246, 345, 310, 63, 63, 20, 401, 401, 432, 445, 246, 445, 345, 310, 89, 445, 63, 273, 25, 55, 159, 25, 63, 145], [20, 273, 51, 20, 142, 432, 148, 20, 142, 432, 148, 89, 235, 20, 249, 142, 432, 89, 273, 25, 367, 58, 273, 159, 145], [42, 20, 273, 51, 20, 142, 20, 142, 42, 20, 445, 142, 63, 367, 25, 367, 42, 159, 145], [84, 273, 51, 142, 458, 142, 401, 246, 63, 432, 148, 161, 95, 310, 310, 84, 63, 84, 142, 458, 446, 401, 246, 25, 63, 432, 148, 95, 310, 310, 84, 63, 458, 401, 246, 142, 446, 445, 63, 432, 161, 95, 310, 310, 246, 25, 249, 84, 25, 249, 84, 273, 25, 58, 273, 159, 273, 84, 273, 145], [20, 273, 51, 20, 246, 432, 148, 161, 310, 345, 20, 246, 432, 310, 345, 20, 142, 235, 20, 142, 249, 249, 310, 345, 25, 367, 58, 273, 25, 145], [273, 51, 161, 310, 246, 63, 142, 458, 142, 161, 246, 63, 142, 458, 446, 404, 445, 161, 310, 63, 246, 142, 458, 142, 246, 142, 446, 367, 273, 25, 367, 55, 273, 273, 25, 404, 145], [20, 273, 51, 20, 142, 432, 148, 266, 20, 432, 148, 266, 235, 20, 249, 142, 432, 249, 266, 89, 367, 273, 25, 58, 25, 159, 55, 273, 20, 404, 145, 25, 273, 273], [84, 273, 51, 20, 142, 266, 95, 310, 84, 20, 142, 266, 95, 310, 84, 345, 235, 20, 249, 142, 367, 273, 25, 58, 25, 159, 55, 273, 404, 145], [300, 330, 161, 300, 348, 254, 76, 370, 161, 300, 254, 370, 161, 18, 135, 18, 452, 402, 338, 135, 161, 135, 358, 348, 135, 76, 15, 76, 119, 55, 370, 145], [26, 79, 300, 330, 132, 79, 26, 348, 49, 254, 76, 268, 26, 79, 26, 348, 49, 254, 268, 348, 49, 358, 26, 348, 79, 254, 268, 370, 145], [49, 300, 330, 132, 14, 417, 254, 76, 417, 254, 18, 23, 290, 26, 348, 79, 254, 76, 268, 14, 294, 417, 348, 254, 417, 18, 23, 290, 157, 26, 79, 254, 268, 268, 271, 358, 14, 294, 358, 300, 14, 294, 417, 348, 254, 417, 254, 18, 23, 26, 79, 300, 14, 294, 26, 348, 79, 254, 268, 15, 76, 300, 23, 290, 157, 26, 79, 300, 268, 330, 300, 26, 79, 300, 79, 254, 145], [300, 119, 330, 132, 300, 73, 119, 330, 73, 119, 55, 73, 254, 345, 73, 294, 163, 315, 73, 163, 315, 73, 100, 254, 270, 23, 345, 358, 73, 251, 251, 90, 73, 251, 119, 119, 55, 300, 348, 300, 15, 76, 145], [79, 300, 330, 267, 345, 290, 270, 157, 300, 119, 254, 76, 268, 417, 267, 267, 345, 290, 270, 157, 254, 268, 417, 267, 345, 290, 270, 157, 348, 271, 267, 300, 119, 119, 348, 300, 119, 254, 268, 271, 119, 300, 417, 145], [300, 330, 300, 135, 267, 417, 348, 254, 76, 23, 300, 267, 417, 254, 23, 185, 300, 267, 267, 135, 90, 300, 185, 267, 185, 300, 300, 135, 135, 348, 76, 23, 300, 185, 300, 76, 119, 300, 76, 119, 145], [300, 330, 161, 345, 401, 441, 456, 436, 267, 417, 254, 76, 23, 401, 441, 456, 436, 267, 417, 254, 23, 100, 161, 345, 401, 441, 456, 267, 332, 251, 251, 100, 135, 332, 452, 402, 135, 18, 18, 358, 332, 163, 315, 100, 417, 267, 345, 300, 396, 452, 402, 119, 18, 135, 135, 135, 90, 135, 90, 348, 135, 119, 370, 254, 348, 23, 300, 290, 268, 300, 358, 300, 396, 271, 76, 119, 145], [343, 11, 268, 439, 356, 333, 262, 294, 343, 345, 343, 441, 156, 187, 268, 417, 345, 345, 345, 345, 345, 345, 345, 345, 124, 11, 11, 11, 11, 294, 268, 251, 11, 251, 187, 337, 217, 207, 217, 217, 207, 217, 207, 11, 207, 217, 187, 251, 439, 439, 36, 439, 187, 36, 36, 357, 356, 375, 348, 417, 417, 417, 11, 109, 212, 212, 251, 109, 187, 91, 343, 345, 343, 343, 156, 347, 124, 370, 91, 36, 390, 91, 370, 343, 390, 91, 390, 343, 343, 390, 343, 343, 343, 169, 370, 169, 343, 267, 343, 343, 393, 169, 343, 343, 343, 343, 343, 343, 169, 267, 343, 267, 343, 174, 343, 333, 262, 262, 343, 187, 109, 333, 262, 187], [428, 439, 375, 348, 176, 180, 306, 155, 180, 370], [403, 152, 41, 403, 152, 41, 109, 212, 357, 356, 267, 428, 428, 152, 370, 370, 152, 124, 428, 370, 41, 41, 428, 356, 267, 155, 267, 428, 109, 152, 402, 152, 41, 38, 156, 428, 41, 156, 156, 306, 152, 41, 4, 124, 4, 38, 370, 156, 370, 428, 41, 38, 124, 370, 174, 174, 155, 71, 18, 306, 18, 370, 403, 71, 18, 38, 306, 370, 294, 370, 370, 343, 155, 155, 403, 18, 306, 18, 120, 71, 18, 428, 370, 403, 370, 4, 306, 4, 370, 174, 370, 155, 370, 428, 370, 370, 174, 370, 212, 120, 18, 428, 370, 120, 174, 11, 370, 11, 155, 11, 101, 306, 370, 403, 101, 18, 176, 428, 370, 399, 347, 370, 375, 348, 439], [109, 212, 357, 356, 267, 403, 358, 109, 109, 428, 267, 109, 294, 403, 428, 267, 428, 109, 428, 399, 109, 399, 428, 428, 357, 356, 267, 399, 109, 428, 156, 428, 428, 347, 156, 428, 358, 358, 109, 428, 358, 263, 337, 428, 263, 337, 337, 337, 337, 294, 124, 212, 71, 428, 375, 348, 133], [403, 152, 180, 441, 345, 401, 267, 152, 403, 152, 180, 343, 152, 441, 345, 401, 152, 156, 343, 152, 343, 152, 13, 267, 13, 13, 0, 267, 13, 441, 156, 13, 13, 402, 343, 13, 156, 399, 343, 306, 399, 152, 399, 347, 347, 11, 375, 0, 375, 348, 0, 375, 348, 0, 306, 370, 11, 375, 348, 155, 343, 370, 11, 343], [403, 109, 428, 347, 294, 428, 156, 428, 156, 428, 428, 41, 109, 428, 109, 428, 393, 267, 370], [428, 11, 124, 370, 267, 217, 207, 217, 268, 375, 348, 428, 370, 370, 11, 267, 217, 207, 11, 428, 268, 345, 375, 348, 133, 217], [403, 333, 333, 435, 435, 401, 345, 343, 11, 294, 263, 399, 348, 375, 439, 357, 333, 333, 441, 333, 333, 435, 401, 345, 401, 345, 333, 435, 251, 333, 343, 11, 263, 375, 348, 439, 348, 439, 439, 375, 348, 348], [435, 375, 348, 267, 186, 290, 451, 348, 268, 310, 345, 290, 149, 391, 435, 375, 348, 267, 186, 290, 451, 348, 268, 310, 345, 290, 391, 230, 240, 312, 185, 267, 185, 183, 230], [81, 217, 391, 330, 132, 364, 391, 88, 207, 441, 345, 81, 310, 186, 121, 345, 290, 207, 364, 88, 207, 441, 345, 81, 310, 186, 121, 345, 290, 92, 173, 207, 217, 88, 207, 441, 345, 40, 81, 310, 121, 345, 290, 173, 207, 81, 217, 391, 391, 185, 185, 185, 391, 185, 391, 185, 391, 391, 240, 399, 240, 88, 217, 348, 88, 145], [391, 330, 222, 391, 375, 348, 290, 267, 391, 375, 348, 290, 173, 92, 267, 391, 375, 348, 290, 173, 13, 13, 391, 230, 391, 230, 93, 267, 391, 391, 145], [63, 391, 330, 132, 391, 345, 290, 267, 63, 391, 345, 267, 92, 384, 384, 312, 353, 391, 345, 290, 449, 323, 358, 327, 391, 267, 267, 267, 391, 159, 63, 173, 391, 24, 391, 63, 24, 159, 24, 391, 391, 449, 323, 391, 207, 391, 399, 370, 145], [149, 391, 330, 222, 391, 345, 290, 267, 63, 149, 391, 345, 290, 267, 63, 384, 384, 312, 353, 391, 345, 290, 449, 323, 358, 327, 391, 267, 391, 159, 63, 173, 391, 24, 391, 63, 24, 159, 24, 391, 391, 449, 323, 391, 207, 391, 399, 370, 145], [391, 132, 53, 268, 391, 267, 333, 345, 290, 310, 268, 268, 267, 333, 310, 268, 310, 312, 345, 290, 310, 268, 310, 53, 268, 391, 93, 53, 268, 267, 391, 53, 268, 391, 333, 145], [358, 391, 222, 267, 345, 290, 364, 391, 63, 267, 345, 290, 92, 364, 391, 63, 312, 358, 267, 353, 345, 290, 353, 391, 345, 290, 358, 391, 327, 63, 145], [104, 391, 222, 364, 391, 268, 375, 348, 267, 176, 33, 290, 333, 364, 268, 375, 348, 267, 176, 92, 33, 290, 333, 364, 391, 399, 375, 348, 267, 176, 268, 240, 104, 104, 268, 104, 309, 290, 333, 145], [391, 345, 186, 290, 267, 353, 391, 345, 290, 185, 312, 185, 391, 185, 391, 267, 185, 391, 391], [391, 366, 391, 10, 345, 290, 267, 19, 391, 294, 267, 19, 366, 294, 391, 10, 19, 366, 391, 10, 267, 366, 345, 290, 159, 353, 345, 290, 145], [391, 366, 132, 391, 121, 81, 310, 310, 345, 290, 33, 290, 33, 290, 268, 19, 391, 81, 310, 310, 345, 290, 33, 290, 294, 33, 290, 294, 268, 294, 19, 366, 441, 185, 391, 121, 81, 310, 310, 345, 290, 391, 294, 366, 294, 290, 290, 268, 366, 366, 402, 185, 183, 366, 145], [391, 330, 132, 399, 391, 345, 290, 267, 399, 391, 391, 345, 267, 10, 353, 391, 345, 290, 267, 367, 391, 391, 327, 391, 10, 345, 290, 391, 370, 10, 145], [63, 92, 364, 290, 312, 185, 63, 310, 391, 345, 290, 391, 63], [375, 348, 435, 186, 176, 310, 267, 40, 375, 348, 435, 176, 310, 267, 391, 391, 309, 230, 391, 312, 185, 183, 267, 391, 185, 230, 391], [267, 451, 348, 268, 290, 348, 149, 267, 451, 348, 375, 348, 268, 345, 290, 391, 348, 40, 391, 309, 230, 391, 185, 391, 230, 230, 267, 402, 391, 185, 391], [364, 391, 401, 290, 310, 312, 185, 402, 391, 391, 348, 401, 309, 348, 391, 345, 290, 310, 391, 345, 290, 310, 391, 230, 309, 230, 267, 185, 183, 391, 24, 391, 230], [435, 290, 208, 345, 267, 391, 63, 83, 290, 435, 290, 345, 267, 391, 13, 230, 13, 185, 267, 183, 391, 230, 230, 391, 185, 63, 83, 290, 391, 185, 327, 391, 63], [63, 83, 290, 345, 268, 310, 186, 290, 391, 267, 312, 185, 63, 83, 290, 345, 268, 310, 290, 391, 267, 93, 240, 267, 185, 183, 240, 185, 185, 185, 370, 399, 63], [403, 333, 435, 401, 345, 333, 441, 358, 154, 120, 120, 237, 237, 290, 237, 268, 237, 348, 294, 127, 237, 127, 11, 207, 19, 403, 236, 333, 435, 401, 345, 333, 362, 136, 333, 441, 349, 44, 425, 349, 425, 441, 333, 333, 251, 333, 275, 333, 333, 425, 337, 403, 236, 376, 358, 236, 155, 441, 313, 333, 263, 154, 333, 333, 236, 263, 333, 154, 126, 236, 337, 236, 337, 236, 263, 36, 441, 403, 403, 337, 333, 154, 403, 337, 294, 375, 348], [375, 348, 333, 447, 435, 176, 356, 356, 267, 238, 427, 267, 348, 375, 348, 333, 435, 176, 220, 251, 207, 220, 348, 403, 337, 357, 356, 251, 357, 356, 267, 249, 403, 337, 348, 419, 238, 267, 403, 337, 391, 364, 391, 155, 391, 155, 391, 411, 124, 211, 391, 411, 357, 356, 267, 364, 352, 345, 387, 402, 391, 244, 418, 152, 433, 357, 356, 267, 10, 256, 10, 256, 352, 345, 418, 256, 267, 403, 337, 267, 403, 337, 358, 155, 403, 333, 358, 333, 358, 19, 236, 459, 13, 333, 236, 358, 155, 333, 236, 306, 358, 244, 418, 211, 256, 10, 358, 337, 236, 155, 155, 314, 19, 236, 358, 413, 155, 337, 314, 364, 364, 306, 158, 370, 411, 391, 294, 370, 403, 337, 403, 337, 155, 370, 364, 306, 337, 403, 337, 337, 155, 236, 403, 337, 155, 441, 38, 124, 403, 337, 124, 294, 403, 91, 294, 403, 337, 306, 294, 403, 4, 322, 306, 18, 176, 18, 439, 348, 403, 337, 370, 420, 348, 420, 402, 402, 370, 295, 385, 268, 427, 267, 238, 295, 357, 356, 370], [403, 343, 191, 328, 127, 343, 336, 11, 11, 396, 11, 396, 396, 396, 11, 413, 11, 294, 11, 306, 11, 4, 11, 155, 370, 411, 38, 155, 11, 11, 370, 339, 91, 339, 370, 339, 370, 339, 403, 337, 337, 306, 294, 403, 343, 152, 339, 339, 91, 11, 152, 91, 314, 155, 403, 337, 403, 337, 294, 328, 306, 328, 362, 244, 328, 362, 207, 362, 207, 57, 57, 328, 402, 86, 11, 337, 294, 328, 57, 306, 370, 86, 343, 124, 294, 127, 322, 38, 322, 11, 337, 86, 343, 336, 370, 86, 314], [403, 333, 358, 120, 370, 11, 427, 154, 358, 403, 333, 263, 155, 333, 333, 358, 263, 402, 425, 333, 358, 152, 358, 120, 120, 358, 337, 403, 337, 19, 236, 413, 358, 337, 358, 154, 337, 19, 236, 459, 13, 337, 236, 337, 313, 155, 337, 337, 337, 370, 11, 403, 337, 376, 370, 337, 91, 155, 91, 441, 337, 337, 337, 124, 19, 236, 91, 419, 63, 370, 403, 370, 420, 427, 420, 402, 370, 370, 336, 370, 370, 279, 267, 279, 267, 36, 251, 403, 370, 306, 370, 403, 4, 376, 370, 433, 120, 71, 370, 424, 294, 439], [113, 393, 333, 358, 370, 370, 357, 356, 113, 113, 393, 393, 337, 238, 113, 393, 238, 249, 333, 358, 358, 403, 333, 358, 155, 333, 358, 387, 356, 333, 333, 333, 19, 236, 459, 13, 358, 236, 113, 251, 358, 370, 120, 370, 376, 358, 337, 236, 154, 154, 413, 337, 314, 236, 337, 236, 36, 337, 337, 236, 403, 337, 337, 91, 337, 337, 91, 236, 91, 337, 91, 337, 333, 337, 403, 294, 403, 337, 333, 38, 337, 337, 370, 403, 337, 38, 314, 403, 337, 337, 403, 294, 176, 439, 375, 348], [403, 333, 358, 10, 393, 336, 407, 25, 403, 358, 263, 13, 236, 403, 333, 358, 155, 333, 358, 393, 10, 358, 211, 10, 156, 142, 403, 337, 10, 399, 347, 343, 403, 337, 358, 19, 236, 155, 358, 393, 337, 403, 403, 337, 403, 337, 407, 25, 25, 403, 337, 25, 403, 337, 333, 407, 25, 337, 337, 25, 337, 169, 343, 418, 275, 267, 403, 337, 403, 337, 441, 38, 333, 25, 124, 337, 294, 403, 403, 337, 403, 337, 403, 337, 439, 176, 348], [337, 330, 132, 403, 333, 333, 435, 403, 263, 357, 356, 199, 113, 120, 333, 333, 435, 358, 357, 356, 294, 199, 120, 237, 353, 403, 333, 333, 435, 236, 358, 263, 151, 431, 191, 333, 263, 333, 263, 333, 353, 333, 295, 357, 356, 263, 431, 9, 339, 47, 113, 339, 11, 339, 120, 236, 236, 154, 376, 424, 337, 337, 337, 275, 425, 337, 305, 145], [403, 330, 132, 142, 401, 345, 333, 435, 333, 120, 343, 428, 375, 348, 439, 176, 142, 401, 345, 333, 435, 403, 333, 358, 120, 343, 399, 428, 399, 375, 348, 439, 176, 142, 142, 47, 211, 142, 220, 401, 345, 333, 435, 403, 435, 333, 333, 358, 263, 441, 403, 333, 263, 333, 358, 399, 358, 263, 403, 236, 399, 120, 263, 120, 399, 403, 333, 120, 399, 343, 399, 263, 343, 399, 428, 337, 294, 343, 399, 333, 263, 343, 399, 120, 294, 403, 294, 120, 403, 263, 337, 337, 370, 348, 439, 176, 145], [403, 337, 330, 403, 333, 357, 356, 379, 97, 336, 355, 120, 435, 401, 333, 357, 356, 379, 97, 336, 358, 120, 435, 401, 435, 401, 345, 333, 394, 333, 357, 356, 394, 333, 379, 97, 336, 394, 358, 355, 124, 358, 19, 236, 97, 336, 333, 355, 337, 126, 337, 337, 19, 236, 19, 236, 337, 126, 294, 120, 97, 337, 124, 358, 126, 358, 355, 357, 333, 337, 333, 358, 403, 145], [403, 330, 132, 333, 441, 401, 345, 333, 435, 428, 238, 389, 348, 199, 113, 238, 79, 409, 348, 120, 71, 11, 403, 263, 375, 348, 439, 176, 333, 441, 401, 345, 333, 435, 428, 238, 389, 348, 199, 113, 238, 79, 348, 120, 71, 11, 403, 355, 375, 348, 439, 176, 333, 441, 307, 333, 441, 401, 345, 435, 401, 333, 435, 333, 435, 124, 333, 333, 370, 428, 240, 249, 238, 389, 348, 79, 409, 348, 79, 370, 199, 113, 294, 249, 238, 434, 370, 294, 333, 403, 19, 236, 441, 333, 337, 358, 263, 191, 333, 263, 333, 263, 333, 376, 47, 263, 333, 339, 333, 236, 236, 376, 424, 337, 236, 333, 337, 337, 337, 425, 333, 337, 454, 236, 454, 337, 434, 337, 409, 79, 337, 337, 145], [337, 330, 132, 403, 333, 333, 435, 403, 263, 343, 116, 76, 120, 333, 333, 435, 358, 116, 76, 237, 120, 237, 353, 403, 333, 333, 435, 358, 263, 151, 431, 191, 333, 263, 333, 263, 333, 9, 339, 47, 343, 116, 76, 120, 236, 236, 154, 376, 424, 337, 337, 337, 275, 425, 337, 305, 145], [434, 337, 330, 132, 403, 333, 333, 435, 403, 263, 11, 120, 33, 290, 356, 333, 333, 435, 358, 11, 120, 237, 33, 290, 237, 353, 403, 333, 333, 435, 358, 263, 151, 431, 191, 333, 263, 387, 356, 333, 263, 333, 263, 431, 9, 339, 47, 33, 290, 120, 370, 236, 236, 154, 376, 424, 337, 337, 337, 275, 425, 337, 305, 145], [434, 403, 132, 333, 435, 401, 345, 333, 333, 267, 360, 256, 375, 348, 310, 290, 120, 71, 267, 161, 310, 345, 290, 198, 441, 308, 11, 11, 120, 228, 409, 416, 33, 290, 71, 439, 176, 333, 435, 401, 345, 333, 333, 358, 267, 360, 256, 348, 310, 290, 120, 393, 71, 393, 267, 310, 345, 290, 198, 441, 308, 393, 370, 370, 120, 393, 228, 393, 416, 399, 33, 393, 71, 393, 176, 375, 348, 307, 333, 435, 401, 345, 445, 44, 345, 401, 307, 403, 333, 333, 333, 435, 251, 403, 333, 333, 249, 435, 251, 360, 434, 337, 357, 267, 267, 360, 256, 375, 348, 310, 290, 256, 207, 358, 263, 403, 236, 403, 333, 263, 441, 155, 333, 155, 333, 211, 46, 337, 263, 120, 71, 256, 155, 433, 263, 19, 236, 337, 403, 337, 403, 337, 423, 403, 294, 308, 407, 337, 267, 307, 161, 310, 345, 290, 249, 198, 441, 249, 308, 393, 25, 155, 308, 393, 267, 275, 425, 358, 263, 403, 236, 403, 333, 263, 155, 333, 211, 46, 337, 263, 308, 407, 11, 155, 433, 263, 19, 236, 337, 403, 337, 403, 337, 423, 403, 294, 120, 228, 337, 358, 263, 403, 236, 333, 263, 155, 333, 211, 46, 337, 263, 11, 120, 228, 155, 154, 263, 19, 236, 337, 403, 337, 403, 337, 423, 403, 294, 409, 416, 337, 358, 263, 403, 236, 333, 263, 155, 333, 211, 46, 337, 337, 409, 416, 33, 290, 71, 155, 154, 263, 19, 236, 337, 403, 337, 403, 337, 423, 403, 294, 403, 439, 176, 348, 145], [333, 333, 435, 401, 345, 333, 441, 358, 154, 120, 120, 237, 11, 207, 375, 348, 19, 403, 236, 337, 333, 435, 401, 345, 333, 441, 136, 333, 441, 349, 44, 333, 425, 349, 425, 333, 441, 333, 333, 251, 333, 275, 358, 333, 333, 425, 337, 403, 236, 376, 358, 236, 155, 441, 313, 333, 263, 154, 333, 333, 236, 263, 333, 154, 126, 236, 337, 236, 337, 236, 263, 36, 441, 403, 403, 275, 425, 333, 154, 403, 337, 294, 375, 348], [361, 407, 357, 356, 345, 267, 361, 379, 348, 238, 348, 333, 447, 435, 207, 361, 407, 445, 161, 198, 25, 445, 357, 356, 345, 207, 25, 155, 161, 169, 267, 267, 349, 267, 361, 25, 169, 136, 275, 425, 358, 387, 345, 361, 169, 379, 348, 445, 238, 389, 333, 447, 435, 207, 389, 361, 379, 348], [333, 403, 333, 333, 447, 435, 401, 238, 238, 333, 447, 435, 375, 348, 403, 13, 355, 358, 154, 399, 343, 428, 290, 11, 385, 268, 403, 439, 176, 375, 348, 337, 333, 358, 236, 441, 191, 333, 211, 238, 211, 238, 333, 154, 238, 154, 428, 120, 337, 236, 333, 314, 337, 333, 57, 441, 337, 454, 236, 337, 403, 370, 91, 403, 211, 343, 91, 211, 91, 333, 313, 313, 333, 156, 91, 403, 155, 91, 18, 419, 419, 294, 343, 18, 333, 419, 18, 91, 18], [358, 333, 403, 343, 156, 116, 76, 399, 348, 355, 403, 355, 211, 403, 333, 333, 333, 333, 403, 294, 343, 294, 156, 337, 156, 156, 403, 343, 403, 337, 152, 294, 399, 333, 11, 116, 76, 337, 337, 19, 236, 275, 454, 403, 337, 294, 403, 337, 375, 348, 176], [333, 347, 343, 11, 71, 10, 127, 336, 124, 294, 127, 358, 124, 124, 425, 244, 366, 425, 366, 433, 366, 47, 433, 125, 358, 124, 47, 10, 441, 352, 345, 10, 441, 71, 71, 124, 294, 294, 399, 402, 370, 370, 370, 399, 124, 370, 370, 399, 11, 11, 158, 11, 294, 347, 343, 124, 294, 343, 294, 124, 337, 294, 263, 236, 441, 435, 333, 263, 36, 439, 10, 36, 370, 71, 336, 337, 337, 36, 279, 333, 358, 337, 358, 358, 294, 337, 294, 337, 279, 294, 145], [113, 63, 176, 11, 403, 333, 358, 113, 113, 213, 155, 113, 240, 213, 240, 113, 213, 294, 113, 240, 353, 213, 352, 345, 213, 345, 290, 63, 63, 113, 240, 418, 418, 113, 176, 362, 220, 251, 11, 356, 356, 11, 370, 124, 38, 207, 370, 403, 337, 358, 403, 333, 358, 155, 333, 19, 236, 459, 13, 358, 236, 333, 176, 399, 358, 113, 213, 176, 413, 19, 236, 358, 314, 155, 403, 337, 236, 306, 124, 370, 403, 337, 306, 155, 11, 370, 403, 337, 38, 155, 337, 125, 403, 337, 236, 91, 403, 337, 236, 155, 11, 337, 236, 11, 337, 314, 11, 337, 236, 91, 403, 337, 294, 337, 294, 236, 370, 91, 370, 4, 322, 379, 238, 370, 336], [73, 421, 405, 330, 132, 73, 217, 270, 81, 310, 345, 290, 73, 81, 310, 345, 290, 73, 73, 345, 73, 425, 73, 249, 207, 290, 270, 310, 251, 145], [79, 405, 330, 222, 79, 33, 290, 33, 290, 33, 290, 268, 267, 81, 310, 121, 345, 290, 110, 421, 116, 170, 79, 33, 290, 33, 290, 33, 290, 268, 267, 81, 310, 121, 345, 290, 110, 421, 116, 367, 267, 422, 268, 33, 290, 345, 81, 310, 121, 290, 350, 268, 290, 79, 422, 251, 421, 251, 421, 116, 145], [88, 217, 79, 405, 330, 79, 116, 88, 267, 345, 110, 421, 170, 116, 217, 88, 92, 267, 345, 292, 344, 110, 421, 367, 267, 422, 173, 350, 79, 345, 116, 207, 249, 79, 251, 88, 251, 421, 251, 421, 292, 145], [308, 405, 330, 132, 308, 116, 173, 121, 345, 270, 110, 421, 267, 116, 344, 76, 170, 308, 116, 92, 121, 345, 270, 110, 421, 267, 116, 344, 88, 308, 308, 308, 249, 116, 173, 121, 345, 290, 270, 421, 425, 425, 421, 308, 251, 421, 421, 267, 308, 405, 267, 267, 425, 145], [129, 308, 405, 330, 132, 308, 63, 173, 116, 345, 290, 421, 76, 110, 405, 359, 207, 170, 308, 63, 92, 116, 290, 421, 76, 110, 405, 359, 207, 292, 308, 173, 63, 116, 308, 251, 251, 308, 345, 290, 129, 405, 359, 421, 76, 76, 110, 405, 359, 421, 421, 129, 405, 308, 207, 129, 405, 292, 145], [405, 88, 348, 330, 222, 361, 310, 270, 121, 290, 345, 366, 267, 110, 421, 116, 88, 345, 207, 65, 65, 88, 170, 310, 121, 345, 366, 267, 110, 421, 116, 217, 345, 207, 65, 65, 361, 310, 270, 121, 290, 345, 422, 366, 361, 366, 361, 366, 361, 361, 366, 267, 267, 366, 405, 88, 348, 65, 65, 116, 251, 65, 348, 405, 405, 110, 421, 65, 251, 361, 88, 348, 88, 207, 145], [79, 405, 330, 132, 79, 268, 81, 310, 345, 268, 88, 110, 421, 170, 268, 92, 81, 310, 345, 268, 88, 110, 421, 367, 267, 422, 268, 345, 350, 81, 310, 79, 249, 79, 251, 421, 251, 421, 268, 88, 145], [79, 405, 330, 222, 79, 268, 270, 186, 121, 290, 345, 110, 421, 170, 79, 268, 92, 186, 121, 290, 345, 110, 367, 267, 422, 268, 345, 350, 270, 121, 79, 79, 251, 251, 421, 251, 421, 145], [142, 458, 401, 234, 142, 446, 116, 98, 209, 95, 310, 28, 142, 458, 401, 234, 76, 142, 446, 116, 401, 98, 209, 98, 251, 209, 211, 211, 251, 209, 211, 251, 387, 95], [398, 102, 401, 234, 116, 219, 209, 310, 398, 34, 34, 398, 102, 76, 116, 401, 219, 28, 250, 378, 209, 146, 98, 383, 251, 209, 211, 398, 209, 146, 251, 398, 209, 116, 401, 28, 250, 378, 116, 211, 209, 95, 116, 398, 309, 398], [142, 458, 401, 116, 432, 234, 98, 95, 310, 142, 458, 401, 251, 116, 432, 286, 234, 251, 98, 209, 98, 251, 209, 47, 234, 251, 209, 116, 211, 387, 95], [142, 458, 401, 116, 209, 98, 95, 310, 84, 142, 458, 401, 441, 444, 458, 234, 458, 28, 444, 116, 286, 251, 47, 98, 219, 116, 209, 98, 219, 211, 95, 84, 122, 84, 122, 84], [401, 142, 146, 401, 209, 310, 28, 401, 142, 458, 250, 142, 251, 28, 28, 142, 446, 401, 250, 286, 142, 446, 234, 251, 309, 146, 209, 146, 209, 383, 234, 251, 209, 211, 95, 310, 209, 146, 234, 251, 95, 310, 209, 146, 234, 251], [98, 219, 116, 76, 116, 209, 95, 310, 98, 116, 76, 98, 251, 116, 432, 286, 116, 116, 76, 209, 47, 98, 251, 116, 251, 211, 387, 95], [67, 67, 98, 310, 98, 98, 102, 401, 98, 116, 102, 401, 98, 95, 310, 84, 67, 25, 98, 25, 67, 98, 98, 219, 250, 378, 234, 102, 401, 219, 28, 250, 378, 116, 102, 401, 219, 116, 251, 251, 67, 67, 211, 67, 98, 251, 67, 251, 98, 67, 211, 67, 98, 251, 211, 383, 67, 211, 67, 309, 98, 251, 383, 67, 95, 67, 122, 84, 122, 84], [432, 437, 98, 441, 116, 76, 401, 116, 142, 444, 95, 310, 383, 437, 47, 98, 441, 437, 98, 28, 116, 76, 401, 250, 116, 142, 378, 25, 98, 441, 25, 437, 437, 98, 25, 444, 95, 310], [56, 438, 330, 56, 249, 432, 116, 69, 348, 56, 249, 432, 116, 69, 348, 56, 25, 438, 202, 56, 432, 116, 69, 348, 145], [438, 330, 51, 142, 345, 290, 382, 142, 345, 290, 382, 142, 345, 290, 249, 438, 202, 382, 142, 142, 142, 438, 202, 145], [79, 51, 404, 345, 290, 290, 142, 246, 432, 267, 161, 310, 79, 294, 345, 142, 246, 432, 267, 310, 401, 404, 394, 79, 345, 290, 142, 246, 432, 267, 161, 310, 401, 345, 394, 79, 438, 25, 438, 202, 294, 79, 25, 79, 438, 55, 25, 348, 145], [89, 337, 438, 89, 337, 135, 89, 337, 135, 89, 337, 438, 202, 55, 145], [273, 249, 330, 51, 266, 246, 142, 432, 310, 345, 404, 401, 266, 246, 142, 432, 310, 345, 404, 266, 246, 142, 432, 266, 310, 345, 273, 25, 273, 438, 25, 438, 202, 438, 202, 273, 438, 404, 145], [438, 330, 51, 49, 142, 370, 49, 142, 370, 370, 370, 49, 438, 202, 142, 142, 76, 370, 49, 49, 55, 145], [254, 438, 254, 76, 161, 142, 382, 231, 348, 246, 254, 142, 118, 382, 231, 348, 438, 254, 370, 370, 161, 142, 118, 254, 161, 142, 49, 118, 254, 254, 438, 202, 382, 254, 438, 202, 49, 118, 55, 76, 231, 145], [438, 51, 249, 310, 267, 142, 288, 116, 401, 116, 310, 267, 142, 288, 116, 55, 401, 116, 345, 249, 142, 267, 310, 394, 69, 348, 438, 202, 249, 202, 288, 69, 249, 438, 202, 438, 288, 69, 116, 145]]\n"
     ]
    }
   ],
   "source": [
    "text_data = text_to_numbers(preprocessed_texts, word_dictionary)\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[428, 333, 403, 337, 347, 391, 185, 348, 116, 77, 300, 210, 190]\n"
     ]
    }
   ],
   "source": [
    "valid_words = ['tuna', 'rice', 'sushi', 'roll', 'sashimi','steak','grill', 'sauce', 'cream',\n",
    "               'cheesecake', 'pizza', 'lasagna', 'hamburger']\n",
    "\n",
    "valid_examples = [word_dictionary[x] for x in valid_words if x in word_dictionary.keys()]\n",
    "print(valid_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['onion' '154']\n",
      " ['onion' '154']\n",
      " ['onion' '154']\n",
      " ['onion' '154']\n",
      " ['onion' '154']\n",
      " ['onion' '154']\n",
      " ['onion' '154']\n",
      " ['onion' '154']]\n",
      "['mac' 'onion' 'yeast' 'powder' 'onion' 'recipe' 'onion' 'onion']\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "batch_data, label_data = create_batch_data(preprocessed_texts)\n",
    "print(batch_data)\n",
    "print(label_data)\n",
    "print(np.shape(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "WARNING:tensorflow:From <ipython-input-10-847f449e0dc6>:30: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Starting Training\n",
      "Loss at step 50 : 6.338644981384277\n",
      "Loss at step 100 : 6.314270496368408\n",
      "Loss at step 150 : 6.182187557220459\n",
      "Loss at step 200 : 5.921782493591309\n",
      "Loss at step 250 : 5.674522399902344\n",
      "Loss at step 300 : 6.22251033782959\n",
      "Loss at step 350 : 6.028868675231934\n",
      "Loss at step 400 : 5.887228012084961\n",
      "Loss at step 450 : 5.897741794586182\n",
      "Loss at step 500 : 6.332916259765625\n",
      "Loss at step 550 : 5.957516670227051\n",
      "Loss at step 600 : 5.660001277923584\n",
      "Loss at step 650 : 6.5707621574401855\n",
      "Loss at step 700 : 5.612837791442871\n",
      "Loss at step 750 : 6.297659873962402\n",
      "Loss at step 800 : 5.873055934906006\n",
      "Loss at step 850 : 5.944061756134033\n",
      "Loss at step 900 : 5.964816093444824\n",
      "Loss at step 950 : 5.920680999755859\n",
      "Loss at step 1000 : 5.87056827545166\n",
      "Loss at step 1050 : 5.650750637054443\n",
      "Loss at step 1100 : 5.806759834289551\n",
      "Loss at step 1150 : 6.009033203125\n",
      "Loss at step 1200 : 6.252737045288086\n",
      "Loss at step 1250 : 6.04218864440918\n",
      "Loss at step 1300 : 5.727919578552246\n",
      "Loss at step 1350 : 5.65419864654541\n",
      "Loss at step 1400 : 6.135430812835693\n",
      "Loss at step 1450 : 5.84287166595459\n",
      "Loss at step 1500 : 5.502551078796387\n",
      "Loss at step 1550 : 5.916476249694824\n",
      "Loss at step 1600 : 5.382387638092041\n",
      "Loss at step 1650 : 6.006305694580078\n",
      "Loss at step 1700 : 6.043030738830566\n",
      "Loss at step 1750 : 6.082204341888428\n",
      "Loss at step 1800 : 5.869243621826172\n",
      "Loss at step 1850 : 5.7841949462890625\n",
      "Loss at step 1900 : 5.986805438995361\n",
      "Loss at step 1950 : 5.454965591430664\n",
      "Loss at step 2000 : 5.815868377685547\n",
      "Loss at step 2050 : 6.075571060180664\n",
      "Loss at step 2100 : 5.539231300354004\n",
      "Loss at step 2150 : 5.775448799133301\n",
      "Loss at step 2200 : 5.594876289367676\n",
      "Loss at step 2250 : 5.665463447570801\n",
      "Loss at step 2300 : 5.5034637451171875\n",
      "Loss at step 2350 : 5.650018215179443\n",
      "Loss at step 2400 : 5.562380313873291\n",
      "Loss at step 2450 : 5.206878662109375\n",
      "Loss at step 2500 : 5.615234375\n",
      "Loss at step 2550 : 5.938831806182861\n",
      "Loss at step 2600 : 5.379823207855225\n",
      "Loss at step 2650 : 5.537725448608398\n",
      "Loss at step 2700 : 5.243569850921631\n",
      "Loss at step 2750 : 5.769681453704834\n",
      "Loss at step 2800 : 5.47243595123291\n",
      "Loss at step 2850 : 5.547982215881348\n",
      "Loss at step 2900 : 5.721383094787598\n",
      "Loss at step 2950 : 6.180791854858398\n",
      "Loss at step 3000 : 5.573712348937988\n",
      "Loss at step 3050 : 5.179000377655029\n",
      "Loss at step 3100 : 5.803227424621582\n",
      "Loss at step 3150 : 4.504123210906982\n",
      "Loss at step 3200 : 5.446734428405762\n",
      "Loss at step 3250 : 5.2240777015686035\n",
      "Loss at step 3300 : 5.414717674255371\n",
      "Loss at step 3350 : 5.674916744232178\n",
      "Loss at step 3400 : 5.199489593505859\n",
      "Loss at step 3450 : 5.769461154937744\n",
      "Loss at step 3500 : 5.343071937561035\n",
      "Loss at step 3550 : 5.199354648590088\n",
      "Loss at step 3600 : 5.440887451171875\n",
      "Loss at step 3650 : 5.735351085662842\n",
      "Loss at step 3700 : 6.208381652832031\n",
      "Loss at step 3750 : 5.667728424072266\n",
      "Loss at step 3800 : 5.808809280395508\n",
      "Loss at step 3850 : 5.549046516418457\n",
      "Loss at step 3900 : 5.443222522735596\n",
      "Loss at step 3950 : 5.015462398529053\n",
      "Loss at step 4000 : 5.827696800231934\n",
      "Loss at step 4050 : 5.01312780380249\n",
      "Loss at step 4100 : 5.982700824737549\n",
      "Loss at step 4150 : 5.230228424072266\n",
      "Loss at step 4200 : 5.227523326873779\n",
      "Loss at step 4250 : 5.402984619140625\n",
      "Loss at step 4300 : 5.64977502822876\n",
      "Loss at step 4350 : 5.489840984344482\n",
      "Loss at step 4400 : 5.703413486480713\n",
      "Loss at step 4450 : 5.043516159057617\n",
      "Loss at step 4500 : 4.958731651306152\n",
      "Loss at step 4550 : 4.702289581298828\n",
      "Loss at step 4600 : 5.5231614112854\n",
      "Loss at step 4650 : 4.608626365661621\n",
      "Loss at step 4700 : 5.162557601928711\n",
      "Loss at step 4750 : 5.229846000671387\n",
      "Loss at step 4800 : 5.118760108947754\n",
      "Loss at step 4850 : 5.443935394287109\n",
      "Loss at step 4900 : 4.985075950622559\n",
      "Loss at step 4950 : 4.840667247772217\n",
      "Loss at step 5000 : 5.6992340087890625\n",
      "Nearest to tuna: virgin, metal, nibble, parmesan, beef,\n",
      "Nearest to rice: mandu, crab, thigh, ham, spinach,\n",
      "Nearest to sushi: circle, cornstarch, mandu, sashimi, wedge,\n",
      "Nearest to roll: worcestershire, oil, jam, liqueur, vanilla,\n",
      "Nearest to sashimi: wing, ring, fryer, cheese, gyoza,\n",
      "Nearest to steak: arrange, paste, eye, core, stuffed,\n",
      "Nearest to grill: paper, soy, mesh, dessert, taco,\n",
      "Nearest to sauce: roe, syrup, jack, peel, position,\n",
      "Nearest to cream: dente, skinless, sieve, decker, toast,\n",
      "Nearest to cheesecake: fish, paper, mesh, appetizer, flour,\n",
      "Nearest to pizza: white, air, rice, cabbage, packet,\n",
      "Nearest to lasagna: fry, wrap, rare, boil, almond,\n",
      "Nearest to hamburger: ½, ramekin, total, texture, wrapper,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 5050 : 5.158350467681885\n",
      "Loss at step 5100 : 5.052280902862549\n",
      "Loss at step 5150 : 5.057024955749512\n",
      "Loss at step 5200 : 5.0706892013549805\n",
      "Loss at step 5250 : 5.812776565551758\n",
      "Loss at step 5300 : 4.180526256561279\n",
      "Loss at step 5350 : 5.237442970275879\n",
      "Loss at step 5400 : 4.438122272491455\n",
      "Loss at step 5450 : 5.107611656188965\n",
      "Loss at step 5500 : 5.062841415405273\n",
      "Loss at step 5550 : 5.6735358238220215\n",
      "Loss at step 5600 : 5.490284442901611\n",
      "Loss at step 5650 : 4.469895362854004\n",
      "Loss at step 5700 : 4.21767520904541\n",
      "Loss at step 5750 : 4.477240085601807\n",
      "Loss at step 5800 : 5.01267147064209\n",
      "Loss at step 5850 : 4.8702592849731445\n",
      "Loss at step 5900 : 5.371985912322998\n",
      "Loss at step 5950 : 4.952653884887695\n",
      "Loss at step 6000 : 5.093881607055664\n",
      "Loss at step 6050 : 5.752554416656494\n",
      "Loss at step 6100 : 5.029236793518066\n",
      "Loss at step 6150 : 5.537779331207275\n",
      "Loss at step 6200 : 5.2955732345581055\n",
      "Loss at step 6250 : 4.894110679626465\n",
      "Loss at step 6300 : 5.004203796386719\n",
      "Loss at step 6350 : 5.126608371734619\n",
      "Loss at step 6400 : 5.234165191650391\n",
      "Loss at step 6450 : 4.8672966957092285\n",
      "Loss at step 6500 : 5.4888505935668945\n",
      "Loss at step 6550 : 5.508605480194092\n",
      "Loss at step 6600 : 6.1060895919799805\n",
      "Loss at step 6650 : 5.475468158721924\n",
      "Loss at step 6700 : 5.005967617034912\n",
      "Loss at step 6750 : 4.3967180252075195\n",
      "Loss at step 6800 : 4.468968868255615\n",
      "Loss at step 6850 : 4.8720197677612305\n",
      "Loss at step 6900 : 4.221194267272949\n",
      "Loss at step 6950 : 5.042136192321777\n",
      "Loss at step 7000 : 4.9819560050964355\n",
      "Loss at step 7050 : 4.877570152282715\n",
      "Loss at step 7100 : 4.657696723937988\n",
      "Loss at step 7150 : 4.7080793380737305\n",
      "Loss at step 7200 : 3.782388687133789\n",
      "Loss at step 7250 : 4.198647499084473\n",
      "Loss at step 7300 : 5.068709373474121\n",
      "Loss at step 7350 : 4.630745887756348\n",
      "Loss at step 7400 : 4.576676368713379\n",
      "Loss at step 7450 : 4.755497455596924\n",
      "Loss at step 7500 : 4.9928789138793945\n",
      "Loss at step 7550 : 5.424724578857422\n",
      "Loss at step 7600 : 6.073704719543457\n",
      "Loss at step 7650 : 3.9903998374938965\n",
      "Loss at step 7700 : 4.4271626472473145\n",
      "Loss at step 7750 : 5.278570175170898\n",
      "Loss at step 7800 : 3.2237842082977295\n",
      "Loss at step 7850 : 4.953693389892578\n",
      "Loss at step 7900 : 4.680973052978516\n",
      "Loss at step 7950 : 5.16163444519043\n",
      "Loss at step 8000 : 4.671219348907471\n",
      "Loss at step 8050 : 4.471757411956787\n",
      "Loss at step 8100 : 4.142660617828369\n",
      "Loss at step 8150 : 5.466660022735596\n",
      "Loss at step 8200 : 4.712635040283203\n",
      "Loss at step 8250 : 4.227383613586426\n",
      "Loss at step 8300 : 4.079830169677734\n",
      "Loss at step 8350 : 4.6147685050964355\n",
      "Loss at step 8400 : 4.315548419952393\n",
      "Loss at step 8450 : 4.939935684204102\n",
      "Loss at step 8500 : 4.052008152008057\n",
      "Loss at step 8550 : 5.441673755645752\n",
      "Loss at step 8600 : 4.688854694366455\n",
      "Loss at step 8650 : 5.160519599914551\n",
      "Loss at step 8700 : 5.892295837402344\n",
      "Loss at step 8750 : 4.792814254760742\n",
      "Loss at step 8800 : 4.70473575592041\n",
      "Loss at step 8850 : 4.93067741394043\n",
      "Loss at step 8900 : 4.250317573547363\n",
      "Loss at step 8950 : 4.5341410636901855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 9000 : 5.105460166931152\n",
      "Loss at step 9050 : 4.978597640991211\n",
      "Loss at step 9100 : 4.760387420654297\n",
      "Loss at step 9150 : 4.079474449157715\n",
      "Loss at step 9200 : 4.113181114196777\n",
      "Loss at step 9250 : 4.404670715332031\n",
      "Loss at step 9300 : 4.885677814483643\n",
      "Loss at step 9350 : 3.4827756881713867\n",
      "Loss at step 9400 : 4.3529510498046875\n",
      "Loss at step 9450 : 5.561936378479004\n",
      "Loss at step 9500 : 5.189963340759277\n",
      "Loss at step 9550 : 5.380207061767578\n",
      "Loss at step 9600 : 4.223998069763184\n",
      "Loss at step 9650 : 3.7235617637634277\n",
      "Loss at step 9700 : 3.2066707611083984\n",
      "Loss at step 9750 : 4.291766166687012\n",
      "Loss at step 9800 : 4.555448055267334\n",
      "Loss at step 9850 : 3.640852928161621\n",
      "Loss at step 9900 : 4.270608901977539\n",
      "Loss at step 9950 : 4.952080249786377\n",
      "Loss at step 10000 : 3.5648655891418457\n",
      "Nearest to tuna: virgin, metal, parmesan, beef, nibble,\n",
      "Nearest to rice: mandu, thigh, crab, spinach, glaze,\n",
      "Nearest to sushi: circle, cornstarch, cake, mandu, sashimi,\n",
      "Nearest to roll: worcestershire, oil, sheet, vanilla, jam,\n",
      "Nearest to sashimi: wing, ring, fryer, cheese, gyoza,\n",
      "Nearest to steak: arrange, maple, paste, eye, core,\n",
      "Nearest to grill: paper, soy, chicken, taco, mesh,\n",
      "Nearest to sauce: jack, beet, roe, syrup, position,\n",
      "Nearest to cream: skinless, dente, toast, sieve, preheat,\n",
      "Nearest to cheesecake: fish, paper, mesh, sieve, jasmine,\n",
      "Nearest to pizza: white, coffee, air, packet, scallion,\n",
      "Nearest to lasagna: fry, rare, wrap, almond, teriyaki,\n",
      "Nearest to hamburger: ½, ramekin, total, coloring, texture,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 10050 : 4.709687232971191\n",
      "Loss at step 10100 : 4.471445083618164\n",
      "Loss at step 10150 : 4.916928291320801\n",
      "Loss at step 10200 : 4.526322364807129\n",
      "Loss at step 10250 : 4.495281219482422\n",
      "Loss at step 10300 : 6.08122444152832\n",
      "Loss at step 10350 : 4.69365119934082\n",
      "Loss at step 10400 : 3.8943042755126953\n",
      "Loss at step 10450 : 6.192110061645508\n",
      "Loss at step 10500 : 5.513217926025391\n",
      "Loss at step 10550 : 4.146463394165039\n",
      "Loss at step 10600 : 4.877287864685059\n",
      "Loss at step 10650 : 4.2229204177856445\n",
      "Loss at step 10700 : 4.53777551651001\n",
      "Loss at step 10750 : 3.9119038581848145\n",
      "Loss at step 10800 : 4.880900859832764\n",
      "Loss at step 10850 : 5.147336959838867\n",
      "Loss at step 10900 : 4.306541442871094\n",
      "Loss at step 10950 : 3.9194717407226562\n",
      "Loss at step 11000 : 5.411698341369629\n",
      "Loss at step 11050 : 3.876211643218994\n",
      "Loss at step 11100 : 4.462442398071289\n",
      "Loss at step 11150 : 3.6028971672058105\n",
      "Loss at step 11200 : 3.990790843963623\n",
      "Loss at step 11250 : 5.2787017822265625\n",
      "Loss at step 11300 : 4.764281272888184\n",
      "Loss at step 11350 : 3.2172980308532715\n",
      "Loss at step 11400 : 3.5767648220062256\n",
      "Loss at step 11450 : 3.6930527687072754\n",
      "Loss at step 11500 : 4.917943954467773\n",
      "Loss at step 11550 : 4.822709083557129\n",
      "Loss at step 11600 : 4.364163398742676\n",
      "Loss at step 11650 : 4.533634185791016\n",
      "Loss at step 11700 : 4.859597682952881\n",
      "Loss at step 11750 : 3.8000714778900146\n",
      "Loss at step 11800 : 4.880365371704102\n",
      "Loss at step 11850 : 4.640053749084473\n",
      "Loss at step 11900 : 3.0634217262268066\n",
      "Loss at step 11950 : 4.57504940032959\n",
      "Loss at step 12000 : 4.422436714172363\n",
      "Loss at step 12050 : 5.498793601989746\n",
      "Loss at step 12100 : 4.491525650024414\n",
      "Loss at step 12150 : 4.560162544250488\n",
      "Loss at step 12200 : 4.6899943351745605\n",
      "Loss at step 12250 : 4.372060775756836\n",
      "Loss at step 12300 : 4.142169952392578\n",
      "Loss at step 12350 : 4.778163433074951\n",
      "Loss at step 12400 : 4.691860198974609\n",
      "Loss at step 12450 : 3.1933679580688477\n",
      "Loss at step 12500 : 4.612513542175293\n",
      "Loss at step 12550 : 4.542918682098389\n",
      "Loss at step 12600 : 4.87106990814209\n",
      "Loss at step 12650 : 4.182438373565674\n",
      "Loss at step 12700 : 5.1617631912231445\n",
      "Loss at step 12750 : 3.16221022605896\n",
      "Loss at step 12800 : 4.460988998413086\n",
      "Loss at step 12850 : 4.724301338195801\n",
      "Loss at step 12900 : 4.543811321258545\n",
      "Loss at step 12950 : 4.720111846923828\n",
      "Loss at step 13000 : 3.32855486869812\n",
      "Loss at step 13050 : 3.9178826808929443\n",
      "Loss at step 13100 : 4.019387245178223\n",
      "Loss at step 13150 : 4.311707019805908\n",
      "Loss at step 13200 : 4.462583065032959\n",
      "Loss at step 13250 : 4.776444911956787\n",
      "Loss at step 13300 : 4.934264183044434\n",
      "Loss at step 13350 : 4.189798355102539\n",
      "Loss at step 13400 : 4.945371627807617\n",
      "Loss at step 13450 : 4.142851829528809\n",
      "Loss at step 13500 : 4.233345985412598\n",
      "Loss at step 13550 : 3.187406301498413\n",
      "Loss at step 13600 : 4.131765365600586\n",
      "Loss at step 13650 : 3.9569759368896484\n",
      "Loss at step 13700 : 4.5065507888793945\n",
      "Loss at step 13750 : 4.654189109802246\n",
      "Loss at step 13800 : 3.749471664428711\n",
      "Loss at step 13850 : 4.296852111816406\n",
      "Loss at step 13900 : 3.6187782287597656\n",
      "Loss at step 13950 : 4.4265456199646\n",
      "Loss at step 14000 : 5.326732635498047\n",
      "Loss at step 14050 : 3.1419880390167236\n",
      "Loss at step 14100 : 3.8060836791992188\n",
      "Loss at step 14150 : 4.862794876098633\n",
      "Loss at step 14200 : 4.831157207489014\n",
      "Loss at step 14250 : 4.378565788269043\n",
      "Loss at step 14300 : 3.5913162231445312\n",
      "Loss at step 14350 : 4.170655250549316\n",
      "Loss at step 14400 : 4.341998100280762\n",
      "Loss at step 14450 : 4.012430191040039\n",
      "Loss at step 14500 : 3.7824888229370117\n",
      "Loss at step 14550 : 4.078846454620361\n",
      "Loss at step 14600 : 4.457386016845703\n",
      "Loss at step 14650 : 4.410825252532959\n",
      "Loss at step 14700 : 5.467713356018066\n",
      "Loss at step 14750 : 3.6684694290161133\n",
      "Loss at step 14800 : 5.023139476776123\n",
      "Loss at step 14850 : 4.507535457611084\n",
      "Loss at step 14900 : 3.65902042388916\n",
      "Loss at step 14950 : 3.7509331703186035\n",
      "Loss at step 15000 : 4.320160865783691\n",
      "Nearest to tuna: virgin, metal, parmesan, beef, eye,\n",
      "Nearest to rice: thigh, mandu, spinach, lunch, crab,\n",
      "Nearest to sushi: cornstarch, mandu, circle, cut, cake,\n",
      "Nearest to roll: worcestershire, oil, sheet, liqueur, jam,\n",
      "Nearest to sashimi: wing, ring, fryer, cheese, seaweed,\n",
      "Nearest to steak: maple, paste, eye, arrange, beef,\n",
      "Nearest to grill: paper, chicken, soy, mesh, taco,\n",
      "Nearest to sauce: jack, corn, wasabi, roe, beet,\n",
      "Nearest to cream: skinless, dente, toast, dunk, preheat,\n",
      "Nearest to cheesecake: fish, mesh, paper, cashew, sieve,\n",
      "Nearest to pizza: white, scallion, flake, packet, coffee,\n",
      "Nearest to lasagna: fry, rare, wrap, almond, teriyaki,\n",
      "Nearest to hamburger: ½, total, coloring, ramekin, granola,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 15050 : 3.8617639541625977\n",
      "Loss at step 15100 : 3.6359846591949463\n",
      "Loss at step 15150 : 4.211096286773682\n",
      "Loss at step 15200 : 4.0325493812561035\n",
      "Loss at step 15250 : 5.308985233306885\n",
      "Loss at step 15300 : 4.0713300704956055\n",
      "Loss at step 15350 : 5.11189079284668\n",
      "Loss at step 15400 : 3.7057840824127197\n",
      "Loss at step 15450 : 3.346813678741455\n",
      "Loss at step 15500 : 3.806694984436035\n",
      "Loss at step 15550 : 3.1118927001953125\n",
      "Loss at step 15600 : 4.770751953125\n",
      "Loss at step 15650 : 3.4620680809020996\n",
      "Loss at step 15700 : 4.065710067749023\n",
      "Loss at step 15750 : 4.123228073120117\n",
      "Loss at step 15800 : 4.698098182678223\n",
      "Loss at step 15850 : 5.205018043518066\n",
      "Loss at step 15900 : 4.935872554779053\n",
      "Loss at step 15950 : 3.896620512008667\n",
      "Loss at step 16000 : 4.323736190795898\n",
      "Loss at step 16050 : 4.489402770996094\n",
      "Loss at step 16100 : 3.2352867126464844\n",
      "Loss at step 16150 : 4.499467849731445\n",
      "Loss at step 16200 : 4.2229790687561035\n",
      "Loss at step 16250 : 4.0754499435424805\n",
      "Loss at step 16300 : 1.9816430807113647\n",
      "Loss at step 16350 : 3.4070072174072266\n",
      "Loss at step 16400 : 3.670290470123291\n",
      "Loss at step 16450 : 4.277584075927734\n",
      "Loss at step 16500 : 3.1504323482513428\n",
      "Loss at step 16550 : 3.6897201538085938\n",
      "Loss at step 16600 : 4.260245323181152\n",
      "Loss at step 16650 : 5.174295425415039\n",
      "Loss at step 16700 : 3.8159303665161133\n",
      "Loss at step 16750 : 4.13323974609375\n",
      "Loss at step 16800 : 4.271459102630615\n",
      "Loss at step 16850 : 4.336482048034668\n",
      "Loss at step 16900 : 3.9302618503570557\n",
      "Loss at step 16950 : 3.7908623218536377\n",
      "Loss at step 17000 : 4.001175880432129\n",
      "Loss at step 17050 : 3.3940982818603516\n",
      "Loss at step 17100 : 3.105907917022705\n",
      "Loss at step 17150 : 5.884776592254639\n",
      "Loss at step 17200 : 5.327133655548096\n",
      "Loss at step 17250 : 4.730561256408691\n",
      "Loss at step 17300 : 3.6125502586364746\n",
      "Loss at step 17350 : 4.436627388000488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 17400 : 4.355195045471191\n",
      "Loss at step 17450 : 5.033234596252441\n",
      "Loss at step 17500 : 3.6915688514709473\n",
      "Loss at step 17550 : 4.189692497253418\n",
      "Loss at step 17600 : 4.323850631713867\n",
      "Loss at step 17650 : 4.113953590393066\n",
      "Loss at step 17700 : 4.073544502258301\n",
      "Loss at step 17750 : 3.483029365539551\n",
      "Loss at step 17800 : 4.71878719329834\n",
      "Loss at step 17850 : 6.219027519226074\n",
      "Loss at step 17900 : 4.1261982917785645\n",
      "Loss at step 17950 : 4.7902398109436035\n",
      "Loss at step 18000 : 4.206792831420898\n",
      "Loss at step 18050 : 3.5736303329467773\n",
      "Loss at step 18100 : 4.531538963317871\n",
      "Loss at step 18150 : 2.4960105419158936\n",
      "Loss at step 18200 : 2.9162707328796387\n",
      "Loss at step 18250 : 4.102075576782227\n",
      "Loss at step 18300 : 3.6976475715637207\n",
      "Loss at step 18350 : 4.8748555183410645\n",
      "Loss at step 18400 : 3.4936137199401855\n",
      "Loss at step 18450 : 5.206659317016602\n",
      "Loss at step 18500 : 4.021507263183594\n",
      "Loss at step 18550 : 4.491116046905518\n",
      "Loss at step 18600 : 2.0128424167633057\n",
      "Loss at step 18650 : 4.283050060272217\n",
      "Loss at step 18700 : 3.7764124870300293\n",
      "Loss at step 18750 : 4.312365531921387\n",
      "Loss at step 18800 : 4.137090682983398\n",
      "Loss at step 18850 : 3.1115355491638184\n",
      "Loss at step 18900 : 4.719174861907959\n",
      "Loss at step 18950 : 4.040782928466797\n",
      "Loss at step 19000 : 3.3001303672790527\n",
      "Loss at step 19050 : 3.647914171218872\n",
      "Loss at step 19100 : 4.512847423553467\n",
      "Loss at step 19150 : 4.707535266876221\n",
      "Loss at step 19200 : 2.9360508918762207\n",
      "Loss at step 19250 : 3.8580684661865234\n",
      "Loss at step 19300 : 4.272871971130371\n",
      "Loss at step 19350 : 3.491090774536133\n",
      "Loss at step 19400 : 2.6366405487060547\n",
      "Loss at step 19450 : 3.0573818683624268\n",
      "Loss at step 19500 : 3.085974931716919\n",
      "Loss at step 19550 : 4.129912853240967\n",
      "Loss at step 19600 : 4.077203750610352\n",
      "Loss at step 19650 : 3.277736186981201\n",
      "Loss at step 19700 : 3.593040943145752\n",
      "Loss at step 19750 : 4.326176643371582\n",
      "Loss at step 19800 : 2.4879627227783203\n",
      "Loss at step 19850 : 3.4128050804138184\n",
      "Loss at step 19900 : 3.5888099670410156\n",
      "Loss at step 19950 : 3.505904197692871\n",
      "Loss at step 20000 : 4.176605224609375\n",
      "Nearest to tuna: metal, virgin, parmesan, beef, eye,\n",
      "Nearest to rice: thigh, mandu, lunch, spinach, glaze,\n",
      "Nearest to sushi: cornstarch, cut, sashimi, mandu, rice,\n",
      "Nearest to roll: worcestershire, sheet, oil, hazelnut, liqueur,\n",
      "Nearest to sashimi: cheese, fryer, ring, seaweed, wing,\n",
      "Nearest to steak: maple, paste, eye, beef, milliliter,\n",
      "Nearest to grill: chicken, paper, soy, taco, mesh,\n",
      "Nearest to sauce: jack, wasabi, corn, peel, beet,\n",
      "Nearest to cream: skinless, dente, toast, dunk, sieve,\n",
      "Nearest to cheesecake: fish, mesh, cashew, paper, sieve,\n",
      "Nearest to pizza: white, scallion, flake, packet, mixture,\n",
      "Nearest to lasagna: fry, rare, almond, wrap, teriyaki,\n",
      "Nearest to hamburger: coloring, ½, total, granola, ramekin,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 20050 : 2.818033456802368\n",
      "Loss at step 20100 : 4.37424373626709\n",
      "Loss at step 20150 : 3.4205660820007324\n",
      "Loss at step 20200 : 3.8649823665618896\n",
      "Loss at step 20250 : 4.653329849243164\n",
      "Loss at step 20300 : 3.8289794921875\n",
      "Loss at step 20350 : 5.638469696044922\n",
      "Loss at step 20400 : 2.4619362354278564\n",
      "Loss at step 20450 : 2.832095146179199\n",
      "Loss at step 20500 : 4.2494049072265625\n",
      "Loss at step 20550 : 3.272956132888794\n",
      "Loss at step 20600 : 2.7655105590820312\n",
      "Loss at step 20650 : 3.976083517074585\n",
      "Loss at step 20700 : 4.016648292541504\n",
      "Loss at step 20750 : 4.3746819496154785\n",
      "Loss at step 20800 : 5.05759334564209\n",
      "Loss at step 20850 : 3.3446033000946045\n",
      "Loss at step 20900 : 3.0165345668792725\n",
      "Loss at step 20950 : 3.61063814163208\n",
      "Loss at step 21000 : 3.9788365364074707\n",
      "Loss at step 21050 : 3.6536200046539307\n",
      "Loss at step 21100 : 4.055462837219238\n",
      "Loss at step 21150 : 3.622553586959839\n",
      "Loss at step 21200 : 4.206474304199219\n",
      "Loss at step 21250 : 3.1400206089019775\n",
      "Loss at step 21300 : 3.643113136291504\n",
      "Loss at step 21350 : 4.901403427124023\n",
      "Loss at step 21400 : 3.4985880851745605\n",
      "Loss at step 21450 : 2.928396701812744\n",
      "Loss at step 21500 : 4.494347095489502\n",
      "Loss at step 21550 : 4.117807865142822\n",
      "Loss at step 21600 : 3.089980125427246\n",
      "Loss at step 21650 : 4.056321144104004\n",
      "Loss at step 21700 : 4.448647975921631\n",
      "Loss at step 21750 : 2.754741668701172\n",
      "Loss at step 21800 : 3.2448551654815674\n",
      "Loss at step 21850 : 4.0879807472229\n",
      "Loss at step 21900 : 4.106646537780762\n",
      "Loss at step 21950 : 3.906107187271118\n",
      "Loss at step 22000 : 3.5450849533081055\n",
      "Loss at step 22050 : 2.2931013107299805\n",
      "Loss at step 22100 : 3.314276695251465\n",
      "Loss at step 22150 : 3.9201035499572754\n",
      "Loss at step 22200 : 4.399175643920898\n",
      "Loss at step 22250 : 3.841956615447998\n",
      "Loss at step 22300 : 4.610959053039551\n",
      "Loss at step 22350 : 3.253598690032959\n",
      "Loss at step 22400 : 3.333889961242676\n",
      "Loss at step 22450 : 4.1755690574646\n",
      "Loss at step 22500 : 3.306234836578369\n",
      "Loss at step 22550 : 3.9900050163269043\n",
      "Loss at step 22600 : 3.3947296142578125\n",
      "Loss at step 22650 : 4.757372856140137\n",
      "Loss at step 22700 : 4.249642372131348\n",
      "Loss at step 22750 : 3.4843316078186035\n",
      "Loss at step 22800 : 4.227848529815674\n",
      "Loss at step 22850 : 5.16568660736084\n",
      "Loss at step 22900 : 2.9933505058288574\n",
      "Loss at step 22950 : 3.5191636085510254\n",
      "Loss at step 23000 : 3.3199679851531982\n",
      "Loss at step 23050 : 4.394047737121582\n",
      "Loss at step 23100 : 3.9607224464416504\n",
      "Loss at step 23150 : 3.6189541816711426\n",
      "Loss at step 23200 : 3.5743935108184814\n",
      "Loss at step 23250 : 3.5544774532318115\n",
      "Loss at step 23300 : 2.636111259460449\n",
      "Loss at step 23350 : 5.09540319442749\n",
      "Loss at step 23400 : 2.2650322914123535\n",
      "Loss at step 23450 : 3.5789220333099365\n",
      "Loss at step 23500 : 4.002786636352539\n",
      "Loss at step 23550 : 3.1506853103637695\n",
      "Loss at step 23600 : 2.927870750427246\n",
      "Loss at step 23650 : 3.6030471324920654\n",
      "Loss at step 23700 : 3.520447254180908\n",
      "Loss at step 23750 : 4.311481952667236\n",
      "Loss at step 23800 : 3.357696056365967\n",
      "Loss at step 23850 : 3.75327205657959\n",
      "Loss at step 23900 : 4.443145751953125\n",
      "Loss at step 23950 : 4.400641918182373\n",
      "Loss at step 24000 : 4.1974029541015625\n",
      "Loss at step 24050 : 3.105281114578247\n",
      "Loss at step 24100 : 2.97375226020813\n",
      "Loss at step 24150 : 3.59970760345459\n",
      "Loss at step 24200 : 5.5435566902160645\n",
      "Loss at step 24250 : 4.118921279907227\n",
      "Loss at step 24300 : 3.227764844894409\n",
      "Loss at step 24350 : 2.801739454269409\n",
      "Loss at step 24400 : 3.051211357116699\n",
      "Loss at step 24450 : 3.3965399265289307\n",
      "Loss at step 24500 : 4.438044548034668\n",
      "Loss at step 24550 : 4.949680328369141\n",
      "Loss at step 24600 : 4.5928192138671875\n",
      "Loss at step 24650 : 4.482227325439453\n",
      "Loss at step 24700 : 4.501894950866699\n",
      "Loss at step 24750 : 3.412179470062256\n",
      "Loss at step 24800 : 2.306612014770508\n",
      "Loss at step 24850 : 3.3294620513916016\n",
      "Loss at step 24900 : 2.8373773097991943\n",
      "Loss at step 24950 : 4.083139419555664\n",
      "Loss at step 25000 : 3.901683807373047\n",
      "Nearest to tuna: parmesan, metal, virgin, eye, moisture,\n",
      "Nearest to rice: thigh, mandu, lunch, lime, spinach,\n",
      "Nearest to sushi: cut, sashimi, layer, cornstarch, mandu,\n",
      "Nearest to roll: sheet, worcestershire, oil, hazelnut, jam,\n",
      "Nearest to sashimi: seaweed, fryer, ring, gyoza, cheese,\n",
      "Nearest to steak: maple, paste, eye, beef, preheat,\n",
      "Nearest to grill: chicken, paper, soy, rack, taco,\n",
      "Nearest to sauce: jack, wasabi, position, roe, peel,\n",
      "Nearest to cream: skinless, dente, toast, dunk, chive,\n",
      "Nearest to cheesecake: cashew, mesh, paper, fish, sieve,\n",
      "Nearest to pizza: white, flake, packet, mixture, scallion,\n",
      "Nearest to lasagna: fry, rare, almond, wrap, teriyaki,\n",
      "Nearest to hamburger: total, granola, meat, coloring, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 25050 : 3.5286569595336914\n",
      "Loss at step 25100 : 4.501291275024414\n",
      "Loss at step 25150 : 4.0513410568237305\n",
      "Loss at step 25200 : 3.5902609825134277\n",
      "Loss at step 25250 : 3.749635696411133\n",
      "Loss at step 25300 : 3.5280284881591797\n",
      "Loss at step 25350 : 3.8291995525360107\n",
      "Loss at step 25400 : 3.59771728515625\n",
      "Loss at step 25450 : 4.432249069213867\n",
      "Loss at step 25500 : 2.9906582832336426\n",
      "Loss at step 25550 : 2.733921527862549\n",
      "Loss at step 25600 : 2.699098587036133\n",
      "Loss at step 25650 : 2.8890881538391113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 25700 : 3.5299153327941895\n",
      "Loss at step 25750 : 4.786250114440918\n",
      "Loss at step 25800 : 3.4988746643066406\n",
      "Loss at step 25850 : 3.86175537109375\n",
      "Loss at step 25900 : 3.9531044960021973\n",
      "Loss at step 25950 : 2.161073684692383\n",
      "Loss at step 26000 : 4.1393866539001465\n",
      "Loss at step 26050 : 4.860223770141602\n",
      "Loss at step 26100 : 4.852667331695557\n",
      "Loss at step 26150 : 3.9258241653442383\n",
      "Loss at step 26200 : 2.714966058731079\n",
      "Loss at step 26250 : 3.2622861862182617\n",
      "Loss at step 26300 : 3.080065965652466\n",
      "Loss at step 26350 : 3.38969087600708\n",
      "Loss at step 26400 : 2.7723047733306885\n",
      "Loss at step 26450 : 2.904503107070923\n",
      "Loss at step 26500 : 4.451022148132324\n",
      "Loss at step 26550 : 3.977409839630127\n",
      "Loss at step 26600 : 4.423873424530029\n",
      "Loss at step 26650 : 2.9796459674835205\n",
      "Loss at step 26700 : 2.858307361602783\n",
      "Loss at step 26750 : 4.012808322906494\n",
      "Loss at step 26800 : 4.996893882751465\n",
      "Loss at step 26850 : 3.296677589416504\n",
      "Loss at step 26900 : 3.6795034408569336\n",
      "Loss at step 26950 : 3.2755351066589355\n",
      "Loss at step 27000 : 3.590817928314209\n",
      "Loss at step 27050 : 3.6821515560150146\n",
      "Loss at step 27100 : 3.5533456802368164\n",
      "Loss at step 27150 : 3.75769305229187\n",
      "Loss at step 27200 : 2.489837646484375\n",
      "Loss at step 27250 : 2.9645347595214844\n",
      "Loss at step 27300 : 4.982409477233887\n",
      "Loss at step 27350 : 4.861194133758545\n",
      "Loss at step 27400 : 2.9711506366729736\n",
      "Loss at step 27450 : 3.304189682006836\n",
      "Loss at step 27500 : 3.698302745819092\n",
      "Loss at step 27550 : 3.3976962566375732\n",
      "Loss at step 27600 : 2.732408046722412\n",
      "Loss at step 27650 : 2.324561595916748\n",
      "Loss at step 27700 : 4.734009742736816\n",
      "Loss at step 27750 : 4.041701793670654\n",
      "Loss at step 27800 : 3.7057719230651855\n",
      "Loss at step 27850 : 3.116729974746704\n",
      "Loss at step 27900 : 5.193911552429199\n",
      "Loss at step 27950 : 3.615657329559326\n",
      "Loss at step 28000 : 3.275613307952881\n",
      "Loss at step 28050 : 3.529629707336426\n",
      "Loss at step 28100 : 3.7773385047912598\n",
      "Loss at step 28150 : 2.6736233234405518\n",
      "Loss at step 28200 : 3.7699952125549316\n",
      "Loss at step 28250 : 3.507173538208008\n",
      "Loss at step 28300 : 3.1672005653381348\n",
      "Loss at step 28350 : 3.9713351726531982\n",
      "Loss at step 28400 : 3.319519519805908\n",
      "Loss at step 28450 : 3.104360818862915\n",
      "Loss at step 28500 : 3.607978105545044\n",
      "Loss at step 28550 : 3.414064884185791\n",
      "Loss at step 28600 : 3.4905223846435547\n",
      "Loss at step 28650 : 3.896637439727783\n",
      "Loss at step 28700 : 2.482189178466797\n",
      "Loss at step 28750 : 3.264904499053955\n",
      "Loss at step 28800 : 4.241839408874512\n",
      "Loss at step 28850 : 2.962052822113037\n",
      "Loss at step 28900 : 3.4143056869506836\n",
      "Loss at step 28950 : 3.4043450355529785\n",
      "Loss at step 29000 : 3.977468252182007\n",
      "Loss at step 29050 : 3.2269482612609863\n",
      "Loss at step 29100 : 3.584028959274292\n",
      "Loss at step 29150 : 3.9888250827789307\n",
      "Loss at step 29200 : 3.2649636268615723\n",
      "Loss at step 29250 : 2.9111099243164062\n",
      "Loss at step 29300 : 2.7177114486694336\n",
      "Loss at step 29350 : 4.381153106689453\n",
      "Loss at step 29400 : 2.9142706394195557\n",
      "Loss at step 29450 : 3.0047855377197266\n",
      "Loss at step 29500 : 3.5073585510253906\n",
      "Loss at step 29550 : 3.6086416244506836\n",
      "Loss at step 29600 : 3.4134771823883057\n",
      "Loss at step 29650 : 4.002243995666504\n",
      "Loss at step 29700 : 3.6459128856658936\n",
      "Loss at step 29750 : 4.446833610534668\n",
      "Loss at step 29800 : 3.2508559226989746\n",
      "Loss at step 29850 : 3.276524782180786\n",
      "Loss at step 29900 : 2.941887855529785\n",
      "Loss at step 29950 : 3.1229422092437744\n",
      "Loss at step 30000 : 3.5010123252868652\n",
      "Nearest to tuna: parmesan, metal, virgin, eye, moisture,\n",
      "Nearest to rice: thigh, mandu, lunch, lime, spinach,\n",
      "Nearest to sushi: cut, sashimi, mandu, rice, cornstarch,\n",
      "Nearest to roll: sheet, worcestershire, oil, hazelnut, meal,\n",
      "Nearest to sashimi: seaweed, fryer, ring, sushi, gyoza,\n",
      "Nearest to steak: maple, paste, beef, preheat, eye,\n",
      "Nearest to grill: chicken, paper, soy, rack, teriyaki,\n",
      "Nearest to sauce: jack, wasabi, meat, position, peel,\n",
      "Nearest to cream: skinless, dente, toast, dunk, extract,\n",
      "Nearest to cheesecake: cashew, mesh, paper, jasmine, flake,\n",
      "Nearest to pizza: white, flake, mixture, extract, baguette,\n",
      "Nearest to lasagna: fry, rare, almond, teriyaki, wrap,\n",
      "Nearest to hamburger: meat, parsley, herb, total, coloring,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 30050 : 3.963494062423706\n",
      "Loss at step 30100 : 2.6216602325439453\n",
      "Loss at step 30150 : 4.4150614738464355\n",
      "Loss at step 30200 : 4.23871374130249\n",
      "Loss at step 30250 : 3.652944564819336\n",
      "Loss at step 30300 : 3.3139262199401855\n",
      "Loss at step 30350 : 4.561368942260742\n",
      "Loss at step 30400 : 3.6539433002471924\n",
      "Loss at step 30450 : 3.1526577472686768\n",
      "Loss at step 30500 : 2.382249593734741\n",
      "Loss at step 30550 : 2.3980860710144043\n",
      "Loss at step 30600 : 2.6976189613342285\n",
      "Loss at step 30650 : 3.612963914871216\n",
      "Loss at step 30700 : 3.3198513984680176\n",
      "Loss at step 30750 : 1.9971895217895508\n",
      "Loss at step 30800 : 3.8458375930786133\n",
      "Loss at step 30850 : 3.632478952407837\n",
      "Loss at step 30900 : 4.212674617767334\n",
      "Loss at step 30950 : 4.651480674743652\n",
      "Loss at step 31000 : 2.485811233520508\n",
      "Loss at step 31050 : 4.209000587463379\n",
      "Loss at step 31100 : 3.8075637817382812\n",
      "Loss at step 31150 : 2.432676315307617\n",
      "Loss at step 31200 : 3.1079869270324707\n",
      "Loss at step 31250 : 3.3187882900238037\n",
      "Loss at step 31300 : 3.177926540374756\n",
      "Loss at step 31350 : 3.836944103240967\n",
      "Loss at step 31400 : 3.2202305793762207\n",
      "Loss at step 31450 : 3.770118236541748\n",
      "Loss at step 31500 : 4.024423599243164\n",
      "Loss at step 31550 : 3.3663854598999023\n",
      "Loss at step 31600 : 2.970651388168335\n",
      "Loss at step 31650 : 2.6956472396850586\n",
      "Loss at step 31700 : 3.3722074031829834\n",
      "Loss at step 31750 : 3.423527240753174\n",
      "Loss at step 31800 : 3.5754451751708984\n",
      "Loss at step 31850 : 2.6304895877838135\n",
      "Loss at step 31900 : 3.149658679962158\n",
      "Loss at step 31950 : 2.726548910140991\n",
      "Loss at step 32000 : 3.792588710784912\n",
      "Loss at step 32050 : 4.285322666168213\n",
      "Loss at step 32100 : 3.248491048812866\n",
      "Loss at step 32150 : 3.339122772216797\n",
      "Loss at step 32200 : 4.477478981018066\n",
      "Loss at step 32250 : 4.122413635253906\n",
      "Loss at step 32300 : 3.1517601013183594\n",
      "Loss at step 32350 : 3.194139003753662\n",
      "Loss at step 32400 : 2.3236031532287598\n",
      "Loss at step 32450 : 2.8242647647857666\n",
      "Loss at step 32500 : 3.1210765838623047\n",
      "Loss at step 32550 : 3.2255001068115234\n",
      "Loss at step 32600 : 4.378294944763184\n",
      "Loss at step 32650 : 2.9519338607788086\n",
      "Loss at step 32700 : 2.8724546432495117\n",
      "Loss at step 32750 : 2.4023067951202393\n",
      "Loss at step 32800 : 3.470646858215332\n",
      "Loss at step 32850 : 4.502599239349365\n",
      "Loss at step 32900 : 3.614384651184082\n",
      "Loss at step 32950 : 3.7571609020233154\n",
      "Loss at step 33000 : 3.71156644821167\n",
      "Loss at step 33050 : 2.891447067260742\n",
      "Loss at step 33100 : 3.762584924697876\n",
      "Loss at step 33150 : 3.816389799118042\n",
      "Loss at step 33200 : 3.6021599769592285\n",
      "Loss at step 33250 : 4.322911262512207\n",
      "Loss at step 33300 : 3.2066056728363037\n",
      "Loss at step 33350 : 3.570155620574951\n",
      "Loss at step 33400 : 3.0647406578063965\n",
      "Loss at step 33450 : 2.8148796558380127\n",
      "Loss at step 33500 : 2.5335464477539062\n",
      "Loss at step 33550 : 2.6404953002929688\n",
      "Loss at step 33600 : 2.465585231781006\n",
      "Loss at step 33650 : 3.1079554557800293\n",
      "Loss at step 33700 : 1.913060188293457\n",
      "Loss at step 33750 : 4.083982467651367\n",
      "Loss at step 33800 : 3.943326950073242\n",
      "Loss at step 33850 : 2.447885513305664\n",
      "Loss at step 33900 : 3.39374041557312\n",
      "Loss at step 33950 : 3.1915197372436523\n",
      "Loss at step 34000 : 2.963930606842041\n",
      "Loss at step 34050 : 3.179145574569702\n",
      "Loss at step 34100 : 3.982325315475464\n",
      "Loss at step 34150 : 3.3316116333007812\n",
      "Loss at step 34200 : 3.798914909362793\n",
      "Loss at step 34250 : 3.0963821411132812\n",
      "Loss at step 34300 : 4.081232070922852\n",
      "Loss at step 34350 : 3.4730756282806396\n",
      "Loss at step 34400 : 2.39149808883667\n",
      "Loss at step 34450 : 3.490194797515869\n",
      "Loss at step 34500 : 2.302279472351074\n",
      "Loss at step 34550 : 3.647552490234375\n",
      "Loss at step 34600 : 3.7153637409210205\n",
      "Loss at step 34650 : 3.6317038536071777\n",
      "Loss at step 34700 : 4.76645565032959\n",
      "Loss at step 34750 : 3.9930667877197266\n",
      "Loss at step 34800 : 2.8829965591430664\n",
      "Loss at step 34850 : 2.8732454776763916\n",
      "Loss at step 34900 : 3.9240944385528564\n",
      "Loss at step 34950 : 3.909884214401245\n",
      "Loss at step 35000 : 3.0010905265808105\n",
      "Nearest to tuna: parmesan, metal, virgin, eye, beef,\n",
      "Nearest to rice: thigh, mandu, lunch, lime, spinach,\n",
      "Nearest to sushi: cut, sashimi, cornstarch, piece, layer,\n",
      "Nearest to roll: sheet, oil, worcestershire, hazelnut, meal,\n",
      "Nearest to sashimi: seaweed, fryer, gyoza, ring, cheese,\n",
      "Nearest to steak: maple, paste, beef, eye, preheat,\n",
      "Nearest to grill: chicken, soy, paper, rack, teriyaki,\n",
      "Nearest to sauce: jack, wasabi, meat, juice, oregano,\n",
      "Nearest to cream: skinless, dente, toast, dunk, extract,\n",
      "Nearest to cheesecake: cashew, mesh, paper, flake, flour,\n",
      "Nearest to pizza: white, flake, mixture, beat, baguette,\n",
      "Nearest to lasagna: fry, rare, almond, lemon, teriyaki,\n",
      "Nearest to hamburger: meat, parsley, herb, total, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 35050 : 3.0607621669769287\n",
      "Loss at step 35100 : 2.9005136489868164\n",
      "Loss at step 35150 : 4.201502323150635\n",
      "Loss at step 35200 : 3.349221706390381\n",
      "Loss at step 35250 : 3.9242799282073975\n",
      "Loss at step 35300 : 2.6907718181610107\n",
      "Loss at step 35350 : 3.0557656288146973\n",
      "Loss at step 35400 : 2.5199365615844727\n",
      "Loss at step 35450 : 2.0943939685821533\n",
      "Loss at step 35500 : 3.6857380867004395\n",
      "Loss at step 35550 : 3.273022174835205\n",
      "Loss at step 35600 : 3.565244197845459\n",
      "Loss at step 35650 : 3.3825998306274414\n",
      "Loss at step 35700 : 2.950794219970703\n",
      "Loss at step 35750 : 3.8420560359954834\n",
      "Loss at step 35800 : 3.7575361728668213\n",
      "Loss at step 35850 : 3.9630002975463867\n",
      "Loss at step 35900 : 3.7063212394714355\n",
      "Loss at step 35950 : 4.701062202453613\n",
      "Loss at step 36000 : 3.0407538414001465\n",
      "Loss at step 36050 : 2.6676628589630127\n",
      "Loss at step 36100 : 3.171696662902832\n",
      "Loss at step 36150 : 3.3568716049194336\n",
      "Loss at step 36200 : 2.6609020233154297\n",
      "Loss at step 36250 : 3.9080519676208496\n",
      "Loss at step 36300 : 4.839059829711914\n",
      "Loss at step 36350 : 3.6374640464782715\n",
      "Loss at step 36400 : 3.187875986099243\n",
      "Loss at step 36450 : 2.963355541229248\n",
      "Loss at step 36500 : 3.507087230682373\n",
      "Loss at step 36550 : 2.0761055946350098\n",
      "Loss at step 36600 : 2.9965901374816895\n",
      "Loss at step 36650 : 3.183936595916748\n",
      "Loss at step 36700 : 3.7270569801330566\n",
      "Loss at step 36750 : 4.125502586364746\n",
      "Loss at step 36800 : 3.1920835971832275\n",
      "Loss at step 36850 : 3.444035768508911\n",
      "Loss at step 36900 : 3.8812100887298584\n",
      "Loss at step 36950 : 3.923746109008789\n",
      "Loss at step 37000 : 4.084997177124023\n",
      "Loss at step 37050 : 4.359933376312256\n",
      "Loss at step 37100 : 3.8368124961853027\n",
      "Loss at step 37150 : 3.200005054473877\n",
      "Loss at step 37200 : 4.115680694580078\n",
      "Loss at step 37250 : 2.0857977867126465\n",
      "Loss at step 37300 : 2.609518051147461\n",
      "Loss at step 37350 : 3.676772117614746\n",
      "Loss at step 37400 : 2.4741175174713135\n",
      "Loss at step 37450 : 4.041701793670654\n",
      "Loss at step 37500 : 3.2078747749328613\n",
      "Loss at step 37550 : 2.1877803802490234\n",
      "Loss at step 37600 : 5.1745524406433105\n",
      "Loss at step 37650 : 2.6732749938964844\n",
      "Loss at step 37700 : 3.140807628631592\n",
      "Loss at step 37750 : 3.5389246940612793\n",
      "Loss at step 37800 : 3.289858818054199\n",
      "Loss at step 37850 : 2.893465995788574\n",
      "Loss at step 37900 : 3.4837021827697754\n",
      "Loss at step 37950 : 3.3102684020996094\n",
      "Loss at step 38000 : 3.965365409851074\n",
      "Loss at step 38050 : 3.5757060050964355\n",
      "Loss at step 38100 : 2.3066463470458984\n",
      "Loss at step 38150 : 3.5152955055236816\n",
      "Loss at step 38200 : 4.126847267150879\n",
      "Loss at step 38250 : 4.236534118652344\n",
      "Loss at step 38300 : 3.569032669067383\n",
      "Loss at step 38350 : 3.3256330490112305\n",
      "Loss at step 38400 : 3.889402389526367\n",
      "Loss at step 38450 : 2.9185166358947754\n",
      "Loss at step 38500 : 3.1865291595458984\n",
      "Loss at step 38550 : 2.073173999786377\n",
      "Loss at step 38600 : 2.8814539909362793\n",
      "Loss at step 38650 : 3.6592540740966797\n",
      "Loss at step 38700 : 1.698874592781067\n",
      "Loss at step 38750 : 3.281838893890381\n",
      "Loss at step 38800 : 3.8299858570098877\n",
      "Loss at step 38850 : 2.825955390930176\n",
      "Loss at step 38900 : 4.723937034606934\n",
      "Loss at step 38950 : 3.6291165351867676\n",
      "Loss at step 39000 : 4.288677215576172\n",
      "Loss at step 39050 : 3.9397711753845215\n",
      "Loss at step 39100 : 2.5012667179107666\n",
      "Loss at step 39150 : 4.077402114868164\n",
      "Loss at step 39200 : 4.187135696411133\n",
      "Loss at step 39250 : 4.082683563232422\n",
      "Loss at step 39300 : 3.0134949684143066\n",
      "Loss at step 39350 : 3.5850329399108887\n",
      "Loss at step 39400 : 2.5811188220977783\n",
      "Loss at step 39450 : 4.7771992683410645\n",
      "Loss at step 39500 : 3.323129415512085\n",
      "Loss at step 39550 : 3.3688764572143555\n",
      "Loss at step 39600 : 3.141477584838867\n",
      "Loss at step 39650 : 3.3765194416046143\n",
      "Loss at step 39700 : 2.0400710105895996\n",
      "Loss at step 39750 : 5.200865268707275\n",
      "Loss at step 39800 : 3.421769142150879\n",
      "Loss at step 39850 : 2.601637125015259\n",
      "Loss at step 39900 : 3.264369010925293\n",
      "Loss at step 39950 : 3.4559075832366943\n",
      "Loss at step 40000 : 4.761946201324463\n",
      "Nearest to tuna: parmesan, metal, virgin, eye, beef,\n",
      "Nearest to rice: mandu, thigh, lunch, lime, glaze,\n",
      "Nearest to sushi: cut, sashimi, rice, cornstarch, piece,\n",
      "Nearest to roll: sheet, worcestershire, oil, meal, hazelnut,\n",
      "Nearest to sashimi: fryer, seaweed, ring, gyoza, sushi,\n",
      "Nearest to steak: maple, paste, beef, eye, preheat,\n",
      "Nearest to grill: chicken, soy, rack, paper, cider,\n",
      "Nearest to sauce: jack, wasabi, meat, sirloin, oregano,\n",
      "Nearest to cream: dente, skinless, toast, dunk, sieve,\n",
      "Nearest to cheesecake: cashew, paper, mesh, flake, flour,\n",
      "Nearest to pizza: white, flake, mixture, beat, run,\n",
      "Nearest to lasagna: fry, rare, almond, lemon, teriyaki,\n",
      "Nearest to hamburger: meat, parsley, herb, total, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 40050 : 3.6394097805023193\n",
      "Loss at step 40100 : 3.975193977355957\n",
      "Loss at step 40150 : 4.038165092468262\n",
      "Loss at step 40200 : 4.263138294219971\n",
      "Loss at step 40250 : 3.8823094367980957\n",
      "Loss at step 40300 : 2.727689743041992\n",
      "Loss at step 40350 : 2.5013506412506104\n",
      "Loss at step 40400 : 3.698681592941284\n",
      "Loss at step 40450 : 2.811941623687744\n",
      "Loss at step 40500 : 2.9306418895721436\n",
      "Loss at step 40550 : 2.883213758468628\n",
      "Loss at step 40600 : 3.2508060932159424\n",
      "Loss at step 40650 : 2.545531988143921\n",
      "Loss at step 40700 : 2.4629435539245605\n",
      "Loss at step 40750 : 2.8421061038970947\n",
      "Loss at step 40800 : 2.9991514682769775\n",
      "Loss at step 40850 : 2.740149974822998\n",
      "Loss at step 40900 : 3.4327504634857178\n",
      "Loss at step 40950 : 3.6865234375\n",
      "Loss at step 41000 : 4.14801025390625\n",
      "Loss at step 41050 : 3.9441416263580322\n",
      "Loss at step 41100 : 3.3262510299682617\n",
      "Loss at step 41150 : 3.0314817428588867\n",
      "Loss at step 41200 : 3.599757194519043\n",
      "Loss at step 41250 : 2.421840190887451\n",
      "Loss at step 41300 : 2.577484369277954\n",
      "Loss at step 41350 : 4.960381984710693\n",
      "Loss at step 41400 : 2.5749974250793457\n",
      "Loss at step 41450 : 3.565173625946045\n",
      "Loss at step 41500 : 3.362874746322632\n",
      "Loss at step 41550 : 3.9933974742889404\n",
      "Loss at step 41600 : 4.076265335083008\n",
      "Loss at step 41650 : 4.559063911437988\n",
      "Loss at step 41700 : 3.454026460647583\n",
      "Loss at step 41750 : 3.461393356323242\n",
      "Loss at step 41800 : 2.912198066711426\n",
      "Loss at step 41850 : 2.015026330947876\n",
      "Loss at step 41900 : 3.414546012878418\n",
      "Loss at step 41950 : 3.851057529449463\n",
      "Loss at step 42000 : 2.9518048763275146\n",
      "Loss at step 42050 : 4.166492938995361\n",
      "Loss at step 42100 : 2.312204360961914\n",
      "Loss at step 42150 : 3.7685189247131348\n",
      "Loss at step 42200 : 3.597970485687256\n",
      "Loss at step 42250 : 2.9161202907562256\n",
      "Loss at step 42300 : 3.1108767986297607\n",
      "Loss at step 42350 : 3.10349178314209\n",
      "Loss at step 42400 : 2.687572479248047\n",
      "Loss at step 42450 : 3.5257036685943604\n",
      "Loss at step 42500 : 3.9696552753448486\n",
      "Loss at step 42550 : 2.4966487884521484\n",
      "Loss at step 42600 : 2.702446460723877\n",
      "Loss at step 42650 : 3.8830223083496094\n",
      "Loss at step 42700 : 3.7833971977233887\n",
      "Loss at step 42750 : 2.715825080871582\n",
      "Loss at step 42800 : 3.277578830718994\n",
      "Loss at step 42850 : 3.039980411529541\n",
      "Loss at step 42900 : 2.843231678009033\n",
      "Loss at step 42950 : 4.541119575500488\n",
      "Loss at step 43000 : 3.3088419437408447\n",
      "Loss at step 43050 : 3.0091867446899414\n",
      "Loss at step 43100 : 3.437291383743286\n",
      "Loss at step 43150 : 2.877927303314209\n",
      "Loss at step 43200 : 4.184286594390869\n",
      "Loss at step 43250 : 3.097245693206787\n",
      "Loss at step 43300 : 3.2857329845428467\n",
      "Loss at step 43350 : 2.7856366634368896\n",
      "Loss at step 43400 : 3.629138946533203\n",
      "Loss at step 43450 : 3.514482021331787\n",
      "Loss at step 43500 : 2.5752124786376953\n",
      "Loss at step 43550 : 3.530870199203491\n",
      "Loss at step 43600 : 3.3410236835479736\n",
      "Loss at step 43650 : 5.046964168548584\n",
      "Loss at step 43700 : 3.724553346633911\n",
      "Loss at step 43750 : 4.344951152801514\n",
      "Loss at step 43800 : 3.3723814487457275\n",
      "Loss at step 43850 : 2.1407248973846436\n",
      "Loss at step 43900 : 3.3871750831604004\n",
      "Loss at step 43950 : 3.6867804527282715\n",
      "Loss at step 44000 : 2.681868076324463\n",
      "Loss at step 44050 : 2.3247594833374023\n",
      "Loss at step 44100 : 4.404356002807617\n",
      "Loss at step 44150 : 2.666416645050049\n",
      "Loss at step 44200 : 3.4687557220458984\n",
      "Loss at step 44250 : 2.731632947921753\n",
      "Loss at step 44300 : 2.3273701667785645\n",
      "Loss at step 44350 : 2.8392202854156494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 44400 : 2.7440176010131836\n",
      "Loss at step 44450 : 3.173643112182617\n",
      "Loss at step 44500 : 3.0863752365112305\n",
      "Loss at step 44550 : 2.9951648712158203\n",
      "Loss at step 44600 : 3.3136367797851562\n",
      "Loss at step 44650 : 2.9920692443847656\n",
      "Loss at step 44700 : 3.004729986190796\n",
      "Loss at step 44750 : 3.8694825172424316\n",
      "Loss at step 44800 : 3.0637006759643555\n",
      "Loss at step 44850 : 3.047079086303711\n",
      "Loss at step 44900 : 3.8340003490448\n",
      "Loss at step 44950 : 2.6396050453186035\n",
      "Loss at step 45000 : 3.244363307952881\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, moisture,\n",
      "Nearest to rice: thigh, mandu, lunch, lime, glaze,\n",
      "Nearest to sushi: piece, cut, layer, sesame, cornstarch,\n",
      "Nearest to roll: sheet, worcestershire, oil, meal, jam,\n",
      "Nearest to sashimi: fryer, seaweed, gyoza, sushi, ring,\n",
      "Nearest to steak: maple, eye, paste, beef, preheat,\n",
      "Nearest to grill: chicken, soy, teriyaki, rack, paper,\n",
      "Nearest to sauce: jack, wasabi, meat, sirloin, oregano,\n",
      "Nearest to cream: dente, skinless, sieve, toast, pastry,\n",
      "Nearest to cheesecake: cashew, flake, paper, flour, mesh,\n",
      "Nearest to pizza: white, flake, mixture, baguette, run,\n",
      "Nearest to lasagna: fry, rare, lemon, almond, teriyaki,\n",
      "Nearest to hamburger: meat, herb, parsley, pine, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 45050 : 3.0982558727264404\n",
      "Loss at step 45100 : 2.4927754402160645\n",
      "Loss at step 45150 : 2.2638115882873535\n",
      "Loss at step 45200 : 3.8358569145202637\n",
      "Loss at step 45250 : 3.6940927505493164\n",
      "Loss at step 45300 : 3.2918496131896973\n",
      "Loss at step 45350 : 2.9229073524475098\n",
      "Loss at step 45400 : 3.6364054679870605\n",
      "Loss at step 45450 : 3.2977075576782227\n",
      "Loss at step 45500 : 3.3938705921173096\n",
      "Loss at step 45550 : 3.186129093170166\n",
      "Loss at step 45600 : 3.2855894565582275\n",
      "Loss at step 45650 : 2.920318603515625\n",
      "Loss at step 45700 : 2.5624260902404785\n",
      "Loss at step 45750 : 2.167449951171875\n",
      "Loss at step 45800 : 2.5920424461364746\n",
      "Loss at step 45850 : 2.510972261428833\n",
      "Loss at step 45900 : 4.195769309997559\n",
      "Loss at step 45950 : 2.2907514572143555\n",
      "Loss at step 46000 : 2.9056215286254883\n",
      "Loss at step 46050 : 3.578301191329956\n",
      "Loss at step 46100 : 3.495070457458496\n",
      "Loss at step 46150 : 4.107416152954102\n",
      "Loss at step 46200 : 3.1266214847564697\n",
      "Loss at step 46250 : 2.4064745903015137\n",
      "Loss at step 46300 : 4.3548264503479\n",
      "Loss at step 46350 : 3.449530839920044\n",
      "Loss at step 46400 : 3.4182605743408203\n",
      "Loss at step 46450 : 2.8449621200561523\n",
      "Loss at step 46500 : 3.9216246604919434\n",
      "Loss at step 46550 : 3.8625895977020264\n",
      "Loss at step 46600 : 2.879164695739746\n",
      "Loss at step 46650 : 3.3440303802490234\n",
      "Loss at step 46700 : 3.833554744720459\n",
      "Loss at step 46750 : 3.926006317138672\n",
      "Loss at step 46800 : 3.5182547569274902\n",
      "Loss at step 46850 : 2.747182846069336\n",
      "Loss at step 46900 : 2.0143561363220215\n",
      "Loss at step 46950 : 3.0014164447784424\n",
      "Loss at step 47000 : 3.7652339935302734\n",
      "Loss at step 47050 : 2.9144177436828613\n",
      "Loss at step 47100 : 3.5276622772216797\n",
      "Loss at step 47150 : 3.9027791023254395\n",
      "Loss at step 47200 : 3.1697757244110107\n",
      "Loss at step 47250 : 3.7841074466705322\n",
      "Loss at step 47300 : 3.9666550159454346\n",
      "Loss at step 47350 : 3.4598357677459717\n",
      "Loss at step 47400 : 2.389130115509033\n",
      "Loss at step 47450 : 2.9607081413269043\n",
      "Loss at step 47500 : 2.8457045555114746\n",
      "Loss at step 47550 : 2.465113639831543\n",
      "Loss at step 47600 : 2.4301397800445557\n",
      "Loss at step 47650 : 3.136528491973877\n",
      "Loss at step 47700 : 3.1863789558410645\n",
      "Loss at step 47750 : 3.4597511291503906\n",
      "Loss at step 47800 : 3.47878360748291\n",
      "Loss at step 47850 : 3.5426554679870605\n",
      "Loss at step 47900 : 4.201190948486328\n",
      "Loss at step 47950 : 2.960901975631714\n",
      "Loss at step 48000 : 3.8838210105895996\n",
      "Loss at step 48050 : 3.1009292602539062\n",
      "Loss at step 48100 : 3.2698311805725098\n",
      "Loss at step 48150 : 2.2953481674194336\n",
      "Loss at step 48200 : 1.716963529586792\n",
      "Loss at step 48250 : 2.7048497200012207\n",
      "Loss at step 48300 : 3.526320457458496\n",
      "Loss at step 48350 : 2.879859209060669\n",
      "Loss at step 48400 : 2.8044517040252686\n",
      "Loss at step 48450 : 3.031158447265625\n",
      "Loss at step 48500 : 3.4949350357055664\n",
      "Loss at step 48550 : 2.9896295070648193\n",
      "Loss at step 48600 : 2.554492950439453\n",
      "Loss at step 48650 : 2.6273112297058105\n",
      "Loss at step 48700 : 3.353975534439087\n",
      "Loss at step 48750 : 2.9767189025878906\n",
      "Loss at step 48800 : 2.673762798309326\n",
      "Loss at step 48850 : 2.9599533081054688\n",
      "Loss at step 48900 : 2.398984909057617\n",
      "Loss at step 48950 : 3.0077691078186035\n",
      "Loss at step 49000 : 3.4591281414031982\n",
      "Loss at step 49050 : 3.1361207962036133\n",
      "Loss at step 49100 : 3.861738681793213\n",
      "Loss at step 49150 : 4.747450351715088\n",
      "Loss at step 49200 : 2.7867472171783447\n",
      "Loss at step 49250 : 2.735995292663574\n",
      "Loss at step 49300 : 3.988358974456787\n",
      "Loss at step 49350 : 3.022700786590576\n",
      "Loss at step 49400 : 2.9517502784729004\n",
      "Loss at step 49450 : 2.3970906734466553\n",
      "Loss at step 49500 : 4.040999412536621\n",
      "Loss at step 49550 : 2.9464964866638184\n",
      "Loss at step 49600 : 3.085606575012207\n",
      "Loss at step 49650 : 2.9168643951416016\n",
      "Loss at step 49700 : 2.301640510559082\n",
      "Loss at step 49750 : 5.1902055740356445\n",
      "Loss at step 49800 : 3.1848702430725098\n",
      "Loss at step 49850 : 3.13722562789917\n",
      "Loss at step 49900 : 2.4307281970977783\n",
      "Loss at step 49950 : 2.596871852874756\n",
      "Loss at step 50000 : 3.1713836193084717\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, beef,\n",
      "Nearest to rice: thigh, mandu, lunch, lime, carrot,\n",
      "Nearest to sushi: piece, sesame, sashimi, layer, cut,\n",
      "Nearest to roll: sheet, worcestershire, oil, meal, jam,\n",
      "Nearest to sashimi: fryer, sushi, seaweed, gyoza, spread,\n",
      "Nearest to steak: maple, paste, eye, beef, milliliter,\n",
      "Nearest to grill: soy, chicken, rack, teriyaki, paper,\n",
      "Nearest to sauce: jack, wasabi, soy, mat, meat,\n",
      "Nearest to cream: dente, skinless, extract, dunk, toast,\n",
      "Nearest to cheesecake: cashew, flake, flour, paper, mesh,\n",
      "Nearest to pizza: white, flake, mixture, run, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, almond, teriyaki,\n",
      "Nearest to hamburger: meat, herb, parsley, pine, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 50050 : 3.270576000213623\n",
      "Loss at step 50100 : 3.146322727203369\n",
      "Loss at step 50150 : 3.474613904953003\n",
      "Loss at step 50200 : 3.7420899868011475\n",
      "Loss at step 50250 : 2.92010235786438\n",
      "Loss at step 50300 : 2.173631191253662\n",
      "Loss at step 50350 : 3.3609848022460938\n",
      "Loss at step 50400 : 3.7805163860321045\n",
      "Loss at step 50450 : 3.983245849609375\n",
      "Loss at step 50500 : 3.7987120151519775\n",
      "Loss at step 50550 : 3.0128040313720703\n",
      "Loss at step 50600 : 3.3344509601593018\n",
      "Loss at step 50650 : 1.9907398223876953\n",
      "Loss at step 50700 : 2.163785696029663\n",
      "Loss at step 50750 : 3.7164249420166016\n",
      "Loss at step 50800 : 1.9685983657836914\n",
      "Loss at step 50850 : 2.7781338691711426\n",
      "Loss at step 50900 : 3.5619335174560547\n",
      "Loss at step 50950 : 2.6652703285217285\n",
      "Loss at step 51000 : 3.333157539367676\n",
      "Loss at step 51050 : 4.0701189041137695\n",
      "Loss at step 51100 : 3.5548362731933594\n",
      "Loss at step 51150 : 3.2157235145568848\n",
      "Loss at step 51200 : 3.2181167602539062\n",
      "Loss at step 51250 : 3.9741082191467285\n",
      "Loss at step 51300 : 3.124511480331421\n",
      "Loss at step 51350 : 2.365485668182373\n",
      "Loss at step 51400 : 3.5510640144348145\n",
      "Loss at step 51450 : 4.371539115905762\n",
      "Loss at step 51500 : 2.1347501277923584\n",
      "Loss at step 51550 : 2.9093093872070312\n",
      "Loss at step 51600 : 2.614149570465088\n",
      "Loss at step 51650 : 3.299072742462158\n",
      "Loss at step 51700 : 3.9983103275299072\n",
      "Loss at step 51750 : 2.3013830184936523\n",
      "Loss at step 51800 : 3.1921308040618896\n",
      "Loss at step 51850 : 2.074610948562622\n",
      "Loss at step 51900 : 3.1275203227996826\n",
      "Loss at step 51950 : 4.062009811401367\n",
      "Loss at step 52000 : 2.002305030822754\n",
      "Loss at step 52050 : 3.312765598297119\n",
      "Loss at step 52100 : 2.988651990890503\n",
      "Loss at step 52150 : 3.438995838165283\n",
      "Loss at step 52200 : 3.159733295440674\n",
      "Loss at step 52250 : 4.731252670288086\n",
      "Loss at step 52300 : 3.188934564590454\n",
      "Loss at step 52350 : 2.2881827354431152\n",
      "Loss at step 52400 : 3.1524362564086914\n",
      "Loss at step 52450 : 2.7518739700317383\n",
      "Loss at step 52500 : 3.0387277603149414\n",
      "Loss at step 52550 : 3.594578266143799\n",
      "Loss at step 52600 : 3.081108570098877\n",
      "Loss at step 52650 : 3.0184953212738037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 52700 : 3.5357561111450195\n",
      "Loss at step 52750 : 3.7574422359466553\n",
      "Loss at step 52800 : 3.731555938720703\n",
      "Loss at step 52850 : 3.7012741565704346\n",
      "Loss at step 52900 : 3.1957356929779053\n",
      "Loss at step 52950 : 4.2552690505981445\n",
      "Loss at step 53000 : 3.9604785442352295\n",
      "Loss at step 53050 : 3.7745888233184814\n",
      "Loss at step 53100 : 2.3541321754455566\n",
      "Loss at step 53150 : 2.8900365829467773\n",
      "Loss at step 53200 : 3.740760326385498\n",
      "Loss at step 53250 : 3.2729220390319824\n",
      "Loss at step 53300 : 3.3461971282958984\n",
      "Loss at step 53350 : 3.153270959854126\n",
      "Loss at step 53400 : 2.799079656600952\n",
      "Loss at step 53450 : 3.2706096172332764\n",
      "Loss at step 53500 : 3.1491432189941406\n",
      "Loss at step 53550 : 3.328494071960449\n",
      "Loss at step 53600 : 2.2739248275756836\n",
      "Loss at step 53650 : 3.504202127456665\n",
      "Loss at step 53700 : 3.3706374168395996\n",
      "Loss at step 53750 : 2.820474863052368\n",
      "Loss at step 53800 : 3.156815528869629\n",
      "Loss at step 53850 : 2.9206912517547607\n",
      "Loss at step 53900 : 3.0678014755249023\n",
      "Loss at step 53950 : 2.6875483989715576\n",
      "Loss at step 54000 : 3.6696553230285645\n",
      "Loss at step 54050 : 2.982351541519165\n",
      "Loss at step 54100 : 2.963235855102539\n",
      "Loss at step 54150 : 2.6639466285705566\n",
      "Loss at step 54200 : 3.6842246055603027\n",
      "Loss at step 54250 : 3.3532135486602783\n",
      "Loss at step 54300 : 2.6037349700927734\n",
      "Loss at step 54350 : 2.8090219497680664\n",
      "Loss at step 54400 : 3.3846051692962646\n",
      "Loss at step 54450 : 3.1743898391723633\n",
      "Loss at step 54500 : 4.031912803649902\n",
      "Loss at step 54550 : 3.0145933628082275\n",
      "Loss at step 54600 : 2.5936803817749023\n",
      "Loss at step 54650 : 3.8782801628112793\n",
      "Loss at step 54700 : 2.7976226806640625\n",
      "Loss at step 54750 : 3.4719924926757812\n",
      "Loss at step 54800 : 2.229628562927246\n",
      "Loss at step 54850 : 3.07464599609375\n",
      "Loss at step 54900 : 2.7670116424560547\n",
      "Loss at step 54950 : 3.780566453933716\n",
      "Loss at step 55000 : 2.620802402496338\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, moisture,\n",
      "Nearest to rice: thigh, mandu, lime, lunch, carrot,\n",
      "Nearest to sushi: piece, layer, sashimi, sesame, cut,\n",
      "Nearest to roll: sheet, worcestershire, oil, meal, heart,\n",
      "Nearest to sashimi: fryer, sushi, seaweed, spread, gyoza,\n",
      "Nearest to steak: maple, paste, eye, beef, milliliter,\n",
      "Nearest to grill: soy, chicken, teriyaki, rack, paper,\n",
      "Nearest to sauce: jack, wasabi, peel, soy, pat,\n",
      "Nearest to cream: dente, skinless, pastry, sieve, extract,\n",
      "Nearest to cheesecake: cashew, flake, flour, paper, tin,\n",
      "Nearest to pizza: white, flake, mixture, run, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, almond, teriyaki,\n",
      "Nearest to hamburger: meat, herb, parsley, pine, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 55050 : 3.6738901138305664\n",
      "Loss at step 55100 : 3.147217035293579\n",
      "Loss at step 55150 : 2.429119825363159\n",
      "Loss at step 55200 : 1.9258347749710083\n",
      "Loss at step 55250 : 3.5842580795288086\n",
      "Loss at step 55300 : 3.2837698459625244\n",
      "Loss at step 55350 : 2.040353298187256\n",
      "Loss at step 55400 : 3.7584033012390137\n",
      "Loss at step 55450 : 2.368849039077759\n",
      "Loss at step 55500 : 1.9571268558502197\n",
      "Loss at step 55550 : 2.949833393096924\n",
      "Loss at step 55600 : 3.155867338180542\n",
      "Loss at step 55650 : 2.0277721881866455\n",
      "Loss at step 55700 : 3.0163497924804688\n",
      "Loss at step 55750 : 3.3891236782073975\n",
      "Loss at step 55800 : 3.1163365840911865\n",
      "Loss at step 55850 : 2.794003963470459\n",
      "Loss at step 55900 : 2.357898712158203\n",
      "Loss at step 55950 : 2.9046058654785156\n",
      "Loss at step 56000 : 3.4508137702941895\n",
      "Loss at step 56050 : 3.0743908882141113\n",
      "Loss at step 56100 : 3.260974884033203\n",
      "Loss at step 56150 : 4.031817436218262\n",
      "Loss at step 56200 : 3.6356258392333984\n",
      "Loss at step 56250 : 3.5804500579833984\n",
      "Loss at step 56300 : 3.4230058193206787\n",
      "Loss at step 56350 : 3.244734764099121\n",
      "Loss at step 56400 : 3.234231948852539\n",
      "Loss at step 56450 : 3.5820090770721436\n",
      "Loss at step 56500 : 2.1129190921783447\n",
      "Loss at step 56550 : 3.339907169342041\n",
      "Loss at step 56600 : 2.5332889556884766\n",
      "Loss at step 56650 : 3.629091739654541\n",
      "Loss at step 56700 : 3.4835972785949707\n",
      "Loss at step 56750 : 3.47512149810791\n",
      "Loss at step 56800 : 3.0629899501800537\n",
      "Loss at step 56850 : 1.8753571510314941\n",
      "Loss at step 56900 : 3.0591511726379395\n",
      "Loss at step 56950 : 2.848017692565918\n",
      "Loss at step 57000 : 1.7774208784103394\n",
      "Loss at step 57050 : 1.8731608390808105\n",
      "Loss at step 57100 : 3.216660261154175\n",
      "Loss at step 57150 : 2.6574950218200684\n",
      "Loss at step 57200 : 2.48663067817688\n",
      "Loss at step 57250 : 2.0637872219085693\n",
      "Loss at step 57300 : 3.3721156120300293\n",
      "Loss at step 57350 : 3.36726975440979\n",
      "Loss at step 57400 : 3.4878907203674316\n",
      "Loss at step 57450 : 3.6330807209014893\n",
      "Loss at step 57500 : 4.112184047698975\n",
      "Loss at step 57550 : 2.9788408279418945\n",
      "Loss at step 57600 : 1.997819423675537\n",
      "Loss at step 57650 : 2.9902241230010986\n",
      "Loss at step 57700 : 3.0121755599975586\n",
      "Loss at step 57750 : 3.2446937561035156\n",
      "Loss at step 57800 : 2.218170642852783\n",
      "Loss at step 57850 : 2.835664987564087\n",
      "Loss at step 57900 : 2.4477529525756836\n",
      "Loss at step 57950 : 2.4214553833007812\n",
      "Loss at step 58000 : 3.170537233352661\n",
      "Loss at step 58050 : 3.1086769104003906\n",
      "Loss at step 58100 : 2.978504180908203\n",
      "Loss at step 58150 : 2.21026611328125\n",
      "Loss at step 58200 : 2.736570358276367\n",
      "Loss at step 58250 : 3.1809725761413574\n",
      "Loss at step 58300 : 3.595407724380493\n",
      "Loss at step 58350 : 2.424036741256714\n",
      "Loss at step 58400 : 2.9939470291137695\n",
      "Loss at step 58450 : 3.288083553314209\n",
      "Loss at step 58500 : 3.8700766563415527\n",
      "Loss at step 58550 : 2.6601381301879883\n",
      "Loss at step 58600 : 2.686467170715332\n",
      "Loss at step 58650 : 2.3486416339874268\n",
      "Loss at step 58700 : 2.5817787647247314\n",
      "Loss at step 58750 : 4.387202262878418\n",
      "Loss at step 58800 : 2.618936777114868\n",
      "Loss at step 58850 : 3.3702902793884277\n",
      "Loss at step 58900 : 2.6115031242370605\n",
      "Loss at step 58950 : 2.125519037246704\n",
      "Loss at step 59000 : 2.920412302017212\n",
      "Loss at step 59050 : 3.5759222507476807\n",
      "Loss at step 59100 : 3.2720773220062256\n",
      "Loss at step 59150 : 3.3232715129852295\n",
      "Loss at step 59200 : 3.3198792934417725\n",
      "Loss at step 59250 : 3.364642381668091\n",
      "Loss at step 59300 : 3.0413687229156494\n",
      "Loss at step 59350 : 3.1038641929626465\n",
      "Loss at step 59400 : 2.5565571784973145\n",
      "Loss at step 59450 : 2.142850637435913\n",
      "Loss at step 59500 : 3.09352970123291\n",
      "Loss at step 59550 : 4.120115280151367\n",
      "Loss at step 59600 : 3.6045279502868652\n",
      "Loss at step 59650 : 3.510270118713379\n",
      "Loss at step 59700 : 3.3245410919189453\n",
      "Loss at step 59750 : 3.7744457721710205\n",
      "Loss at step 59800 : 2.889280080795288\n",
      "Loss at step 59850 : 2.881183624267578\n",
      "Loss at step 59900 : 3.6389660835266113\n",
      "Loss at step 59950 : 3.280773162841797\n",
      "Loss at step 60000 : 2.048459529876709\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, beef,\n",
      "Nearest to rice: thigh, mandu, lime, lunch, carrot,\n",
      "Nearest to sushi: piece, layer, sesame, rice, cornstarch,\n",
      "Nearest to roll: sheet, worcestershire, oil, meal, heart,\n",
      "Nearest to sashimi: fryer, seaweed, gyoza, spread, sushi,\n",
      "Nearest to steak: maple, paste, eye, beef, meat,\n",
      "Nearest to grill: soy, chicken, teriyaki, rack, paper,\n",
      "Nearest to sauce: jack, wasabi, peel, pat, mat,\n",
      "Nearest to cream: skinless, dente, pastry, extract, sieve,\n",
      "Nearest to cheesecake: cashew, flake, flour, paper, tin,\n",
      "Nearest to pizza: white, flake, mixture, run, baguette,\n",
      "Nearest to lasagna: fry, rare, almond, lemon, teriyaki,\n",
      "Nearest to hamburger: meat, herb, parsley, pine, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 60050 : 2.597217321395874\n",
      "Loss at step 60100 : 4.430568695068359\n",
      "Loss at step 60150 : 3.6409409046173096\n",
      "Loss at step 60200 : 3.0714099407196045\n",
      "Loss at step 60250 : 3.439767837524414\n",
      "Loss at step 60300 : 2.771164894104004\n",
      "Loss at step 60350 : 2.4171409606933594\n",
      "Loss at step 60400 : 3.4437694549560547\n",
      "Loss at step 60450 : 4.0092949867248535\n",
      "Loss at step 60500 : 2.428593158721924\n",
      "Loss at step 60550 : 2.411702871322632\n",
      "Loss at step 60600 : 2.9845948219299316\n",
      "Loss at step 60650 : 3.022125720977783\n",
      "Loss at step 60700 : 3.6908159255981445\n",
      "Loss at step 60750 : 3.578768253326416\n",
      "Loss at step 60800 : 3.3874378204345703\n",
      "Loss at step 60850 : 3.2333879470825195\n",
      "Loss at step 60900 : 3.0777926445007324\n",
      "Loss at step 60950 : 2.3419461250305176\n",
      "Loss at step 61000 : 3.4984869956970215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 61050 : 4.080774784088135\n",
      "Loss at step 61100 : 3.4297566413879395\n",
      "Loss at step 61150 : 3.6220760345458984\n",
      "Loss at step 61200 : 2.842228412628174\n",
      "Loss at step 61250 : 2.427370548248291\n",
      "Loss at step 61300 : 2.4600939750671387\n",
      "Loss at step 61350 : 3.780013084411621\n",
      "Loss at step 61400 : 3.6605725288391113\n",
      "Loss at step 61450 : 3.1125550270080566\n",
      "Loss at step 61500 : 1.9812142848968506\n",
      "Loss at step 61550 : 3.715433120727539\n",
      "Loss at step 61600 : 2.7501182556152344\n",
      "Loss at step 61650 : 2.490678310394287\n",
      "Loss at step 61700 : 3.114828109741211\n",
      "Loss at step 61750 : 2.5564053058624268\n",
      "Loss at step 61800 : 3.0493626594543457\n",
      "Loss at step 61850 : 3.7117056846618652\n",
      "Loss at step 61900 : 3.0545358657836914\n",
      "Loss at step 61950 : 3.4676194190979004\n",
      "Loss at step 62000 : 2.916902542114258\n",
      "Loss at step 62050 : 3.3921144008636475\n",
      "Loss at step 62100 : 4.0280914306640625\n",
      "Loss at step 62150 : 3.8773672580718994\n",
      "Loss at step 62200 : 3.6122353076934814\n",
      "Loss at step 62250 : 2.92958664894104\n",
      "Loss at step 62300 : 2.333632469177246\n",
      "Loss at step 62350 : 2.066291332244873\n",
      "Loss at step 62400 : 2.8901565074920654\n",
      "Loss at step 62450 : 4.119160175323486\n",
      "Loss at step 62500 : 2.2553114891052246\n",
      "Loss at step 62550 : 3.009462594985962\n",
      "Loss at step 62600 : 1.9803752899169922\n",
      "Loss at step 62650 : 2.4213621616363525\n",
      "Loss at step 62700 : 2.9778995513916016\n",
      "Loss at step 62750 : 2.1681745052337646\n",
      "Loss at step 62800 : 2.94779896736145\n",
      "Loss at step 62850 : 3.098968505859375\n",
      "Loss at step 62900 : 2.908459424972534\n",
      "Loss at step 62950 : 2.7148876190185547\n",
      "Loss at step 63000 : 2.7664918899536133\n",
      "Loss at step 63050 : 2.4898345470428467\n",
      "Loss at step 63100 : 2.7314798831939697\n",
      "Loss at step 63150 : 4.225198745727539\n",
      "Loss at step 63200 : 2.24069881439209\n",
      "Loss at step 63250 : 2.8014464378356934\n",
      "Loss at step 63300 : 2.9369618892669678\n",
      "Loss at step 63350 : 3.5126657485961914\n",
      "Loss at step 63400 : 2.1769156455993652\n",
      "Loss at step 63450 : 2.3800745010375977\n",
      "Loss at step 63500 : 2.844331979751587\n",
      "Loss at step 63550 : 2.5967679023742676\n",
      "Loss at step 63600 : 3.0279736518859863\n",
      "Loss at step 63650 : 3.3841726779937744\n",
      "Loss at step 63700 : 2.890418291091919\n",
      "Loss at step 63750 : 2.2691798210144043\n",
      "Loss at step 63800 : 2.610849380493164\n",
      "Loss at step 63850 : 3.9884204864501953\n",
      "Loss at step 63900 : 4.569622039794922\n",
      "Loss at step 63950 : 2.3316473960876465\n",
      "Loss at step 64000 : 3.0968477725982666\n",
      "Loss at step 64050 : 2.3068296909332275\n",
      "Loss at step 64100 : 2.456718921661377\n",
      "Loss at step 64150 : 3.317906379699707\n",
      "Loss at step 64200 : 3.2220160961151123\n",
      "Loss at step 64250 : 3.1624412536621094\n",
      "Loss at step 64300 : 2.751659631729126\n",
      "Loss at step 64350 : 3.009600877761841\n",
      "Loss at step 64400 : 3.5901598930358887\n",
      "Loss at step 64450 : 2.855257987976074\n",
      "Loss at step 64500 : 2.6283860206604004\n",
      "Loss at step 64550 : 3.8147759437561035\n",
      "Loss at step 64600 : 2.963226795196533\n",
      "Loss at step 64650 : 3.1365222930908203\n",
      "Loss at step 64700 : 4.711378574371338\n",
      "Loss at step 64750 : 2.967327117919922\n",
      "Loss at step 64800 : 3.7767152786254883\n",
      "Loss at step 64850 : 3.4213809967041016\n",
      "Loss at step 64900 : 2.894608974456787\n",
      "Loss at step 64950 : 2.6041324138641357\n",
      "Loss at step 65000 : 3.201486587524414\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, beef,\n",
      "Nearest to rice: thigh, lime, mandu, lunch, beaten,\n",
      "Nearest to sushi: piece, sesame, layer, rice, cut,\n",
      "Nearest to roll: sheet, worcestershire, oil, meal, ball,\n",
      "Nearest to sashimi: fryer, sushi, seaweed, gyoza, spread,\n",
      "Nearest to steak: maple, paste, eye, meat, beef,\n",
      "Nearest to grill: soy, teriyaki, chicken, rack, cider,\n",
      "Nearest to sauce: jack, wasabi, pat, peel, soy,\n",
      "Nearest to cream: dente, skinless, extract, pastry, toast,\n",
      "Nearest to cheesecake: cashew, flour, flake, paper, tin,\n",
      "Nearest to pizza: white, mixture, flake, run, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, almond, teriyaki,\n",
      "Nearest to hamburger: meat, herb, parsley, pine, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 65050 : 4.154467582702637\n",
      "Loss at step 65100 : 3.499721050262451\n",
      "Loss at step 65150 : 2.3171651363372803\n",
      "Loss at step 65200 : 3.602012872695923\n",
      "Loss at step 65250 : 2.565692901611328\n",
      "Loss at step 65300 : 1.8752565383911133\n",
      "Loss at step 65350 : 3.791074752807617\n",
      "Loss at step 65400 : 3.0673270225524902\n",
      "Loss at step 65450 : 3.1218786239624023\n",
      "Loss at step 65500 : 2.411240577697754\n",
      "Loss at step 65550 : 2.848707437515259\n",
      "Loss at step 65600 : 2.777547836303711\n",
      "Loss at step 65650 : 2.6200432777404785\n",
      "Loss at step 65700 : 3.318955659866333\n",
      "Loss at step 65750 : 4.222825527191162\n",
      "Loss at step 65800 : 4.1298041343688965\n",
      "Loss at step 65850 : 3.031768798828125\n",
      "Loss at step 65900 : 2.8469293117523193\n",
      "Loss at step 65950 : 2.904287338256836\n",
      "Loss at step 66000 : 2.651379108428955\n",
      "Loss at step 66050 : 2.794532299041748\n",
      "Loss at step 66100 : 3.1643996238708496\n",
      "Loss at step 66150 : 2.562718391418457\n",
      "Loss at step 66200 : 2.67724609375\n",
      "Loss at step 66250 : 3.703927755355835\n",
      "Loss at step 66300 : 2.5719194412231445\n",
      "Loss at step 66350 : 2.288954496383667\n",
      "Loss at step 66400 : 3.4104607105255127\n",
      "Loss at step 66450 : 2.889533519744873\n",
      "Loss at step 66500 : 2.5919103622436523\n",
      "Loss at step 66550 : 3.5759072303771973\n",
      "Loss at step 66600 : 3.306910991668701\n",
      "Loss at step 66650 : 2.8956925868988037\n",
      "Loss at step 66700 : 3.061537504196167\n",
      "Loss at step 66750 : 2.106971263885498\n",
      "Loss at step 66800 : 3.714370012283325\n",
      "Loss at step 66850 : 3.1282334327697754\n",
      "Loss at step 66900 : 3.298391580581665\n",
      "Loss at step 66950 : 3.0362777709960938\n",
      "Loss at step 67000 : 2.779959201812744\n",
      "Loss at step 67050 : 2.498922109603882\n",
      "Loss at step 67100 : 3.516665458679199\n",
      "Loss at step 67150 : 3.780057430267334\n",
      "Loss at step 67200 : 3.420787811279297\n",
      "Loss at step 67250 : 3.7960143089294434\n",
      "Loss at step 67300 : 2.448963165283203\n",
      "Loss at step 67350 : 2.9784088134765625\n",
      "Loss at step 67400 : 2.6894617080688477\n",
      "Loss at step 67450 : 2.463827610015869\n",
      "Loss at step 67500 : 1.7287979125976562\n",
      "Loss at step 67550 : 3.264151096343994\n",
      "Loss at step 67600 : 4.061468124389648\n",
      "Loss at step 67650 : 3.0034918785095215\n",
      "Loss at step 67700 : 3.5269107818603516\n",
      "Loss at step 67750 : 2.751187801361084\n",
      "Loss at step 67800 : 3.0447473526000977\n",
      "Loss at step 67850 : 2.7761216163635254\n",
      "Loss at step 67900 : 3.277924060821533\n",
      "Loss at step 67950 : 3.0665392875671387\n",
      "Loss at step 68000 : 3.4097206592559814\n",
      "Loss at step 68050 : 2.366302013397217\n",
      "Loss at step 68100 : 2.841259241104126\n",
      "Loss at step 68150 : 3.319605588912964\n",
      "Loss at step 68200 : 3.2242653369903564\n",
      "Loss at step 68250 : 3.7337496280670166\n",
      "Loss at step 68300 : 2.01399827003479\n",
      "Loss at step 68350 : 3.034745216369629\n",
      "Loss at step 68400 : 3.853813886642456\n",
      "Loss at step 68450 : 2.1152122020721436\n",
      "Loss at step 68500 : 2.773141860961914\n",
      "Loss at step 68550 : 3.0698328018188477\n",
      "Loss at step 68600 : 2.11136531829834\n",
      "Loss at step 68650 : 2.2515106201171875\n",
      "Loss at step 68700 : 2.7998604774475098\n",
      "Loss at step 68750 : 2.892603874206543\n",
      "Loss at step 68800 : 3.5724985599517822\n",
      "Loss at step 68850 : 3.364380359649658\n",
      "Loss at step 68900 : 3.6317646503448486\n",
      "Loss at step 68950 : 2.981903553009033\n",
      "Loss at step 69000 : 3.8987441062927246\n",
      "Loss at step 69050 : 2.3337531089782715\n",
      "Loss at step 69100 : 3.046903133392334\n",
      "Loss at step 69150 : 2.6628479957580566\n",
      "Loss at step 69200 : 3.3033084869384766\n",
      "Loss at step 69250 : 2.957824468612671\n",
      "Loss at step 69300 : 3.4906697273254395\n",
      "Loss at step 69350 : 2.9591023921966553\n",
      "Loss at step 69400 : 3.7248754501342773\n",
      "Loss at step 69450 : 3.393439292907715\n",
      "Loss at step 69500 : 2.82438325881958\n",
      "Loss at step 69550 : 2.3705105781555176\n",
      "Loss at step 69600 : 3.383249521255493\n",
      "Loss at step 69650 : 2.946814775466919\n",
      "Loss at step 69700 : 2.580869197845459\n",
      "Loss at step 69750 : 4.281379222869873\n",
      "Loss at step 69800 : 3.2802488803863525\n",
      "Loss at step 69850 : 3.3483643531799316\n",
      "Loss at step 69900 : 3.6120238304138184\n",
      "Loss at step 69950 : 2.5659866333007812\n",
      "Loss at step 70000 : 2.9219131469726562\n",
      "Nearest to tuna: parmesan, eye, metal, virgin, moisture,\n",
      "Nearest to rice: thigh, lime, mandu, beaten, lunch,\n",
      "Nearest to sushi: piece, sesame, rice, layer, cut,\n",
      "Nearest to roll: sheet, oil, worcestershire, meal, ball,\n",
      "Nearest to sashimi: fryer, sushi, gyoza, seaweed, spread,\n",
      "Nearest to steak: maple, paste, eye, meat, beef,\n",
      "Nearest to grill: soy, teriyaki, chicken, rack, mesh,\n",
      "Nearest to sauce: jack, wasabi, pat, oregano, peel,\n",
      "Nearest to cream: skinless, dente, extract, pastry, sieve,\n",
      "Nearest to cheesecake: cashew, flour, flake, paper, tin,\n",
      "Nearest to pizza: white, mixture, flake, run, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, almond, teriyaki,\n",
      "Nearest to hamburger: meat, herb, pine, parsley, confectioner,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 70050 : 3.36217999458313\n",
      "Loss at step 70100 : 3.160017490386963\n",
      "Loss at step 70150 : 2.4968838691711426\n",
      "Loss at step 70200 : 3.0374503135681152\n",
      "Loss at step 70250 : 2.582841157913208\n",
      "Loss at step 70300 : 2.9583163261413574\n",
      "Loss at step 70350 : 3.5224251747131348\n",
      "Loss at step 70400 : 4.256953239440918\n",
      "Loss at step 70450 : 1.9281361103057861\n",
      "Loss at step 70500 : 1.8299975395202637\n",
      "Loss at step 70550 : 3.381807804107666\n",
      "Loss at step 70600 : 3.2997121810913086\n",
      "Loss at step 70650 : 2.8174195289611816\n",
      "Loss at step 70700 : 2.872687816619873\n",
      "Loss at step 70750 : 3.1522884368896484\n",
      "Loss at step 70800 : 2.870661973953247\n",
      "Loss at step 70850 : 2.6671009063720703\n",
      "Loss at step 70900 : 3.3460240364074707\n",
      "Loss at step 70950 : 3.2811334133148193\n",
      "Loss at step 71000 : 3.106703281402588\n",
      "Loss at step 71050 : 3.2512238025665283\n",
      "Loss at step 71100 : 2.2738423347473145\n",
      "Loss at step 71150 : 3.3883256912231445\n",
      "Loss at step 71200 : 2.9605743885040283\n",
      "Loss at step 71250 : 3.0825629234313965\n",
      "Loss at step 71300 : 2.4128787517547607\n",
      "Loss at step 71350 : 4.328339576721191\n",
      "Loss at step 71400 : 3.1286230087280273\n",
      "Loss at step 71450 : 3.1757373809814453\n",
      "Loss at step 71500 : 3.6142046451568604\n",
      "Loss at step 71550 : 2.8500428199768066\n",
      "Loss at step 71600 : 3.4967570304870605\n",
      "Loss at step 71650 : 2.4511771202087402\n",
      "Loss at step 71700 : 3.2038984298706055\n",
      "Loss at step 71750 : 2.9630908966064453\n",
      "Loss at step 71800 : 2.84454345703125\n",
      "Loss at step 71850 : 3.0584354400634766\n",
      "Loss at step 71900 : 3.886997699737549\n",
      "Loss at step 71950 : 3.4802167415618896\n",
      "Loss at step 72000 : 2.7312488555908203\n",
      "Loss at step 72050 : 3.8428754806518555\n",
      "Loss at step 72100 : 2.6610257625579834\n",
      "Loss at step 72150 : 2.7884442806243896\n",
      "Loss at step 72200 : 2.6771252155303955\n",
      "Loss at step 72250 : 2.977572441101074\n",
      "Loss at step 72300 : 2.327726125717163\n",
      "Loss at step 72350 : 3.9496653079986572\n",
      "Loss at step 72400 : 2.506692886352539\n",
      "Loss at step 72450 : 2.9549601078033447\n",
      "Loss at step 72500 : 3.5647659301757812\n",
      "Loss at step 72550 : 2.290457248687744\n",
      "Loss at step 72600 : 3.223754405975342\n",
      "Loss at step 72650 : 2.931422472000122\n",
      "Loss at step 72700 : 3.2748897075653076\n",
      "Loss at step 72750 : 2.6324126720428467\n",
      "Loss at step 72800 : 3.681485414505005\n",
      "Loss at step 72850 : 3.0487234592437744\n",
      "Loss at step 72900 : 3.3169186115264893\n",
      "Loss at step 72950 : 3.068202018737793\n",
      "Loss at step 73000 : 1.9016377925872803\n",
      "Loss at step 73050 : 2.7002477645874023\n",
      "Loss at step 73100 : 2.898939371109009\n",
      "Loss at step 73150 : 2.205019950866699\n",
      "Loss at step 73200 : 2.306431293487549\n",
      "Loss at step 73250 : 1.51810884475708\n",
      "Loss at step 73300 : 2.592472553253174\n",
      "Loss at step 73350 : 3.853611469268799\n",
      "Loss at step 73400 : 3.864706516265869\n",
      "Loss at step 73450 : 2.994828701019287\n",
      "Loss at step 73500 : 2.780590295791626\n",
      "Loss at step 73550 : 3.465693473815918\n",
      "Loss at step 73600 : 3.209583282470703\n",
      "Loss at step 73650 : 3.44950532913208\n",
      "Loss at step 73700 : 3.722750425338745\n",
      "Loss at step 73750 : 2.7508225440979004\n",
      "Loss at step 73800 : 2.1458871364593506\n",
      "Loss at step 73850 : 3.6754095554351807\n",
      "Loss at step 73900 : 3.909029245376587\n",
      "Loss at step 73950 : 2.4253287315368652\n",
      "Loss at step 74000 : 4.337622165679932\n",
      "Loss at step 74050 : 3.0675909519195557\n",
      "Loss at step 74100 : 3.106492280960083\n",
      "Loss at step 74150 : 2.849459171295166\n",
      "Loss at step 74200 : 3.1358542442321777\n",
      "Loss at step 74250 : 3.3398334980010986\n",
      "Loss at step 74300 : 2.7305290699005127\n",
      "Loss at step 74350 : 3.8973584175109863\n",
      "Loss at step 74400 : 2.5839481353759766\n",
      "Loss at step 74450 : 3.205768346786499\n",
      "Loss at step 74500 : 2.616443634033203\n",
      "Loss at step 74550 : 3.739656925201416\n",
      "Loss at step 74600 : 2.7027413845062256\n",
      "Loss at step 74650 : 3.676187038421631\n",
      "Loss at step 74700 : 2.062403440475464\n",
      "Loss at step 74750 : 3.2217507362365723\n",
      "Loss at step 74800 : 3.3062307834625244\n",
      "Loss at step 74850 : 2.6280956268310547\n",
      "Loss at step 74900 : 2.463170051574707\n",
      "Loss at step 74950 : 3.169067621231079\n",
      "Loss at step 75000 : 3.5427536964416504\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, moisture,\n",
      "Nearest to rice: thigh, lime, mandu, beaten, lunch,\n",
      "Nearest to sushi: sesame, piece, layer, sashimi, cut,\n",
      "Nearest to roll: sheet, oil, worcestershire, meal, ball,\n",
      "Nearest to sashimi: fryer, sushi, spread, gyoza, seaweed,\n",
      "Nearest to steak: maple, paste, eye, meat, beef,\n",
      "Nearest to grill: soy, teriyaki, cider, chicken, rack,\n",
      "Nearest to sauce: jack, wasabi, peel, oregano, pat,\n",
      "Nearest to cream: dente, skinless, extract, peak, dessert,\n",
      "Nearest to cheesecake: cashew, flake, paper, flour, tin,\n",
      "Nearest to pizza: white, mixture, flake, baguette, run,\n",
      "Nearest to lasagna: fry, lemon, rare, almond, teriyaki,\n",
      "Nearest to hamburger: meat, herb, pine, parsley, enjoy,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 75050 : 2.853602409362793\n",
      "Loss at step 75100 : 3.447101593017578\n",
      "Loss at step 75150 : 2.5386040210723877\n",
      "Loss at step 75200 : 3.70847487449646\n",
      "Loss at step 75250 : 2.9495692253112793\n",
      "Loss at step 75300 : 3.2169289588928223\n",
      "Loss at step 75350 : 3.9273934364318848\n",
      "Loss at step 75400 : 2.985706329345703\n",
      "Loss at step 75450 : 3.118489980697632\n",
      "Loss at step 75500 : 3.083434581756592\n",
      "Loss at step 75550 : 2.9662418365478516\n",
      "Loss at step 75600 : 3.3453524112701416\n",
      "Loss at step 75650 : 2.6458375453948975\n",
      "Loss at step 75700 : 3.4487833976745605\n",
      "Loss at step 75750 : 3.9800403118133545\n",
      "Loss at step 75800 : 2.8284568786621094\n",
      "Loss at step 75850 : 2.2446389198303223\n",
      "Loss at step 75900 : 3.259378433227539\n",
      "Loss at step 75950 : 3.477933406829834\n",
      "Loss at step 76000 : 3.301650047302246\n",
      "Loss at step 76050 : 2.8465681076049805\n",
      "Loss at step 76100 : 2.2758631706237793\n",
      "Loss at step 76150 : 2.5517430305480957\n",
      "Loss at step 76200 : 3.563986301422119\n",
      "Loss at step 76250 : 2.2272136211395264\n",
      "Loss at step 76300 : 4.0439534187316895\n",
      "Loss at step 76350 : 2.9589426517486572\n",
      "Loss at step 76400 : 3.5889806747436523\n",
      "Loss at step 76450 : 2.461242198944092\n",
      "Loss at step 76500 : 3.1737301349639893\n",
      "Loss at step 76550 : 3.4031307697296143\n",
      "Loss at step 76600 : 3.3165335655212402\n",
      "Loss at step 76650 : 3.8933067321777344\n",
      "Loss at step 76700 : 3.8325798511505127\n",
      "Loss at step 76750 : 2.5189709663391113\n",
      "Loss at step 76800 : 3.0407631397247314\n",
      "Loss at step 76850 : 2.5719079971313477\n",
      "Loss at step 76900 : 3.033182144165039\n",
      "Loss at step 76950 : 2.7611184120178223\n",
      "Loss at step 77000 : 3.017578601837158\n",
      "Loss at step 77050 : 2.462064266204834\n",
      "Loss at step 77100 : 2.826641082763672\n",
      "Loss at step 77150 : 1.9919154644012451\n",
      "Loss at step 77200 : 2.7154428958892822\n",
      "Loss at step 77250 : 3.060349225997925\n",
      "Loss at step 77300 : 3.117014169692993\n",
      "Loss at step 77350 : 2.7639822959899902\n",
      "Loss at step 77400 : 2.80288028717041\n",
      "Loss at step 77450 : 2.8466076850891113\n",
      "Loss at step 77500 : 3.1137218475341797\n",
      "Loss at step 77550 : 3.270939350128174\n",
      "Loss at step 77600 : 3.9354231357574463\n",
      "Loss at step 77650 : 2.3706085681915283\n",
      "Loss at step 77700 : 4.05730676651001\n",
      "Loss at step 77750 : 3.399393320083618\n",
      "Loss at step 77800 : 2.4635231494903564\n",
      "Loss at step 77850 : 3.7598936557769775\n",
      "Loss at step 77900 : 3.118089437484741\n",
      "Loss at step 77950 : 2.4539170265197754\n",
      "Loss at step 78000 : 3.4328036308288574\n",
      "Loss at step 78050 : 2.752758026123047\n",
      "Loss at step 78100 : 3.486664295196533\n",
      "Loss at step 78150 : 2.7368011474609375\n",
      "Loss at step 78200 : 2.510453462600708\n",
      "Loss at step 78250 : 2.38090181350708\n",
      "Loss at step 78300 : 3.3520994186401367\n",
      "Loss at step 78350 : 3.9807960987091064\n",
      "Loss at step 78400 : 3.8069045543670654\n",
      "Loss at step 78450 : 2.975353240966797\n",
      "Loss at step 78500 : 2.8635082244873047\n",
      "Loss at step 78550 : 2.6009132862091064\n",
      "Loss at step 78600 : 3.0373644828796387\n",
      "Loss at step 78650 : 3.1534552574157715\n",
      "Loss at step 78700 : 2.7874746322631836\n",
      "Loss at step 78750 : 2.5623090267181396\n",
      "Loss at step 78800 : 3.212573528289795\n",
      "Loss at step 78850 : 3.0542144775390625\n",
      "Loss at step 78900 : 2.433096170425415\n",
      "Loss at step 78950 : 2.183473825454712\n",
      "Loss at step 79000 : 2.9420571327209473\n",
      "Loss at step 79050 : 3.346407175064087\n",
      "Loss at step 79100 : 2.078679084777832\n",
      "Loss at step 79150 : 2.450411558151245\n",
      "Loss at step 79200 : 3.759298801422119\n",
      "Loss at step 79250 : 3.1941237449645996\n",
      "Loss at step 79300 : 2.3617639541625977\n",
      "Loss at step 79350 : 2.848742961883545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 79400 : 3.131545066833496\n",
      "Loss at step 79450 : 3.933582305908203\n",
      "Loss at step 79500 : 2.789793014526367\n",
      "Loss at step 79550 : 2.5185294151306152\n",
      "Loss at step 79600 : 3.511779546737671\n",
      "Loss at step 79650 : 3.0292601585388184\n",
      "Loss at step 79700 : 3.923837184906006\n",
      "Loss at step 79750 : 3.0056207180023193\n",
      "Loss at step 79800 : 3.192819118499756\n",
      "Loss at step 79850 : 2.6625571250915527\n",
      "Loss at step 79900 : 2.395799160003662\n",
      "Loss at step 79950 : 3.8034396171569824\n",
      "Loss at step 80000 : 2.3671109676361084\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, moisture,\n",
      "Nearest to rice: thigh, mandu, lime, beaten, carrot,\n",
      "Nearest to sushi: sesame, piece, layer, sashimi, cut,\n",
      "Nearest to roll: sheet, worcestershire, oil, meal, crumb,\n",
      "Nearest to sashimi: fryer, sushi, gyoza, spread, seaweed,\n",
      "Nearest to steak: maple, paste, eye, meat, grate,\n",
      "Nearest to grill: teriyaki, soy, cider, chicken, rack,\n",
      "Nearest to sauce: jack, wasabi, soy, oregano, peel,\n",
      "Nearest to cream: dente, skinless, pastry, sieve, chocolate,\n",
      "Nearest to cheesecake: cashew, flake, paper, nutmeg, tin,\n",
      "Nearest to pizza: white, mixture, flake, run, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, almond, teriyaki,\n",
      "Nearest to hamburger: meat, herb, pine, parsley, confectioner,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 80050 : 3.618135929107666\n",
      "Loss at step 80100 : 2.45828914642334\n",
      "Loss at step 80150 : 3.1211087703704834\n",
      "Loss at step 80200 : 3.1965277194976807\n",
      "Loss at step 80250 : 3.25057315826416\n",
      "Loss at step 80300 : 2.674527168273926\n",
      "Loss at step 80350 : 2.396007537841797\n",
      "Loss at step 80400 : 2.6896181106567383\n",
      "Loss at step 80450 : 2.1576714515686035\n",
      "Loss at step 80500 : 3.42965030670166\n",
      "Loss at step 80550 : 2.2593772411346436\n",
      "Loss at step 80600 : 2.807097911834717\n",
      "Loss at step 80650 : 3.108168125152588\n",
      "Loss at step 80700 : 2.6580166816711426\n",
      "Loss at step 80750 : 2.8976471424102783\n",
      "Loss at step 80800 : 3.4188408851623535\n",
      "Loss at step 80850 : 2.7788162231445312\n",
      "Loss at step 80900 : 2.6856017112731934\n",
      "Loss at step 80950 : 2.076411008834839\n",
      "Loss at step 81000 : 2.813058614730835\n",
      "Loss at step 81050 : 2.1796817779541016\n",
      "Loss at step 81100 : 2.300603151321411\n",
      "Loss at step 81150 : 3.5260720252990723\n",
      "Loss at step 81200 : 3.3381292819976807\n",
      "Loss at step 81250 : 3.31980299949646\n",
      "Loss at step 81300 : 2.5575830936431885\n",
      "Loss at step 81350 : 2.8369030952453613\n",
      "Loss at step 81400 : 1.7917364835739136\n",
      "Loss at step 81450 : 2.511277198791504\n",
      "Loss at step 81500 : 4.033717155456543\n",
      "Loss at step 81550 : 3.106363296508789\n",
      "Loss at step 81600 : 2.6494858264923096\n",
      "Loss at step 81650 : 3.242863655090332\n",
      "Loss at step 81700 : 2.152601718902588\n",
      "Loss at step 81750 : 2.5896191596984863\n",
      "Loss at step 81800 : 3.2725296020507812\n",
      "Loss at step 81850 : 2.205148220062256\n",
      "Loss at step 81900 : 3.3701839447021484\n",
      "Loss at step 81950 : 3.1183829307556152\n",
      "Loss at step 82000 : 3.052870512008667\n",
      "Loss at step 82050 : 2.552623748779297\n",
      "Loss at step 82100 : 3.413745164871216\n",
      "Loss at step 82150 : 3.5836610794067383\n",
      "Loss at step 82200 : 2.5915403366088867\n",
      "Loss at step 82250 : 3.292942523956299\n",
      "Loss at step 82300 : 2.534501075744629\n",
      "Loss at step 82350 : 3.2602782249450684\n",
      "Loss at step 82400 : 3.406038284301758\n",
      "Loss at step 82450 : 2.7987115383148193\n",
      "Loss at step 82500 : 2.9641599655151367\n",
      "Loss at step 82550 : 2.7573769092559814\n",
      "Loss at step 82600 : 3.304304838180542\n",
      "Loss at step 82650 : 2.8988890647888184\n",
      "Loss at step 82700 : 3.1605582237243652\n",
      "Loss at step 82750 : 2.537079334259033\n",
      "Loss at step 82800 : 3.1475722789764404\n",
      "Loss at step 82850 : 2.9940943717956543\n",
      "Loss at step 82900 : 2.615995407104492\n",
      "Loss at step 82950 : 2.2106244564056396\n",
      "Loss at step 83000 : 4.023870468139648\n",
      "Loss at step 83050 : 2.957423686981201\n",
      "Loss at step 83100 : 2.575345039367676\n",
      "Loss at step 83150 : 3.9148924350738525\n",
      "Loss at step 83200 : 3.7709286212921143\n",
      "Loss at step 83250 : 3.0491933822631836\n",
      "Loss at step 83300 : 2.275482654571533\n",
      "Loss at step 83350 : 3.454881191253662\n",
      "Loss at step 83400 : 2.7340331077575684\n",
      "Loss at step 83450 : 3.5424275398254395\n",
      "Loss at step 83500 : 2.7434051036834717\n",
      "Loss at step 83550 : 3.9936156272888184\n",
      "Loss at step 83600 : 2.839299201965332\n",
      "Loss at step 83650 : 2.5131325721740723\n",
      "Loss at step 83700 : 2.6703758239746094\n",
      "Loss at step 83750 : 3.263660430908203\n",
      "Loss at step 83800 : 3.5569815635681152\n",
      "Loss at step 83850 : 2.6258769035339355\n",
      "Loss at step 83900 : 3.1941170692443848\n",
      "Loss at step 83950 : 3.1851987838745117\n",
      "Loss at step 84000 : 3.3661856651306152\n",
      "Loss at step 84050 : 2.778095245361328\n",
      "Loss at step 84100 : 2.543696880340576\n",
      "Loss at step 84150 : 2.906996011734009\n",
      "Loss at step 84200 : 3.308624505996704\n",
      "Loss at step 84250 : 2.671074867248535\n",
      "Loss at step 84300 : 2.2641546726226807\n",
      "Loss at step 84350 : 3.4481892585754395\n",
      "Loss at step 84400 : 2.384011745452881\n",
      "Loss at step 84450 : 3.2484726905822754\n",
      "Loss at step 84500 : 2.8167009353637695\n",
      "Loss at step 84550 : 2.406019926071167\n",
      "Loss at step 84600 : 3.080900192260742\n",
      "Loss at step 84650 : 3.194058418273926\n",
      "Loss at step 84700 : 2.730607509613037\n",
      "Loss at step 84750 : 3.0712404251098633\n",
      "Loss at step 84800 : 2.986968994140625\n",
      "Loss at step 84850 : 3.6279287338256836\n",
      "Loss at step 84900 : 2.9410266876220703\n",
      "Loss at step 84950 : 3.567758560180664\n",
      "Loss at step 85000 : 3.002711772918701\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, moisture,\n",
      "Nearest to rice: thigh, mandu, lime, beaten, lunch,\n",
      "Nearest to sushi: sesame, piece, layer, cut, rice,\n",
      "Nearest to roll: sheet, oil, worcestershire, meal, crumb,\n",
      "Nearest to sashimi: fryer, sushi, gyoza, seaweed, spread,\n",
      "Nearest to steak: maple, paste, eye, meat, grate,\n",
      "Nearest to grill: teriyaki, soy, cider, chicken, rack,\n",
      "Nearest to sauce: jack, wasabi, oregano, soy, pat,\n",
      "Nearest to cream: dente, extract, skinless, chocolate, burger,\n",
      "Nearest to cheesecake: cashew, flake, nutmeg, flour, chill,\n",
      "Nearest to pizza: white, mixture, flake, baguette, run,\n",
      "Nearest to lasagna: fry, lemon, rare, teriyaki, noodle,\n",
      "Nearest to hamburger: herb, meat, parsley, pine, enjoy,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 85050 : 2.7425484657287598\n",
      "Loss at step 85100 : 2.6802845001220703\n",
      "Loss at step 85150 : 2.6710922718048096\n",
      "Loss at step 85200 : 2.8739826679229736\n",
      "Loss at step 85250 : 3.220951795578003\n",
      "Loss at step 85300 : 2.8092079162597656\n",
      "Loss at step 85350 : 3.59078311920166\n",
      "Loss at step 85400 : 2.475102663040161\n",
      "Loss at step 85450 : 3.0423195362091064\n",
      "Loss at step 85500 : 3.393557071685791\n",
      "Loss at step 85550 : 3.754394292831421\n",
      "Loss at step 85600 : 2.887280225753784\n",
      "Loss at step 85650 : 2.6949234008789062\n",
      "Loss at step 85700 : 3.7743985652923584\n",
      "Loss at step 85750 : 3.325594902038574\n",
      "Loss at step 85800 : 1.8650304079055786\n",
      "Loss at step 85850 : 2.973794460296631\n",
      "Loss at step 85900 : 3.2782349586486816\n",
      "Loss at step 85950 : 3.1512880325317383\n",
      "Loss at step 86000 : 3.5055813789367676\n",
      "Loss at step 86050 : 3.1798408031463623\n",
      "Loss at step 86100 : 2.332730770111084\n",
      "Loss at step 86150 : 2.5100860595703125\n",
      "Loss at step 86200 : 2.870027542114258\n",
      "Loss at step 86250 : 2.510336399078369\n",
      "Loss at step 86300 : 4.667189598083496\n",
      "Loss at step 86350 : 3.3310327529907227\n",
      "Loss at step 86400 : 2.2068474292755127\n",
      "Loss at step 86450 : 3.9304256439208984\n",
      "Loss at step 86500 : 2.7573461532592773\n",
      "Loss at step 86550 : 2.9531450271606445\n",
      "Loss at step 86600 : 3.2996883392333984\n",
      "Loss at step 86650 : 3.1968469619750977\n",
      "Loss at step 86700 : 3.2618789672851562\n",
      "Loss at step 86750 : 2.7694473266601562\n",
      "Loss at step 86800 : 3.2637195587158203\n",
      "Loss at step 86850 : 4.415121078491211\n",
      "Loss at step 86900 : 2.2044692039489746\n",
      "Loss at step 86950 : 2.5122642517089844\n",
      "Loss at step 87000 : 3.5223777294158936\n",
      "Loss at step 87050 : 3.375086784362793\n",
      "Loss at step 87100 : 2.884228229522705\n",
      "Loss at step 87150 : 2.370802402496338\n",
      "Loss at step 87200 : 3.326859951019287\n",
      "Loss at step 87250 : 3.485496997833252\n",
      "Loss at step 87300 : 3.447925090789795\n",
      "Loss at step 87350 : 2.3677215576171875\n",
      "Loss at step 87400 : 3.5782454013824463\n",
      "Loss at step 87450 : 2.5635480880737305\n",
      "Loss at step 87500 : 2.6974759101867676\n",
      "Loss at step 87550 : 2.38836669921875\n",
      "Loss at step 87600 : 2.7756247520446777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 87650 : 3.7221317291259766\n",
      "Loss at step 87700 : 3.558469772338867\n",
      "Loss at step 87750 : 2.923855781555176\n",
      "Loss at step 87800 : 2.7858028411865234\n",
      "Loss at step 87850 : 3.5812535285949707\n",
      "Loss at step 87900 : 2.958897590637207\n",
      "Loss at step 87950 : 2.44873309135437\n",
      "Loss at step 88000 : 2.7826528549194336\n",
      "Loss at step 88050 : 2.6925694942474365\n",
      "Loss at step 88100 : 2.437847137451172\n",
      "Loss at step 88150 : 2.872934341430664\n",
      "Loss at step 88200 : 3.6214218139648438\n",
      "Loss at step 88250 : 2.83937931060791\n",
      "Loss at step 88300 : 2.8154385089874268\n",
      "Loss at step 88350 : 2.827651023864746\n",
      "Loss at step 88400 : 2.699449062347412\n",
      "Loss at step 88450 : 2.6280438899993896\n",
      "Loss at step 88500 : 2.4715356826782227\n",
      "Loss at step 88550 : 2.114389419555664\n",
      "Loss at step 88600 : 2.530552387237549\n",
      "Loss at step 88650 : 3.350518226623535\n",
      "Loss at step 88700 : 2.6078271865844727\n",
      "Loss at step 88750 : 3.7625834941864014\n",
      "Loss at step 88800 : 3.3654685020446777\n",
      "Loss at step 88850 : 2.764375686645508\n",
      "Loss at step 88900 : 3.0833492279052734\n",
      "Loss at step 88950 : 2.9571456909179688\n",
      "Loss at step 89000 : 3.010192394256592\n",
      "Loss at step 89050 : 2.660762310028076\n",
      "Loss at step 89100 : 3.0766892433166504\n",
      "Loss at step 89150 : 4.005472183227539\n",
      "Loss at step 89200 : 2.554539203643799\n",
      "Loss at step 89250 : 2.7151479721069336\n",
      "Loss at step 89300 : 2.6857283115386963\n",
      "Loss at step 89350 : 3.285061836242676\n",
      "Loss at step 89400 : 2.4930760860443115\n",
      "Loss at step 89450 : 3.813136577606201\n",
      "Loss at step 89500 : 3.2795495986938477\n",
      "Loss at step 89550 : 3.130558490753174\n",
      "Loss at step 89600 : 3.4170563220977783\n",
      "Loss at step 89650 : 3.145704746246338\n",
      "Loss at step 89700 : 2.799434185028076\n",
      "Loss at step 89750 : 3.1446475982666016\n",
      "Loss at step 89800 : 2.988023519515991\n",
      "Loss at step 89850 : 2.83187198638916\n",
      "Loss at step 89900 : 3.2612991333007812\n",
      "Loss at step 89950 : 3.070009469985962\n",
      "Loss at step 90000 : 4.040157794952393\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, moisture,\n",
      "Nearest to rice: lime, thigh, mandu, beaten, lunch,\n",
      "Nearest to sushi: piece, sesame, layer, rice, cut,\n",
      "Nearest to roll: sheet, oil, worcestershire, meal, crumb,\n",
      "Nearest to sashimi: fryer, ring, sushi, seaweed, gyoza,\n",
      "Nearest to steak: maple, paste, meat, eye, grate,\n",
      "Nearest to grill: teriyaki, soy, cider, chicken, rack,\n",
      "Nearest to sauce: jack, wasabi, oregano, soy, sirloin,\n",
      "Nearest to cream: chocolate, dente, extract, pastry, peak,\n",
      "Nearest to cheesecake: cashew, flake, paper, nutmeg, tin,\n",
      "Nearest to pizza: mixture, white, flake, run, baguette,\n",
      "Nearest to lasagna: fry, lemon, rare, teriyaki, noodle,\n",
      "Nearest to hamburger: meat, herb, pine, parsley, confectioner,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 90050 : 3.0372891426086426\n",
      "Loss at step 90100 : 2.94708514213562\n",
      "Loss at step 90150 : 3.4741787910461426\n",
      "Loss at step 90200 : 3.4389913082122803\n",
      "Loss at step 90250 : 2.1231656074523926\n",
      "Loss at step 90300 : 2.459714412689209\n",
      "Loss at step 90350 : 3.182567596435547\n",
      "Loss at step 90400 : 2.2945032119750977\n",
      "Loss at step 90450 : 2.124166488647461\n",
      "Loss at step 90500 : 3.0623064041137695\n",
      "Loss at step 90550 : 2.4351468086242676\n",
      "Loss at step 90600 : 3.1245014667510986\n",
      "Loss at step 90650 : 2.6749491691589355\n",
      "Loss at step 90700 : 3.1768712997436523\n",
      "Loss at step 90750 : 4.008655548095703\n",
      "Loss at step 90800 : 3.105738639831543\n",
      "Loss at step 90850 : 2.4855823516845703\n",
      "Loss at step 90900 : 2.630568027496338\n",
      "Loss at step 90950 : 3.037280321121216\n",
      "Loss at step 91000 : 3.217073917388916\n",
      "Loss at step 91050 : 3.537113904953003\n",
      "Loss at step 91100 : 3.6729423999786377\n",
      "Loss at step 91150 : 4.0929059982299805\n",
      "Loss at step 91200 : 4.300394535064697\n",
      "Loss at step 91250 : 3.8547682762145996\n",
      "Loss at step 91300 : 2.159526824951172\n",
      "Loss at step 91350 : 2.7867445945739746\n",
      "Loss at step 91400 : 3.521554470062256\n",
      "Loss at step 91450 : 3.0619447231292725\n",
      "Loss at step 91500 : 2.677586078643799\n",
      "Loss at step 91550 : 2.537400007247925\n",
      "Loss at step 91600 : 3.1364586353302\n",
      "Loss at step 91650 : 4.449617385864258\n",
      "Loss at step 91700 : 2.9487709999084473\n",
      "Loss at step 91750 : 2.3675575256347656\n",
      "Loss at step 91800 : 3.3697493076324463\n",
      "Loss at step 91850 : 2.85756778717041\n",
      "Loss at step 91900 : 3.3959689140319824\n",
      "Loss at step 91950 : 2.693585157394409\n",
      "Loss at step 92000 : 3.2036099433898926\n",
      "Loss at step 92050 : 2.5592381954193115\n",
      "Loss at step 92100 : 2.8197274208068848\n",
      "Loss at step 92150 : 2.962315797805786\n",
      "Loss at step 92200 : 3.4535269737243652\n",
      "Loss at step 92250 : 3.0574791431427\n",
      "Loss at step 92300 : 2.944507598876953\n",
      "Loss at step 92350 : 2.3925580978393555\n",
      "Loss at step 92400 : 3.22640323638916\n",
      "Loss at step 92450 : 2.532747507095337\n",
      "Loss at step 92500 : 3.819492816925049\n",
      "Loss at step 92550 : 2.2216594219207764\n",
      "Loss at step 92600 : 3.451396942138672\n",
      "Loss at step 92650 : 3.6567394733428955\n",
      "Loss at step 92700 : 3.180793046951294\n",
      "Loss at step 92750 : 3.487689256668091\n",
      "Loss at step 92800 : 3.24918532371521\n",
      "Loss at step 92850 : 3.407067060470581\n",
      "Loss at step 92900 : 2.845956325531006\n",
      "Loss at step 92950 : 3.157733201980591\n",
      "Loss at step 93000 : 2.960519313812256\n",
      "Loss at step 93050 : 1.512628436088562\n",
      "Loss at step 93100 : 2.835298538208008\n",
      "Loss at step 93150 : 2.184387683868408\n",
      "Loss at step 93200 : 3.275991678237915\n",
      "Loss at step 93250 : 3.325171709060669\n",
      "Loss at step 93300 : 2.4485373497009277\n",
      "Loss at step 93350 : 2.8884084224700928\n",
      "Loss at step 93400 : 2.43992018699646\n",
      "Loss at step 93450 : 2.860145092010498\n",
      "Loss at step 93500 : 3.4018635749816895\n",
      "Loss at step 93550 : 3.9806594848632812\n",
      "Loss at step 93600 : 1.7909117937088013\n",
      "Loss at step 93650 : 3.2503814697265625\n",
      "Loss at step 93700 : 3.770421266555786\n",
      "Loss at step 93750 : 2.814038038253784\n",
      "Loss at step 93800 : 4.012331962585449\n",
      "Loss at step 93850 : 3.266690731048584\n",
      "Loss at step 93900 : 2.466385841369629\n",
      "Loss at step 93950 : 2.6612863540649414\n",
      "Loss at step 94000 : 2.827883720397949\n",
      "Loss at step 94050 : 3.369364023208618\n",
      "Loss at step 94100 : 2.8339552879333496\n",
      "Loss at step 94150 : 3.5255112648010254\n",
      "Loss at step 94200 : 2.1790895462036133\n",
      "Loss at step 94250 : 2.995426654815674\n",
      "Loss at step 94300 : 3.6240479946136475\n",
      "Loss at step 94350 : 3.4627957344055176\n",
      "Loss at step 94400 : 3.32784366607666\n",
      "Loss at step 94450 : 2.82371187210083\n",
      "Loss at step 94500 : 3.074202537536621\n",
      "Loss at step 94550 : 2.6718361377716064\n",
      "Loss at step 94600 : 3.919234275817871\n",
      "Loss at step 94650 : 3.603285312652588\n",
      "Loss at step 94700 : 2.7258565425872803\n",
      "Loss at step 94750 : 2.7504613399505615\n",
      "Loss at step 94800 : 2.6934001445770264\n",
      "Loss at step 94850 : 3.16341495513916\n",
      "Loss at step 94900 : 3.8310790061950684\n",
      "Loss at step 94950 : 2.6721854209899902\n",
      "Loss at step 95000 : 2.821138381958008\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, moisture,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, lunch,\n",
      "Nearest to sushi: sesame, piece, rice, layer, cut,\n",
      "Nearest to roll: sheet, oil, meal, worcestershire, crumb,\n",
      "Nearest to sashimi: fryer, ring, gyoza, sushi, seaweed,\n",
      "Nearest to steak: maple, meat, eye, paste, grate,\n",
      "Nearest to grill: teriyaki, soy, cider, chicken, rack,\n",
      "Nearest to sauce: jack, soy, wasabi, oregano, peel,\n",
      "Nearest to cream: chocolate, extract, dente, burger, skinless,\n",
      "Nearest to cheesecake: cashew, flake, nutmeg, paper, flour,\n",
      "Nearest to pizza: mixture, white, flake, run, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, teriyaki, noodle,\n",
      "Nearest to hamburger: meat, pine, herb, parsley, tomato,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 95050 : 3.4939043521881104\n",
      "Loss at step 95100 : 2.6843791007995605\n",
      "Loss at step 95150 : 3.678395986557007\n",
      "Loss at step 95200 : 3.1263129711151123\n",
      "Loss at step 95250 : 3.3332743644714355\n",
      "Loss at step 95300 : 2.766516923904419\n",
      "Loss at step 95350 : 3.332294464111328\n",
      "Loss at step 95400 : 3.536771297454834\n",
      "Loss at step 95450 : 2.674067974090576\n",
      "Loss at step 95500 : 3.2539279460906982\n",
      "Loss at step 95550 : 2.500110149383545\n",
      "Loss at step 95600 : 1.9523537158966064\n",
      "Loss at step 95650 : 2.806593656539917\n",
      "Loss at step 95700 : 3.4575295448303223\n",
      "Loss at step 95750 : 2.7391104698181152\n",
      "Loss at step 95800 : 3.2604408264160156\n",
      "Loss at step 95850 : 2.3253121376037598\n",
      "Loss at step 95900 : 2.227389335632324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 95950 : 2.7362942695617676\n",
      "Loss at step 96000 : 2.92832088470459\n",
      "Loss at step 96050 : 2.622905731201172\n",
      "Loss at step 96100 : 2.023082733154297\n",
      "Loss at step 96150 : 3.036203145980835\n",
      "Loss at step 96200 : 2.6404566764831543\n",
      "Loss at step 96250 : 2.8500211238861084\n",
      "Loss at step 96300 : 2.9884676933288574\n",
      "Loss at step 96350 : 3.1462173461914062\n",
      "Loss at step 96400 : 3.0085434913635254\n",
      "Loss at step 96450 : 2.504985809326172\n",
      "Loss at step 96500 : 3.1613588333129883\n",
      "Loss at step 96550 : 3.66349458694458\n",
      "Loss at step 96600 : 3.488436698913574\n",
      "Loss at step 96650 : 3.649318218231201\n",
      "Loss at step 96700 : 2.8997702598571777\n",
      "Loss at step 96750 : 3.0786526203155518\n",
      "Loss at step 96800 : 2.8429067134857178\n",
      "Loss at step 96850 : 1.9208886623382568\n",
      "Loss at step 96900 : 3.181675434112549\n",
      "Loss at step 96950 : 2.8327794075012207\n",
      "Loss at step 97000 : 2.378693103790283\n",
      "Loss at step 97050 : 2.7953500747680664\n",
      "Loss at step 97100 : 1.9138213396072388\n",
      "Loss at step 97150 : 2.5047965049743652\n",
      "Loss at step 97200 : 2.7794907093048096\n",
      "Loss at step 97250 : 2.810703754425049\n",
      "Loss at step 97300 : 3.1862387657165527\n",
      "Loss at step 97350 : 3.514857769012451\n",
      "Loss at step 97400 : 3.0082454681396484\n",
      "Loss at step 97450 : 2.442279100418091\n",
      "Loss at step 97500 : 3.6567494869232178\n",
      "Loss at step 97550 : 3.095886707305908\n",
      "Loss at step 97600 : 3.939650058746338\n",
      "Loss at step 97650 : 3.1800923347473145\n",
      "Loss at step 97700 : 2.4237661361694336\n",
      "Loss at step 97750 : 2.865387439727783\n",
      "Loss at step 97800 : 2.5959949493408203\n",
      "Loss at step 97850 : 2.537710666656494\n",
      "Loss at step 97900 : 3.4157042503356934\n",
      "Loss at step 97950 : 2.957913875579834\n",
      "Loss at step 98000 : 2.8055009841918945\n",
      "Loss at step 98050 : 2.712418794631958\n",
      "Loss at step 98100 : 4.401523590087891\n",
      "Loss at step 98150 : 3.47566819190979\n",
      "Loss at step 98200 : 2.8224081993103027\n",
      "Loss at step 98250 : 2.3129022121429443\n",
      "Loss at step 98300 : 4.051587104797363\n",
      "Loss at step 98350 : 3.3081459999084473\n",
      "Loss at step 98400 : 2.3251380920410156\n",
      "Loss at step 98450 : 2.271963119506836\n",
      "Loss at step 98500 : 1.724332571029663\n",
      "Loss at step 98550 : 2.9712867736816406\n",
      "Loss at step 98600 : 3.2926886081695557\n",
      "Loss at step 98650 : 3.21608829498291\n",
      "Loss at step 98700 : 2.533060073852539\n",
      "Loss at step 98750 : 2.2244529724121094\n",
      "Loss at step 98800 : 3.4867172241210938\n",
      "Loss at step 98850 : 2.724912643432617\n",
      "Loss at step 98900 : 3.235689163208008\n",
      "Loss at step 98950 : 2.9777565002441406\n",
      "Loss at step 99000 : 2.4308481216430664\n",
      "Loss at step 99050 : 3.00717830657959\n",
      "Loss at step 99100 : 3.2544209957122803\n",
      "Loss at step 99150 : 1.9028985500335693\n",
      "Loss at step 99200 : 3.888495922088623\n",
      "Loss at step 99250 : 5.424100875854492\n",
      "Loss at step 99300 : 3.7281248569488525\n",
      "Loss at step 99350 : 2.5449159145355225\n",
      "Loss at step 99400 : 3.0298800468444824\n",
      "Loss at step 99450 : 3.0166873931884766\n",
      "Loss at step 99500 : 3.1203389167785645\n",
      "Loss at step 99550 : 3.0778722763061523\n",
      "Loss at step 99600 : 3.49617075920105\n",
      "Loss at step 99650 : 3.696908712387085\n",
      "Loss at step 99700 : 3.82525372505188\n",
      "Loss at step 99750 : 3.3572473526000977\n",
      "Loss at step 99800 : 2.6051597595214844\n",
      "Loss at step 99850 : 2.7988762855529785\n",
      "Loss at step 99900 : 3.3842122554779053\n",
      "Loss at step 99950 : 3.4904885292053223\n",
      "Loss at step 100000 : 2.633314371109009\n",
      "Nearest to tuna: parmesan, metal, eye, moisture, virgin,\n",
      "Nearest to rice: thigh, lime, mandu, sushi, spinach,\n",
      "Nearest to sushi: sesame, piece, rice, cut, layer,\n",
      "Nearest to roll: sheet, oil, crumb, meal, worcestershire,\n",
      "Nearest to sashimi: fryer, ring, seaweed, gyoza, spread,\n",
      "Nearest to steak: maple, meat, eye, paste, grate,\n",
      "Nearest to grill: teriyaki, soy, cider, chicken, mesh,\n",
      "Nearest to sauce: jack, soy, wasabi, pat, oregano,\n",
      "Nearest to cream: chocolate, extract, dente, puff, cake,\n",
      "Nearest to cheesecake: cashew, flake, nutmeg, tin, paper,\n",
      "Nearest to pizza: white, mixture, flake, run, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, teriyaki, noodle,\n",
      "Nearest to hamburger: meat, herb, pine, parsley, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 100050 : 2.4912853240966797\n",
      "Loss at step 100100 : 3.9288501739501953\n",
      "Loss at step 100150 : 2.959458827972412\n",
      "Loss at step 100200 : 3.275437355041504\n",
      "Loss at step 100250 : 2.4798521995544434\n",
      "Loss at step 100300 : 2.878472089767456\n",
      "Loss at step 100350 : 1.7366235256195068\n",
      "Loss at step 100400 : 2.7647457122802734\n",
      "Loss at step 100450 : 2.805560827255249\n",
      "Loss at step 100500 : 3.3981831073760986\n",
      "Loss at step 100550 : 2.8540894985198975\n",
      "Loss at step 100600 : 3.0093655586242676\n",
      "Loss at step 100650 : 2.5398428440093994\n",
      "Loss at step 100700 : 2.45247483253479\n",
      "Loss at step 100750 : 2.5199475288391113\n",
      "Loss at step 100800 : 2.4842724800109863\n",
      "Loss at step 100850 : 2.7470827102661133\n",
      "Loss at step 100900 : 2.2111663818359375\n",
      "Loss at step 100950 : 2.8988187313079834\n",
      "Loss at step 101000 : 3.0002388954162598\n",
      "Loss at step 101050 : 3.030200958251953\n",
      "Loss at step 101100 : 2.86570143699646\n",
      "Loss at step 101150 : 2.6380183696746826\n",
      "Loss at step 101200 : 2.9670162200927734\n",
      "Loss at step 101250 : 2.471219539642334\n",
      "Loss at step 101300 : 2.7348716259002686\n",
      "Loss at step 101350 : 3.8126206398010254\n",
      "Loss at step 101400 : 4.042243480682373\n",
      "Loss at step 101450 : 3.3660078048706055\n",
      "Loss at step 101500 : 2.9209814071655273\n",
      "Loss at step 101550 : 3.1043901443481445\n",
      "Loss at step 101600 : 2.7747292518615723\n",
      "Loss at step 101650 : 3.276233434677124\n",
      "Loss at step 101700 : 3.847325325012207\n",
      "Loss at step 101750 : 3.3432774543762207\n",
      "Loss at step 101800 : 2.8220338821411133\n",
      "Loss at step 101850 : 3.2346270084381104\n",
      "Loss at step 101900 : 2.7200798988342285\n",
      "Loss at step 101950 : 3.589437484741211\n",
      "Loss at step 102000 : 2.1453542709350586\n",
      "Loss at step 102050 : 2.8197765350341797\n",
      "Loss at step 102100 : 2.104238748550415\n",
      "Loss at step 102150 : 3.0525307655334473\n",
      "Loss at step 102200 : 3.6952872276306152\n",
      "Loss at step 102250 : 2.863862991333008\n",
      "Loss at step 102300 : 3.800231456756592\n",
      "Loss at step 102350 : 2.797818183898926\n",
      "Loss at step 102400 : 2.7433857917785645\n",
      "Loss at step 102450 : 2.7354512214660645\n",
      "Loss at step 102500 : 3.3602914810180664\n",
      "Loss at step 102550 : 3.9983458518981934\n",
      "Loss at step 102600 : 2.7862329483032227\n",
      "Loss at step 102650 : 2.847686290740967\n",
      "Loss at step 102700 : 3.116483688354492\n",
      "Loss at step 102750 : 3.5591700077056885\n",
      "Loss at step 102800 : 3.191464424133301\n",
      "Loss at step 102850 : 2.6993322372436523\n",
      "Loss at step 102900 : 3.813774585723877\n",
      "Loss at step 102950 : 2.2927205562591553\n",
      "Loss at step 103000 : 3.3981499671936035\n",
      "Loss at step 103050 : 2.88352632522583\n",
      "Loss at step 103100 : 3.140928268432617\n",
      "Loss at step 103150 : 3.214704751968384\n",
      "Loss at step 103200 : 2.8801686763763428\n",
      "Loss at step 103250 : 2.8286941051483154\n",
      "Loss at step 103300 : 2.2156944274902344\n",
      "Loss at step 103350 : 3.485452175140381\n",
      "Loss at step 103400 : 3.643016815185547\n",
      "Loss at step 103450 : 2.8735029697418213\n",
      "Loss at step 103500 : 1.5643250942230225\n",
      "Loss at step 103550 : 2.181671380996704\n",
      "Loss at step 103600 : 3.0747783184051514\n",
      "Loss at step 103650 : 2.7160472869873047\n",
      "Loss at step 103700 : 3.3794124126434326\n",
      "Loss at step 103750 : 2.428346633911133\n",
      "Loss at step 103800 : 2.3708510398864746\n",
      "Loss at step 103850 : 4.213768005371094\n",
      "Loss at step 103900 : 2.1503491401672363\n",
      "Loss at step 103950 : 2.2687010765075684\n",
      "Loss at step 104000 : 3.160224676132202\n",
      "Loss at step 104050 : 2.41682505607605\n",
      "Loss at step 104100 : 2.1711857318878174\n",
      "Loss at step 104150 : 2.194772720336914\n",
      "Loss at step 104200 : 3.2539873123168945\n",
      "Loss at step 104250 : 2.6292147636413574\n",
      "Loss at step 104300 : 3.0209217071533203\n",
      "Loss at step 104350 : 2.909895658493042\n",
      "Loss at step 104400 : 2.7623422145843506\n",
      "Loss at step 104450 : 3.3773932456970215\n",
      "Loss at step 104500 : 4.001634120941162\n",
      "Loss at step 104550 : 2.529076099395752\n",
      "Loss at step 104600 : 2.912357807159424\n",
      "Loss at step 104650 : 3.3750457763671875\n",
      "Loss at step 104700 : 3.991236686706543\n",
      "Loss at step 104750 : 3.1601529121398926\n",
      "Loss at step 104800 : 2.9167590141296387\n",
      "Loss at step 104850 : 3.8828282356262207\n",
      "Loss at step 104900 : 2.7139716148376465\n",
      "Loss at step 104950 : 3.497626781463623\n",
      "Loss at step 105000 : 2.1884994506835938\n",
      "Nearest to tuna: parmesan, metal, eye, moisture, virgin,\n",
      "Nearest to rice: thigh, mandu, lime, sushi, stone,\n",
      "Nearest to sushi: sesame, piece, rice, cut, sashimi,\n",
      "Nearest to roll: sheet, oil, crumb, meal, worcestershire,\n",
      "Nearest to sashimi: fryer, seaweed, gyoza, ring, spread,\n",
      "Nearest to steak: maple, eye, meat, paste, grate,\n",
      "Nearest to grill: soy, teriyaki, cider, chicken, mesh,\n",
      "Nearest to sauce: jack, soy, wasabi, wine, pat,\n",
      "Nearest to cream: chocolate, extract, dente, puff, burger,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, chill, paper,\n",
      "Nearest to pizza: white, run, mixture, flake, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, teriyaki, noodle,\n",
      "Nearest to hamburger: meat, herb, pine, parsley, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 105050 : 4.113303184509277\n",
      "Loss at step 105100 : 2.271878242492676\n",
      "Loss at step 105150 : 3.075138568878174\n",
      "Loss at step 105200 : 3.129059314727783\n",
      "Loss at step 105250 : 3.1832165718078613\n",
      "Loss at step 105300 : 3.1093571186065674\n",
      "Loss at step 105350 : 2.5345821380615234\n",
      "Loss at step 105400 : 3.167508602142334\n",
      "Loss at step 105450 : 3.420234203338623\n",
      "Loss at step 105500 : 3.8049159049987793\n",
      "Loss at step 105550 : 2.017974376678467\n",
      "Loss at step 105600 : 2.8523454666137695\n",
      "Loss at step 105650 : 3.5393588542938232\n",
      "Loss at step 105700 : 2.936260461807251\n",
      "Loss at step 105750 : 2.8008203506469727\n",
      "Loss at step 105800 : 3.2165491580963135\n",
      "Loss at step 105850 : 2.8381094932556152\n",
      "Loss at step 105900 : 3.5707249641418457\n",
      "Loss at step 105950 : 2.6864304542541504\n",
      "Loss at step 106000 : 3.377950668334961\n",
      "Loss at step 106050 : 2.8468658924102783\n",
      "Loss at step 106100 : 3.1090967655181885\n",
      "Loss at step 106150 : 2.4648165702819824\n",
      "Loss at step 106200 : 3.0925939083099365\n",
      "Loss at step 106250 : 3.0906786918640137\n",
      "Loss at step 106300 : 3.5141472816467285\n",
      "Loss at step 106350 : 3.8593459129333496\n",
      "Loss at step 106400 : 3.2441554069519043\n",
      "Loss at step 106450 : 2.8086495399475098\n",
      "Loss at step 106500 : 2.5345823764801025\n",
      "Loss at step 106550 : 2.585763454437256\n",
      "Loss at step 106600 : 2.8257064819335938\n",
      "Loss at step 106650 : 2.09243106842041\n",
      "Loss at step 106700 : 2.7630486488342285\n",
      "Loss at step 106750 : 3.1097583770751953\n",
      "Loss at step 106800 : 2.7032434940338135\n",
      "Loss at step 106850 : 3.3123717308044434\n",
      "Loss at step 106900 : 2.649914026260376\n",
      "Loss at step 106950 : 2.3956544399261475\n",
      "Loss at step 107000 : 3.037531852722168\n",
      "Loss at step 107050 : 2.6239635944366455\n",
      "Loss at step 107100 : 2.7499234676361084\n",
      "Loss at step 107150 : 3.1715784072875977\n",
      "Loss at step 107200 : 2.9178757667541504\n",
      "Loss at step 107250 : 3.2827935218811035\n",
      "Loss at step 107300 : 2.4579522609710693\n",
      "Loss at step 107350 : 2.8222169876098633\n",
      "Loss at step 107400 : 3.4004478454589844\n",
      "Loss at step 107450 : 2.9724786281585693\n",
      "Loss at step 107500 : 3.324312210083008\n",
      "Loss at step 107550 : 3.3477563858032227\n",
      "Loss at step 107600 : 3.596013069152832\n",
      "Loss at step 107650 : 2.7400360107421875\n",
      "Loss at step 107700 : 3.0757153034210205\n",
      "Loss at step 107750 : 3.8089253902435303\n",
      "Loss at step 107800 : 3.346518039703369\n",
      "Loss at step 107850 : 2.7228317260742188\n",
      "Loss at step 107900 : 3.6682591438293457\n",
      "Loss at step 107950 : 3.219789981842041\n",
      "Loss at step 108000 : 2.1254959106445312\n",
      "Loss at step 108050 : 3.2011802196502686\n",
      "Loss at step 108100 : 3.1317782402038574\n",
      "Loss at step 108150 : 3.4064626693725586\n",
      "Loss at step 108200 : 2.4014501571655273\n",
      "Loss at step 108250 : 2.4263246059417725\n",
      "Loss at step 108300 : 3.4571330547332764\n",
      "Loss at step 108350 : 3.7646565437316895\n",
      "Loss at step 108400 : 3.4594898223876953\n",
      "Loss at step 108450 : 2.1287763118743896\n",
      "Loss at step 108500 : 3.2744643688201904\n",
      "Loss at step 108550 : 2.019536256790161\n",
      "Loss at step 108600 : 2.955709457397461\n",
      "Loss at step 108650 : 3.2009057998657227\n",
      "Loss at step 108700 : 2.6197919845581055\n",
      "Loss at step 108750 : 3.4772205352783203\n",
      "Loss at step 108800 : 3.1587419509887695\n",
      "Loss at step 108850 : 2.9991989135742188\n",
      "Loss at step 108900 : 2.9788126945495605\n",
      "Loss at step 108950 : 2.866471529006958\n",
      "Loss at step 109000 : 1.6560254096984863\n",
      "Loss at step 109050 : 2.423945903778076\n",
      "Loss at step 109100 : 3.3346614837646484\n",
      "Loss at step 109150 : 4.0187177658081055\n",
      "Loss at step 109200 : 3.4813642501831055\n",
      "Loss at step 109250 : 2.373497247695923\n",
      "Loss at step 109300 : 2.462249755859375\n",
      "Loss at step 109350 : 2.5453977584838867\n",
      "Loss at step 109400 : 3.076348304748535\n",
      "Loss at step 109450 : 2.8480429649353027\n",
      "Loss at step 109500 : 2.4738786220550537\n",
      "Loss at step 109550 : 2.6158835887908936\n",
      "Loss at step 109600 : 2.765104055404663\n",
      "Loss at step 109650 : 3.973447561264038\n",
      "Loss at step 109700 : 2.0958499908447266\n",
      "Loss at step 109750 : 3.334291458129883\n",
      "Loss at step 109800 : 2.381370782852173\n",
      "Loss at step 109850 : 2.2097954750061035\n",
      "Loss at step 109900 : 2.3742213249206543\n",
      "Loss at step 109950 : 3.092221975326538\n",
      "Loss at step 110000 : 2.906949758529663\n",
      "Nearest to tuna: parmesan, metal, eye, virgin, avocado,\n",
      "Nearest to rice: thigh, lime, mandu, sushi, spinach,\n",
      "Nearest to sushi: sesame, piece, rice, cut, layer,\n",
      "Nearest to roll: sheet, oil, meal, crumb, worcestershire,\n",
      "Nearest to sashimi: fryer, seaweed, ring, spread, gyoza,\n",
      "Nearest to steak: maple, eye, meat, paste, grate,\n",
      "Nearest to grill: teriyaki, soy, cider, chicken, mesh,\n",
      "Nearest to sauce: jack, wasabi, soy, appetizer, oregano,\n",
      "Nearest to cream: extract, chocolate, dente, cherry, peak,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: white, run, mixture, flake, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, teriyaki, noodle,\n",
      "Nearest to hamburger: meat, herb, pine, parsley, beet,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 110050 : 3.3765740394592285\n",
      "Loss at step 110100 : 3.807387351989746\n",
      "Loss at step 110150 : 3.0144853591918945\n",
      "Loss at step 110200 : 4.178759574890137\n",
      "Loss at step 110250 : 3.6465296745300293\n",
      "Loss at step 110300 : 2.986751079559326\n",
      "Loss at step 110350 : 3.2137622833251953\n",
      "Loss at step 110400 : 3.341681957244873\n",
      "Loss at step 110450 : 3.108895778656006\n",
      "Loss at step 110500 : 2.447969436645508\n",
      "Loss at step 110550 : 3.1257307529449463\n",
      "Loss at step 110600 : 3.2828731536865234\n",
      "Loss at step 110650 : 3.2001829147338867\n",
      "Loss at step 110700 : 3.429837465286255\n",
      "Loss at step 110750 : 2.572007656097412\n",
      "Loss at step 110800 : 3.0939111709594727\n",
      "Loss at step 110850 : 2.670975685119629\n",
      "Loss at step 110900 : 2.919600009918213\n",
      "Loss at step 110950 : 3.15390682220459\n",
      "Loss at step 111000 : 3.2394180297851562\n",
      "Loss at step 111050 : 2.7760496139526367\n",
      "Loss at step 111100 : 2.3070003986358643\n",
      "Loss at step 111150 : 2.249300956726074\n",
      "Loss at step 111200 : 3.271526336669922\n",
      "Loss at step 111250 : 2.8848824501037598\n",
      "Loss at step 111300 : 3.1574127674102783\n",
      "Loss at step 111350 : 2.5186610221862793\n",
      "Loss at step 111400 : 3.54392147064209\n",
      "Loss at step 111450 : 3.3951783180236816\n",
      "Loss at step 111500 : 2.862705707550049\n",
      "Loss at step 111550 : 2.4144978523254395\n",
      "Loss at step 111600 : 3.5440256595611572\n",
      "Loss at step 111650 : 2.7574710845947266\n",
      "Loss at step 111700 : 2.604240894317627\n",
      "Loss at step 111750 : 2.473252296447754\n",
      "Loss at step 111800 : 3.5332465171813965\n",
      "Loss at step 111850 : 2.68654203414917\n",
      "Loss at step 111900 : 3.3398680686950684\n",
      "Loss at step 111950 : 2.2757229804992676\n",
      "Loss at step 112000 : 2.710320234298706\n",
      "Loss at step 112050 : 2.5763275623321533\n",
      "Loss at step 112100 : 2.296236038208008\n",
      "Loss at step 112150 : 2.9154038429260254\n",
      "Loss at step 112200 : 3.2422921657562256\n",
      "Loss at step 112250 : 3.0364668369293213\n",
      "Loss at step 112300 : 3.2944741249084473\n",
      "Loss at step 112350 : 2.376315116882324\n",
      "Loss at step 112400 : 3.4460225105285645\n",
      "Loss at step 112450 : 3.0884954929351807\n",
      "Loss at step 112500 : 3.159687042236328\n",
      "Loss at step 112550 : 2.7596445083618164\n",
      "Loss at step 112600 : 3.281341075897217\n",
      "Loss at step 112650 : 3.0070888996124268\n",
      "Loss at step 112700 : 3.390583038330078\n",
      "Loss at step 112750 : 3.424988269805908\n",
      "Loss at step 112800 : 3.1569974422454834\n",
      "Loss at step 112850 : 2.514683246612549\n",
      "Loss at step 112900 : 3.4443416595458984\n",
      "Loss at step 112950 : 3.393564224243164\n",
      "Loss at step 113000 : 3.4307403564453125\n",
      "Loss at step 113050 : 3.126357078552246\n",
      "Loss at step 113100 : 2.509415626525879\n",
      "Loss at step 113150 : 2.3967742919921875\n",
      "Loss at step 113200 : 3.0868136882781982\n",
      "Loss at step 113250 : 3.193465232849121\n",
      "Loss at step 113300 : 3.333876371383667\n",
      "Loss at step 113350 : 3.068415641784668\n",
      "Loss at step 113400 : 3.3024744987487793\n",
      "Loss at step 113450 : 3.186607837677002\n",
      "Loss at step 113500 : 3.520054817199707\n",
      "Loss at step 113550 : 2.7624711990356445\n",
      "Loss at step 113600 : 3.793133497238159\n",
      "Loss at step 113650 : 3.563983917236328\n",
      "Loss at step 113700 : 2.7985639572143555\n",
      "Loss at step 113750 : 2.8886022567749023\n",
      "Loss at step 113800 : 3.5628418922424316\n",
      "Loss at step 113850 : 2.991870880126953\n",
      "Loss at step 113900 : 3.6713218688964844\n",
      "Loss at step 113950 : 2.479100227355957\n",
      "Loss at step 114000 : 2.7774336338043213\n",
      "Loss at step 114050 : 2.9385154247283936\n",
      "Loss at step 114100 : 2.645385503768921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 114150 : 2.725043773651123\n",
      "Loss at step 114200 : 3.03631329536438\n",
      "Loss at step 114250 : 2.82907772064209\n",
      "Loss at step 114300 : 2.571967124938965\n",
      "Loss at step 114350 : 2.848517417907715\n",
      "Loss at step 114400 : 2.958573341369629\n",
      "Loss at step 114450 : 2.5615053176879883\n",
      "Loss at step 114500 : 3.7290496826171875\n",
      "Loss at step 114550 : 3.046144962310791\n",
      "Loss at step 114600 : 3.0443572998046875\n",
      "Loss at step 114650 : 2.9613466262817383\n",
      "Loss at step 114700 : 3.326634645462036\n",
      "Loss at step 114750 : 3.314736843109131\n",
      "Loss at step 114800 : 2.6941235065460205\n",
      "Loss at step 114850 : 3.0245511531829834\n",
      "Loss at step 114900 : 2.959902286529541\n",
      "Loss at step 114950 : 3.0210018157958984\n",
      "Loss at step 115000 : 2.8268234729766846\n",
      "Nearest to tuna: parmesan, metal, eye, avocado, virgin,\n",
      "Nearest to rice: thigh, lime, mandu, sushi, spinach,\n",
      "Nearest to sushi: sesame, piece, rice, cut, sashimi,\n",
      "Nearest to roll: sheet, meal, heart, oil, crumb,\n",
      "Nearest to sashimi: fryer, seaweed, ring, gyoza, spread,\n",
      "Nearest to steak: maple, eye, meat, paste, grate,\n",
      "Nearest to grill: teriyaki, soy, cider, mesh, spring,\n",
      "Nearest to sauce: jack, wasabi, appetizer, soy, oregano,\n",
      "Nearest to cream: extract, chocolate, dente, cherry, cake,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, chill, paper,\n",
      "Nearest to pizza: white, run, mixture, flake, baguette,\n",
      "Nearest to lasagna: fry, rare, noodle, lemon, teriyaki,\n",
      "Nearest to hamburger: meat, pine, herb, parsley, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 115050 : 3.1127004623413086\n",
      "Loss at step 115100 : 3.5003559589385986\n",
      "Loss at step 115150 : 2.6631526947021484\n",
      "Loss at step 115200 : 2.8737425804138184\n",
      "Loss at step 115250 : 2.9091219902038574\n",
      "Loss at step 115300 : 3.296365976333618\n",
      "Loss at step 115350 : 2.7683253288269043\n",
      "Loss at step 115400 : 2.27443265914917\n",
      "Loss at step 115450 : 2.643219470977783\n",
      "Loss at step 115500 : 3.1382594108581543\n",
      "Loss at step 115550 : 2.8240299224853516\n",
      "Loss at step 115600 : 2.4640002250671387\n",
      "Loss at step 115650 : 2.9027223587036133\n",
      "Loss at step 115700 : 2.7713980674743652\n",
      "Loss at step 115750 : 2.9712252616882324\n",
      "Loss at step 115800 : 3.3019094467163086\n",
      "Loss at step 115850 : 2.770860195159912\n",
      "Loss at step 115900 : 3.319362163543701\n",
      "Loss at step 115950 : 4.91664981842041\n",
      "Loss at step 116000 : 2.251246929168701\n",
      "Loss at step 116050 : 2.4413273334503174\n",
      "Loss at step 116100 : 2.5284581184387207\n",
      "Loss at step 116150 : 2.7357311248779297\n",
      "Loss at step 116200 : 2.797992467880249\n",
      "Loss at step 116250 : 3.158132314682007\n",
      "Loss at step 116300 : 3.008145332336426\n",
      "Loss at step 116350 : 3.5223958492279053\n",
      "Loss at step 116400 : 2.9237911701202393\n",
      "Loss at step 116450 : 3.420844316482544\n",
      "Loss at step 116500 : 2.657602548599243\n",
      "Loss at step 116550 : 2.4706010818481445\n",
      "Loss at step 116600 : 3.0479917526245117\n",
      "Loss at step 116650 : 3.1440744400024414\n",
      "Loss at step 116700 : 2.8216774463653564\n",
      "Loss at step 116750 : 2.942124128341675\n",
      "Loss at step 116800 : 2.7691478729248047\n",
      "Loss at step 116850 : 2.326738119125366\n",
      "Loss at step 116900 : 3.266279697418213\n",
      "Loss at step 116950 : 3.149658679962158\n",
      "Loss at step 117000 : 3.5224742889404297\n",
      "Loss at step 117050 : 2.6941874027252197\n",
      "Loss at step 117100 : 2.9126806259155273\n",
      "Loss at step 117150 : 3.0426430702209473\n",
      "Loss at step 117200 : 4.294109344482422\n",
      "Loss at step 117250 : 2.8785693645477295\n",
      "Loss at step 117300 : 3.0634665489196777\n",
      "Loss at step 117350 : 2.7849884033203125\n",
      "Loss at step 117400 : 2.8219735622406006\n",
      "Loss at step 117450 : 2.772637128829956\n",
      "Loss at step 117500 : 2.8232011795043945\n",
      "Loss at step 117550 : 2.913583755493164\n",
      "Loss at step 117600 : 2.7363033294677734\n",
      "Loss at step 117650 : 2.7601771354675293\n",
      "Loss at step 117700 : 3.0616376399993896\n",
      "Loss at step 117750 : 2.476062059402466\n",
      "Loss at step 117800 : 2.895822525024414\n",
      "Loss at step 117850 : 3.2539103031158447\n",
      "Loss at step 117900 : 3.752656936645508\n",
      "Loss at step 117950 : 3.1832261085510254\n",
      "Loss at step 118000 : 3.921980381011963\n",
      "Loss at step 118050 : 3.0499014854431152\n",
      "Loss at step 118100 : 3.959994077682495\n",
      "Loss at step 118150 : 3.0635910034179688\n",
      "Loss at step 118200 : 3.3638105392456055\n",
      "Loss at step 118250 : 3.3734254837036133\n",
      "Loss at step 118300 : 2.639526844024658\n",
      "Loss at step 118350 : 2.782041072845459\n",
      "Loss at step 118400 : 2.142989158630371\n",
      "Loss at step 118450 : 3.090963840484619\n",
      "Loss at step 118500 : 2.073550224304199\n",
      "Loss at step 118550 : 3.5859317779541016\n",
      "Loss at step 118600 : 2.785414218902588\n",
      "Loss at step 118650 : 2.535275459289551\n",
      "Loss at step 118700 : 3.0214974880218506\n",
      "Loss at step 118750 : 3.660855293273926\n",
      "Loss at step 118800 : 2.555647850036621\n",
      "Loss at step 118850 : 2.669797897338867\n",
      "Loss at step 118900 : 2.7499289512634277\n",
      "Loss at step 118950 : 3.129166603088379\n",
      "Loss at step 119000 : 2.5789551734924316\n",
      "Loss at step 119050 : 2.2761778831481934\n",
      "Loss at step 119100 : 3.0474894046783447\n",
      "Loss at step 119150 : 3.282503843307495\n",
      "Loss at step 119200 : 3.3777551651000977\n",
      "Loss at step 119250 : 2.4722986221313477\n",
      "Loss at step 119300 : 2.511467456817627\n",
      "Loss at step 119350 : 2.5694026947021484\n",
      "Loss at step 119400 : 2.3722329139709473\n",
      "Loss at step 119450 : 2.877613067626953\n",
      "Loss at step 119500 : 3.6044023036956787\n",
      "Loss at step 119550 : 2.822854995727539\n",
      "Loss at step 119600 : 3.686357021331787\n",
      "Loss at step 119650 : 3.860485076904297\n",
      "Loss at step 119700 : 2.6169919967651367\n",
      "Loss at step 119750 : 3.4841325283050537\n",
      "Loss at step 119800 : 2.5103793144226074\n",
      "Loss at step 119850 : 2.132582187652588\n",
      "Loss at step 119900 : 3.185049295425415\n",
      "Loss at step 119950 : 2.234036445617676\n",
      "Loss at step 120000 : 2.7575695514678955\n",
      "Nearest to tuna: parmesan, metal, eye, sushi, avocado,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, sesame,\n",
      "Nearest to sushi: sesame, piece, cut, rice, tuna,\n",
      "Nearest to roll: sheet, meal, heart, crumb, ricotta,\n",
      "Nearest to sashimi: fryer, ring, seaweed, gyoza, spread,\n",
      "Nearest to steak: maple, eye, meat, paste, grate,\n",
      "Nearest to grill: soy, teriyaki, cider, spring, chicken,\n",
      "Nearest to sauce: jack, soy, wasabi, oregano, appetizer,\n",
      "Nearest to cream: extract, chocolate, puff, cherry, dente,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, chill, paper,\n",
      "Nearest to pizza: white, mixture, flake, run, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, noodle, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, fill, parsley,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 120050 : 2.8536629676818848\n",
      "Loss at step 120100 : 3.833979845046997\n",
      "Loss at step 120150 : 3.3168535232543945\n",
      "Loss at step 120200 : 3.2252089977264404\n",
      "Loss at step 120250 : 3.1060357093811035\n",
      "Loss at step 120300 : 3.213031768798828\n",
      "Loss at step 120350 : 2.626352310180664\n",
      "Loss at step 120400 : 2.8511931896209717\n",
      "Loss at step 120450 : 3.1171634197235107\n",
      "Loss at step 120500 : 3.211610794067383\n",
      "Loss at step 120550 : 2.747253656387329\n",
      "Loss at step 120600 : 2.8159115314483643\n",
      "Loss at step 120650 : 3.0638813972473145\n",
      "Loss at step 120700 : 3.4359958171844482\n",
      "Loss at step 120750 : 2.8380069732666016\n",
      "Loss at step 120800 : 2.2059497833251953\n",
      "Loss at step 120850 : 2.0294103622436523\n",
      "Loss at step 120900 : 2.640420436859131\n",
      "Loss at step 120950 : 3.2430310249328613\n",
      "Loss at step 121000 : 2.320997714996338\n",
      "Loss at step 121050 : 3.9911701679229736\n",
      "Loss at step 121100 : 3.0154385566711426\n",
      "Loss at step 121150 : 2.9170947074890137\n",
      "Loss at step 121200 : 2.712371826171875\n",
      "Loss at step 121250 : 2.3445487022399902\n",
      "Loss at step 121300 : 3.7346129417419434\n",
      "Loss at step 121350 : 3.3000545501708984\n",
      "Loss at step 121400 : 3.456076145172119\n",
      "Loss at step 121450 : 2.9946484565734863\n",
      "Loss at step 121500 : 2.9796547889709473\n",
      "Loss at step 121550 : 2.781642436981201\n",
      "Loss at step 121600 : 1.7093226909637451\n",
      "Loss at step 121650 : 2.6312808990478516\n",
      "Loss at step 121700 : 2.571105480194092\n",
      "Loss at step 121750 : 3.9242868423461914\n",
      "Loss at step 121800 : 2.38753342628479\n",
      "Loss at step 121850 : 3.3382983207702637\n",
      "Loss at step 121900 : 3.7269887924194336\n",
      "Loss at step 121950 : 3.958324909210205\n",
      "Loss at step 122000 : 2.891484260559082\n",
      "Loss at step 122050 : 2.2245099544525146\n",
      "Loss at step 122100 : 2.2814364433288574\n",
      "Loss at step 122150 : 2.3642830848693848\n",
      "Loss at step 122200 : 2.8070502281188965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 122250 : 2.6407418251037598\n",
      "Loss at step 122300 : 3.677743434906006\n",
      "Loss at step 122350 : 3.542438268661499\n",
      "Loss at step 122400 : 4.014552593231201\n",
      "Loss at step 122450 : 2.9602715969085693\n",
      "Loss at step 122500 : 2.6193878650665283\n",
      "Loss at step 122550 : 3.160163402557373\n",
      "Loss at step 122600 : 2.8457159996032715\n",
      "Loss at step 122650 : 2.5982747077941895\n",
      "Loss at step 122700 : 3.043389320373535\n",
      "Loss at step 122750 : 3.1235122680664062\n",
      "Loss at step 122800 : 1.7779178619384766\n",
      "Loss at step 122850 : 3.528873920440674\n",
      "Loss at step 122900 : 2.8348145484924316\n",
      "Loss at step 122950 : 3.3904991149902344\n",
      "Loss at step 123000 : 3.0504374504089355\n",
      "Loss at step 123050 : 3.796383857727051\n",
      "Loss at step 123100 : 1.7102357149124146\n",
      "Loss at step 123150 : 2.6997735500335693\n",
      "Loss at step 123200 : 3.0501925945281982\n",
      "Loss at step 123250 : 2.3085832595825195\n",
      "Loss at step 123300 : 2.399214744567871\n",
      "Loss at step 123350 : 2.322968006134033\n",
      "Loss at step 123400 : 3.067312240600586\n",
      "Loss at step 123450 : 3.2428605556488037\n",
      "Loss at step 123500 : 2.332127571105957\n",
      "Loss at step 123550 : 2.0917227268218994\n",
      "Loss at step 123600 : 3.854825735092163\n",
      "Loss at step 123650 : 2.656917095184326\n",
      "Loss at step 123700 : 2.0969491004943848\n",
      "Loss at step 123750 : 2.1783227920532227\n",
      "Loss at step 123800 : 2.0076162815093994\n",
      "Loss at step 123850 : 3.612307548522949\n",
      "Loss at step 123900 : 3.1500749588012695\n",
      "Loss at step 123950 : 2.600027561187744\n",
      "Loss at step 124000 : 2.381856918334961\n",
      "Loss at step 124050 : 3.4792747497558594\n",
      "Loss at step 124100 : 3.123504638671875\n",
      "Loss at step 124150 : 2.198324680328369\n",
      "Loss at step 124200 : 2.846177816390991\n",
      "Loss at step 124250 : 3.460376024246216\n",
      "Loss at step 124300 : 3.188568353652954\n",
      "Loss at step 124350 : 3.6512670516967773\n",
      "Loss at step 124400 : 3.240779399871826\n",
      "Loss at step 124450 : 3.294260263442993\n",
      "Loss at step 124500 : 2.3526906967163086\n",
      "Loss at step 124550 : 2.9908900260925293\n",
      "Loss at step 124600 : 2.84604811668396\n",
      "Loss at step 124650 : 3.097865104675293\n",
      "Loss at step 124700 : 4.325564384460449\n",
      "Loss at step 124750 : 2.4856317043304443\n",
      "Loss at step 124800 : 2.6221399307250977\n",
      "Loss at step 124850 : 2.5977511405944824\n",
      "Loss at step 124900 : 2.569767951965332\n",
      "Loss at step 124950 : 3.341742515563965\n",
      "Loss at step 125000 : 3.1327474117279053\n",
      "Nearest to tuna: parmesan, metal, eye, avocado, sushi,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, sushi,\n",
      "Nearest to sushi: sesame, piece, cut, rice, tuna,\n",
      "Nearest to roll: sheet, meal, ricotta, crumb, heart,\n",
      "Nearest to sashimi: fryer, seaweed, ring, gyoza, spread,\n",
      "Nearest to steak: maple, eye, meat, grate, paste,\n",
      "Nearest to grill: soy, teriyaki, cider, chicken, mesh,\n",
      "Nearest to sauce: jack, oregano, soy, wasabi, appetizer,\n",
      "Nearest to cream: extract, chocolate, dente, cherry, puff,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, chill,\n",
      "Nearest to pizza: white, mixture, run, flake, baguette,\n",
      "Nearest to lasagna: fry, rare, lemon, noodle, teriyaki,\n",
      "Nearest to hamburger: herb, pine, meat, ½, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 125050 : 2.7591381072998047\n",
      "Loss at step 125100 : 3.278315544128418\n",
      "Loss at step 125150 : 2.8343505859375\n",
      "Loss at step 125200 : 3.5169763565063477\n",
      "Loss at step 125250 : 2.984734058380127\n",
      "Loss at step 125300 : 3.296238660812378\n",
      "Loss at step 125350 : 3.4073286056518555\n",
      "Loss at step 125400 : 3.2424097061157227\n",
      "Loss at step 125450 : 3.384644031524658\n",
      "Loss at step 125500 : 2.575746536254883\n",
      "Loss at step 125550 : 2.881246566772461\n",
      "Loss at step 125600 : 2.4894092082977295\n",
      "Loss at step 125650 : 2.617884635925293\n",
      "Loss at step 125700 : 3.795900821685791\n",
      "Loss at step 125750 : 2.881369113922119\n",
      "Loss at step 125800 : 1.946449637413025\n",
      "Loss at step 125850 : 3.325531005859375\n",
      "Loss at step 125900 : 3.7676236629486084\n",
      "Loss at step 125950 : 2.5206193923950195\n",
      "Loss at step 126000 : 2.778923511505127\n",
      "Loss at step 126050 : 2.7231173515319824\n",
      "Loss at step 126100 : 2.8477582931518555\n",
      "Loss at step 126150 : 2.806448459625244\n",
      "Loss at step 126200 : 2.557629108428955\n",
      "Loss at step 126250 : 3.3204100131988525\n",
      "Loss at step 126300 : 2.5611300468444824\n",
      "Loss at step 126350 : 3.3482232093811035\n",
      "Loss at step 126400 : 2.969512939453125\n",
      "Loss at step 126450 : 3.768331527709961\n",
      "Loss at step 126500 : 3.1898813247680664\n",
      "Loss at step 126550 : 3.7854816913604736\n",
      "Loss at step 126600 : 2.4947805404663086\n",
      "Loss at step 126650 : 2.6758649349212646\n",
      "Loss at step 126700 : 2.9252243041992188\n",
      "Loss at step 126750 : 2.183523178100586\n",
      "Loss at step 126800 : 3.006649971008301\n",
      "Loss at step 126850 : 3.010521650314331\n",
      "Loss at step 126900 : 2.367006301879883\n",
      "Loss at step 126950 : 2.805040121078491\n",
      "Loss at step 127000 : 3.680999994277954\n",
      "Loss at step 127050 : 2.5932650566101074\n",
      "Loss at step 127100 : 2.502102851867676\n",
      "Loss at step 127150 : 2.678260087966919\n",
      "Loss at step 127200 : 2.293278217315674\n",
      "Loss at step 127250 : 2.9650092124938965\n",
      "Loss at step 127300 : 3.2883334159851074\n",
      "Loss at step 127350 : 3.7231194972991943\n",
      "Loss at step 127400 : 2.901928424835205\n",
      "Loss at step 127450 : 3.109027147293091\n",
      "Loss at step 127500 : 3.583996295928955\n",
      "Loss at step 127550 : 2.483205795288086\n",
      "Loss at step 127600 : 3.0061941146850586\n",
      "Loss at step 127650 : 3.090298652648926\n",
      "Loss at step 127700 : 3.0439302921295166\n",
      "Loss at step 127750 : 3.1908576488494873\n",
      "Loss at step 127800 : 2.4721181392669678\n",
      "Loss at step 127850 : 3.2389512062072754\n",
      "Loss at step 127900 : 2.5197229385375977\n",
      "Loss at step 127950 : 3.056096315383911\n",
      "Loss at step 128000 : 3.922889232635498\n",
      "Loss at step 128050 : 3.484989643096924\n",
      "Loss at step 128100 : 2.4764084815979004\n",
      "Loss at step 128150 : 2.753051996231079\n",
      "Loss at step 128200 : 2.3493447303771973\n",
      "Loss at step 128250 : 3.0909464359283447\n",
      "Loss at step 128300 : 3.3011057376861572\n",
      "Loss at step 128350 : 2.7373485565185547\n",
      "Loss at step 128400 : 2.350895404815674\n",
      "Loss at step 128450 : 3.880460739135742\n",
      "Loss at step 128500 : 3.045262575149536\n",
      "Loss at step 128550 : 3.0453293323516846\n",
      "Loss at step 128600 : 3.0795350074768066\n",
      "Loss at step 128650 : 1.966503620147705\n",
      "Loss at step 128700 : 3.7197837829589844\n",
      "Loss at step 128750 : 3.1383039951324463\n",
      "Loss at step 128800 : 2.7454800605773926\n",
      "Loss at step 128850 : 2.436856269836426\n",
      "Loss at step 128900 : 2.787545919418335\n",
      "Loss at step 128950 : 2.6523964405059814\n",
      "Loss at step 129000 : 3.093301773071289\n",
      "Loss at step 129050 : 2.4221694469451904\n",
      "Loss at step 129100 : 3.2795071601867676\n",
      "Loss at step 129150 : 3.129554510116577\n",
      "Loss at step 129200 : 3.6324639320373535\n",
      "Loss at step 129250 : 3.20253324508667\n",
      "Loss at step 129300 : 2.2687430381774902\n",
      "Loss at step 129350 : 2.3947272300720215\n",
      "Loss at step 129400 : 3.0616440773010254\n",
      "Loss at step 129450 : 3.413179636001587\n",
      "Loss at step 129500 : 3.310661554336548\n",
      "Loss at step 129550 : 2.977189779281616\n",
      "Loss at step 129600 : 2.6093087196350098\n",
      "Loss at step 129650 : 2.252042293548584\n",
      "Loss at step 129700 : 4.229075908660889\n",
      "Loss at step 129750 : 2.671628952026367\n",
      "Loss at step 129800 : 3.620853900909424\n",
      "Loss at step 129850 : 2.3103525638580322\n",
      "Loss at step 129900 : 3.889759063720703\n",
      "Loss at step 129950 : 2.8581676483154297\n",
      "Loss at step 130000 : 2.651643991470337\n",
      "Nearest to tuna: parmesan, metal, eye, avocado, sushi,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, sesame, cut, rice, sashimi,\n",
      "Nearest to roll: sheet, meal, ricotta, crumb, heart,\n",
      "Nearest to sashimi: seaweed, fryer, ring, gyoza, spread,\n",
      "Nearest to steak: maple, eye, meat, grate, paste,\n",
      "Nearest to grill: soy, cider, teriyaki, mesh, spring,\n",
      "Nearest to sauce: jack, wasabi, soy, appetizer, oregano,\n",
      "Nearest to cream: chocolate, extract, cherry, dente, combine,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, chill, paper,\n",
      "Nearest to pizza: white, mixture, flake, run, well,\n",
      "Nearest to lasagna: fry, rare, noodle, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, pine, meat, ½, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 130050 : 2.974926471710205\n",
      "Loss at step 130100 : 2.7696518898010254\n",
      "Loss at step 130150 : 2.4188578128814697\n",
      "Loss at step 130200 : 3.0645809173583984\n",
      "Loss at step 130250 : 3.0453014373779297\n",
      "Loss at step 130300 : 3.2576863765716553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 130350 : 2.88468074798584\n",
      "Loss at step 130400 : 2.7808899879455566\n",
      "Loss at step 130450 : 2.90342116355896\n",
      "Loss at step 130500 : 3.210902214050293\n",
      "Loss at step 130550 : 3.2699222564697266\n",
      "Loss at step 130600 : 2.6547369956970215\n",
      "Loss at step 130650 : 3.5181825160980225\n",
      "Loss at step 130700 : 2.838167667388916\n",
      "Loss at step 130750 : 3.3644301891326904\n",
      "Loss at step 130800 : 3.255038261413574\n",
      "Loss at step 130850 : 3.494067668914795\n",
      "Loss at step 130900 : 2.7205264568328857\n",
      "Loss at step 130950 : 2.5375800132751465\n",
      "Loss at step 131000 : 3.6089425086975098\n",
      "Loss at step 131050 : 2.8415143489837646\n",
      "Loss at step 131100 : 3.0253705978393555\n",
      "Loss at step 131150 : 3.1772594451904297\n",
      "Loss at step 131200 : 2.656404495239258\n",
      "Loss at step 131250 : 3.4634692668914795\n",
      "Loss at step 131300 : 2.7835137844085693\n",
      "Loss at step 131350 : 3.1139492988586426\n",
      "Loss at step 131400 : 2.9128031730651855\n",
      "Loss at step 131450 : 3.1616861820220947\n",
      "Loss at step 131500 : 3.2058897018432617\n",
      "Loss at step 131550 : 2.4986002445220947\n",
      "Loss at step 131600 : 2.8393712043762207\n",
      "Loss at step 131650 : 3.2403156757354736\n",
      "Loss at step 131700 : 3.6715099811553955\n",
      "Loss at step 131750 : 3.267314910888672\n",
      "Loss at step 131800 : 2.757476806640625\n",
      "Loss at step 131850 : 3.863065719604492\n",
      "Loss at step 131900 : 2.9700818061828613\n",
      "Loss at step 131950 : 2.9894824028015137\n",
      "Loss at step 132000 : 2.9265353679656982\n",
      "Loss at step 132050 : 2.6878085136413574\n",
      "Loss at step 132100 : 3.6198296546936035\n",
      "Loss at step 132150 : 3.1912131309509277\n",
      "Loss at step 132200 : 3.1644084453582764\n",
      "Loss at step 132250 : 2.8585023880004883\n",
      "Loss at step 132300 : 3.0937843322753906\n",
      "Loss at step 132350 : 2.3199849128723145\n",
      "Loss at step 132400 : 2.9970340728759766\n",
      "Loss at step 132450 : 2.604524612426758\n",
      "Loss at step 132500 : 3.789275884628296\n",
      "Loss at step 132550 : 2.8798317909240723\n",
      "Loss at step 132600 : 2.511613607406616\n",
      "Loss at step 132650 : 3.399630546569824\n",
      "Loss at step 132700 : 2.7785892486572266\n",
      "Loss at step 132750 : 3.2255873680114746\n",
      "Loss at step 132800 : 3.809957504272461\n",
      "Loss at step 132850 : 2.9078664779663086\n",
      "Loss at step 132900 : 3.278512716293335\n",
      "Loss at step 132950 : 3.3173060417175293\n",
      "Loss at step 133000 : 2.7599570751190186\n",
      "Loss at step 133050 : 2.699066638946533\n",
      "Loss at step 133100 : 2.716557502746582\n",
      "Loss at step 133150 : 2.9479713439941406\n",
      "Loss at step 133200 : 2.6174893379211426\n",
      "Loss at step 133250 : 3.0915675163269043\n",
      "Loss at step 133300 : 4.136997699737549\n",
      "Loss at step 133350 : 2.6969425678253174\n",
      "Loss at step 133400 : 3.4613094329833984\n",
      "Loss at step 133450 : 3.832612991333008\n",
      "Loss at step 133500 : 3.6103649139404297\n",
      "Loss at step 133550 : 2.576439142227173\n",
      "Loss at step 133600 : 3.287313938140869\n",
      "Loss at step 133650 : 2.5609235763549805\n",
      "Loss at step 133700 : 3.127598285675049\n",
      "Loss at step 133750 : 2.9803662300109863\n",
      "Loss at step 133800 : 3.1866042613983154\n",
      "Loss at step 133850 : 2.860576868057251\n",
      "Loss at step 133900 : 2.528909683227539\n",
      "Loss at step 133950 : 2.638498544692993\n",
      "Loss at step 134000 : 2.531533718109131\n",
      "Loss at step 134050 : 2.087841033935547\n",
      "Loss at step 134100 : 2.95572566986084\n",
      "Loss at step 134150 : 3.709590435028076\n",
      "Loss at step 134200 : 2.9140944480895996\n",
      "Loss at step 134250 : 3.312380075454712\n",
      "Loss at step 134300 : 2.8802900314331055\n",
      "Loss at step 134350 : 2.700099468231201\n",
      "Loss at step 134400 : 4.126672744750977\n",
      "Loss at step 134450 : 2.563741683959961\n",
      "Loss at step 134500 : 2.470221757888794\n",
      "Loss at step 134550 : 2.6442534923553467\n",
      "Loss at step 134600 : 4.316506862640381\n",
      "Loss at step 134650 : 2.7016854286193848\n",
      "Loss at step 134700 : 4.1248297691345215\n",
      "Loss at step 134750 : 2.4808623790740967\n",
      "Loss at step 134800 : 3.362476348876953\n",
      "Loss at step 134850 : 2.281949996948242\n",
      "Loss at step 134900 : 1.6506050825119019\n",
      "Loss at step 134950 : 3.2851016521453857\n",
      "Loss at step 135000 : 2.713831901550293\n",
      "Nearest to tuna: parmesan, metal, eye, sushi, position,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, sesame, cut, sashimi, rice,\n",
      "Nearest to roll: sheet, meal, ricotta, crumb, touch,\n",
      "Nearest to sashimi: seaweed, fryer, ring, gyoza, sushi,\n",
      "Nearest to steak: maple, meat, eye, grate, paste,\n",
      "Nearest to grill: soy, teriyaki, cider, spring, mesh,\n",
      "Nearest to sauce: jack, wasabi, soy, oregano, appetizer,\n",
      "Nearest to cream: extract, chocolate, cherry, dente, cake,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, chill, paper,\n",
      "Nearest to pizza: mixture, white, flake, run, well,\n",
      "Nearest to lasagna: fry, rare, noodle, teriyaki, lemon,\n",
      "Nearest to hamburger: meat, herb, pine, fill, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 135050 : 4.090289115905762\n",
      "Loss at step 135100 : 2.8556876182556152\n",
      "Loss at step 135150 : 3.2567524909973145\n",
      "Loss at step 135200 : 2.5776963233947754\n",
      "Loss at step 135250 : 2.3635215759277344\n",
      "Loss at step 135300 : 2.453322410583496\n",
      "Loss at step 135350 : 3.0233213901519775\n",
      "Loss at step 135400 : 3.459275245666504\n",
      "Loss at step 135450 : 3.6849441528320312\n",
      "Loss at step 135500 : 3.2044808864593506\n",
      "Loss at step 135550 : 2.925980567932129\n",
      "Loss at step 135600 : 3.6276919841766357\n",
      "Loss at step 135650 : 2.96060848236084\n",
      "Loss at step 135700 : 2.2462620735168457\n",
      "Loss at step 135750 : 3.67297101020813\n",
      "Loss at step 135800 : 2.909151077270508\n",
      "Loss at step 135850 : 2.7269253730773926\n",
      "Loss at step 135900 : 3.554553508758545\n",
      "Loss at step 135950 : 2.9238009452819824\n",
      "Loss at step 136000 : 2.836571216583252\n",
      "Loss at step 136050 : 2.6070404052734375\n",
      "Loss at step 136100 : 3.9111781120300293\n",
      "Loss at step 136150 : 4.1382036209106445\n",
      "Loss at step 136200 : 2.8663694858551025\n",
      "Loss at step 136250 : 3.1830337047576904\n",
      "Loss at step 136300 : 2.836190938949585\n",
      "Loss at step 136350 : 2.3920977115631104\n",
      "Loss at step 136400 : 2.7538840770721436\n",
      "Loss at step 136450 : 3.1148252487182617\n",
      "Loss at step 136500 : 3.423522710800171\n",
      "Loss at step 136550 : 3.94943904876709\n",
      "Loss at step 136600 : 2.6942644119262695\n",
      "Loss at step 136650 : 1.9872262477874756\n",
      "Loss at step 136700 : 2.215473175048828\n",
      "Loss at step 136750 : 3.984575033187866\n",
      "Loss at step 136800 : 2.9314725399017334\n",
      "Loss at step 136850 : 3.3862123489379883\n",
      "Loss at step 136900 : 2.679647445678711\n",
      "Loss at step 136950 : 2.8190650939941406\n",
      "Loss at step 137000 : 2.4444236755371094\n",
      "Loss at step 137050 : 3.2061333656311035\n",
      "Loss at step 137100 : 2.8327648639678955\n",
      "Loss at step 137150 : 1.7959458827972412\n",
      "Loss at step 137200 : 2.6809685230255127\n",
      "Loss at step 137250 : 2.792689800262451\n",
      "Loss at step 137300 : 2.5473647117614746\n",
      "Loss at step 137350 : 2.8821935653686523\n",
      "Loss at step 137400 : 2.045065402984619\n",
      "Loss at step 137450 : 3.270671844482422\n",
      "Loss at step 137500 : 3.369314432144165\n",
      "Loss at step 137550 : 3.587597370147705\n",
      "Loss at step 137600 : 3.249872922897339\n",
      "Loss at step 137650 : 2.799370288848877\n",
      "Loss at step 137700 : 2.795863628387451\n",
      "Loss at step 137750 : 2.475374698638916\n",
      "Loss at step 137800 : 2.9009528160095215\n",
      "Loss at step 137850 : 3.8407740592956543\n",
      "Loss at step 137900 : 3.6195430755615234\n",
      "Loss at step 137950 : 2.864968776702881\n",
      "Loss at step 138000 : 2.671476364135742\n",
      "Loss at step 138050 : 2.9975833892822266\n",
      "Loss at step 138100 : 2.813263416290283\n",
      "Loss at step 138150 : 2.0374393463134766\n",
      "Loss at step 138200 : 2.7577438354492188\n",
      "Loss at step 138250 : 2.6906042098999023\n",
      "Loss at step 138300 : 2.0967702865600586\n",
      "Loss at step 138350 : 3.4389166831970215\n",
      "Loss at step 138400 : 3.6701622009277344\n",
      "Loss at step 138450 : 3.303741931915283\n",
      "Loss at step 138500 : 2.7789807319641113\n",
      "Loss at step 138550 : 2.9880003929138184\n",
      "Loss at step 138600 : 3.1287503242492676\n",
      "Loss at step 138650 : 3.4429755210876465\n",
      "Loss at step 138700 : 3.9364805221557617\n",
      "Loss at step 138750 : 3.1680164337158203\n",
      "Loss at step 138800 : 3.3325984477996826\n",
      "Loss at step 138850 : 3.4320480823516846\n",
      "Loss at step 138900 : 2.111764907836914\n",
      "Loss at step 138950 : 2.4132437705993652\n",
      "Loss at step 139000 : 2.8894834518432617\n",
      "Loss at step 139050 : 2.5504119396209717\n",
      "Loss at step 139100 : 2.5257017612457275\n",
      "Loss at step 139150 : 2.7074549198150635\n",
      "Loss at step 139200 : 2.8624777793884277\n",
      "Loss at step 139250 : 3.399355888366699\n",
      "Loss at step 139300 : 3.208408832550049\n",
      "Loss at step 139350 : 3.36484432220459\n",
      "Loss at step 139400 : 3.4118294715881348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 139450 : 2.663421630859375\n",
      "Loss at step 139500 : 2.8574271202087402\n",
      "Loss at step 139550 : 2.4057626724243164\n",
      "Loss at step 139600 : 2.689786672592163\n",
      "Loss at step 139650 : 3.219754219055176\n",
      "Loss at step 139700 : 3.223891258239746\n",
      "Loss at step 139750 : 2.2138829231262207\n",
      "Loss at step 139800 : 2.570228099822998\n",
      "Loss at step 139850 : 2.749394416809082\n",
      "Loss at step 139900 : 2.7820987701416016\n",
      "Loss at step 139950 : 3.3747854232788086\n",
      "Loss at step 140000 : 2.7993083000183105\n",
      "Nearest to tuna: parmesan, metal, eye, sushi, position,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, cut, sesame, rice, sashimi,\n",
      "Nearest to roll: sheet, meal, crumb, touch, ricotta,\n",
      "Nearest to sashimi: seaweed, fryer, ring, gyoza, sushi,\n",
      "Nearest to steak: maple, eye, meat, grate, paste,\n",
      "Nearest to grill: cider, soy, teriyaki, spring, mesh,\n",
      "Nearest to sauce: jack, wasabi, appetizer, oregano, soy,\n",
      "Nearest to cream: extract, chocolate, cherry, cake, burger,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: white, mixture, flake, well, run,\n",
      "Nearest to lasagna: fry, rare, noodle, teriyaki, lemon,\n",
      "Nearest to hamburger: meat, ½, herb, pine, fill,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 140050 : 2.4879589080810547\n",
      "Loss at step 140100 : 2.9326250553131104\n",
      "Loss at step 140150 : 2.1918013095855713\n",
      "Loss at step 140200 : 2.5790934562683105\n",
      "Loss at step 140250 : 3.3047757148742676\n",
      "Loss at step 140300 : 3.7697505950927734\n",
      "Loss at step 140350 : 2.5950193405151367\n",
      "Loss at step 140400 : 2.9434447288513184\n",
      "Loss at step 140450 : 2.4632365703582764\n",
      "Loss at step 140500 : 2.640120506286621\n",
      "Loss at step 140550 : 2.6772725582122803\n",
      "Loss at step 140600 : 1.7677521705627441\n",
      "Loss at step 140650 : 2.483670234680176\n",
      "Loss at step 140700 : 2.8195507526397705\n",
      "Loss at step 140750 : 2.9715471267700195\n",
      "Loss at step 140800 : 2.7428717613220215\n",
      "Loss at step 140850 : 3.420351505279541\n",
      "Loss at step 140900 : 2.411928415298462\n",
      "Loss at step 140950 : 3.3993544578552246\n",
      "Loss at step 141000 : 3.2941689491271973\n",
      "Loss at step 141050 : 1.4844441413879395\n",
      "Loss at step 141100 : 2.928652286529541\n",
      "Loss at step 141150 : 3.2255969047546387\n",
      "Loss at step 141200 : 2.623485803604126\n",
      "Loss at step 141250 : 2.7508420944213867\n",
      "Loss at step 141300 : 2.3877105712890625\n",
      "Loss at step 141350 : 3.753640651702881\n",
      "Loss at step 141400 : 2.7691423892974854\n",
      "Loss at step 141450 : 3.4288041591644287\n",
      "Loss at step 141500 : 3.29814076423645\n",
      "Loss at step 141550 : 3.456294059753418\n",
      "Loss at step 141600 : 2.800682306289673\n",
      "Loss at step 141650 : 3.2470943927764893\n",
      "Loss at step 141700 : 2.7920310497283936\n",
      "Loss at step 141750 : 2.956082344055176\n",
      "Loss at step 141800 : 2.727771043777466\n",
      "Loss at step 141850 : 2.9027163982391357\n",
      "Loss at step 141900 : 2.535518169403076\n",
      "Loss at step 141950 : 2.5191893577575684\n",
      "Loss at step 142000 : 2.9428579807281494\n",
      "Loss at step 142050 : 2.484562397003174\n",
      "Loss at step 142100 : 2.341796636581421\n",
      "Loss at step 142150 : 3.3650708198547363\n",
      "Loss at step 142200 : 2.4189445972442627\n",
      "Loss at step 142250 : 3.3174026012420654\n",
      "Loss at step 142300 : 3.038814067840576\n",
      "Loss at step 142350 : 3.3927202224731445\n",
      "Loss at step 142400 : 3.1804702281951904\n",
      "Loss at step 142450 : 3.20157527923584\n",
      "Loss at step 142500 : 3.208083152770996\n",
      "Loss at step 142550 : 2.5142641067504883\n",
      "Loss at step 142600 : 3.0282368659973145\n",
      "Loss at step 142650 : 3.364325523376465\n",
      "Loss at step 142700 : 2.9298605918884277\n",
      "Loss at step 142750 : 3.6753432750701904\n",
      "Loss at step 142800 : 3.5102362632751465\n",
      "Loss at step 142850 : 3.0391690731048584\n",
      "Loss at step 142900 : 3.139821767807007\n",
      "Loss at step 142950 : 3.3637468814849854\n",
      "Loss at step 143000 : 3.288670301437378\n",
      "Loss at step 143050 : 2.9052271842956543\n",
      "Loss at step 143100 : 3.9706149101257324\n",
      "Loss at step 143150 : 2.9212098121643066\n",
      "Loss at step 143200 : 3.226815700531006\n",
      "Loss at step 143250 : 2.0868711471557617\n",
      "Loss at step 143300 : 2.846538782119751\n",
      "Loss at step 143350 : 3.5276076793670654\n",
      "Loss at step 143400 : 2.665820360183716\n",
      "Loss at step 143450 : 3.07464599609375\n",
      "Loss at step 143500 : 3.1685056686401367\n",
      "Loss at step 143550 : 2.0870463848114014\n",
      "Loss at step 143600 : 3.519017219543457\n",
      "Loss at step 143650 : 2.84936785697937\n",
      "Loss at step 143700 : 2.8817787170410156\n",
      "Loss at step 143750 : 2.7924437522888184\n",
      "Loss at step 143800 : 3.169111490249634\n",
      "Loss at step 143850 : 2.282392740249634\n",
      "Loss at step 143900 : 2.7417421340942383\n",
      "Loss at step 143950 : 3.4646410942077637\n",
      "Loss at step 144000 : 2.8827853202819824\n",
      "Loss at step 144050 : 3.5778284072875977\n",
      "Loss at step 144100 : 2.6968281269073486\n",
      "Loss at step 144150 : 3.119619846343994\n",
      "Loss at step 144200 : 3.6776461601257324\n",
      "Loss at step 144250 : 2.3582096099853516\n",
      "Loss at step 144300 : 2.8254735469818115\n",
      "Loss at step 144350 : 3.4836106300354004\n",
      "Loss at step 144400 : 3.7674875259399414\n",
      "Loss at step 144450 : 2.1927969455718994\n",
      "Loss at step 144500 : 3.015401840209961\n",
      "Loss at step 144550 : 3.8069496154785156\n",
      "Loss at step 144600 : 3.2026424407958984\n",
      "Loss at step 144650 : 3.0704352855682373\n",
      "Loss at step 144700 : 2.4833292961120605\n",
      "Loss at step 144750 : 2.7854223251342773\n",
      "Loss at step 144800 : 2.6711740493774414\n",
      "Loss at step 144850 : 1.8769142627716064\n",
      "Loss at step 144900 : 2.2339935302734375\n",
      "Loss at step 144950 : 3.407787799835205\n",
      "Loss at step 145000 : 1.8466145992279053\n",
      "Nearest to tuna: parmesan, metal, eye, avocado, steak,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, cut, sashimi, rice, cornstarch,\n",
      "Nearest to roll: sheet, meal, ricotta, crumb, touch,\n",
      "Nearest to sashimi: seaweed, ring, fryer, gyoza, sushi,\n",
      "Nearest to steak: maple, meat, eye, grate, paste,\n",
      "Nearest to grill: cider, teriyaki, soy, blender, spring,\n",
      "Nearest to sauce: jack, wasabi, oregano, appetizer, soy,\n",
      "Nearest to cream: chocolate, extract, cherry, burger, cake,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: mixture, white, run, flake, well,\n",
      "Nearest to lasagna: fry, rare, noodle, lemon, teriyaki,\n",
      "Nearest to hamburger: ½, meat, herb, pine, confectioner,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 145050 : 2.808701753616333\n",
      "Loss at step 145100 : 2.719592332839966\n",
      "Loss at step 145150 : 3.036241054534912\n",
      "Loss at step 145200 : 2.9315505027770996\n",
      "Loss at step 145250 : 3.095132827758789\n",
      "Loss at step 145300 : 2.900233745574951\n",
      "Loss at step 145350 : 2.856877326965332\n",
      "Loss at step 145400 : 3.1468331813812256\n",
      "Loss at step 145450 : 3.413193702697754\n",
      "Loss at step 145500 : 3.146749258041382\n",
      "Loss at step 145550 : 2.54913330078125\n",
      "Loss at step 145600 : 1.7986785173416138\n",
      "Loss at step 145650 : 3.071962356567383\n",
      "Loss at step 145700 : 3.235797643661499\n",
      "Loss at step 145750 : 3.456071615219116\n",
      "Loss at step 145800 : 3.404113292694092\n",
      "Loss at step 145850 : 2.251722812652588\n",
      "Loss at step 145900 : 3.6997880935668945\n",
      "Loss at step 145950 : 2.5829379558563232\n",
      "Loss at step 146000 : 3.2188639640808105\n",
      "Loss at step 146050 : 2.7976021766662598\n",
      "Loss at step 146100 : 2.6805977821350098\n",
      "Loss at step 146150 : 2.7587320804595947\n",
      "Loss at step 146200 : 2.7450733184814453\n",
      "Loss at step 146250 : 2.6402578353881836\n",
      "Loss at step 146300 : 2.9650847911834717\n",
      "Loss at step 146350 : 2.279738426208496\n",
      "Loss at step 146400 : 3.673288345336914\n",
      "Loss at step 146450 : 2.975898265838623\n",
      "Loss at step 146500 : 2.5086398124694824\n",
      "Loss at step 146550 : 2.490729808807373\n",
      "Loss at step 146600 : 2.1739048957824707\n",
      "Loss at step 146650 : 2.9901905059814453\n",
      "Loss at step 146700 : 3.0617151260375977\n",
      "Loss at step 146750 : 2.2348568439483643\n",
      "Loss at step 146800 : 2.15545654296875\n",
      "Loss at step 146850 : 2.972538948059082\n",
      "Loss at step 146900 : 3.1983642578125\n",
      "Loss at step 146950 : 2.7263569831848145\n",
      "Loss at step 147000 : 2.8842179775238037\n",
      "Loss at step 147050 : 3.2647738456726074\n",
      "Loss at step 147100 : 2.768239736557007\n",
      "Loss at step 147150 : 3.45759654045105\n",
      "Loss at step 147200 : 3.088491678237915\n",
      "Loss at step 147250 : 3.9015955924987793\n",
      "Loss at step 147300 : 2.9662978649139404\n",
      "Loss at step 147350 : 3.9170873165130615\n",
      "Loss at step 147400 : 2.2457456588745117\n",
      "Loss at step 147450 : 1.4221417903900146\n",
      "Loss at step 147500 : 3.607787609100342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 147550 : 2.779452323913574\n",
      "Loss at step 147600 : 2.417630195617676\n",
      "Loss at step 147650 : 4.643395900726318\n",
      "Loss at step 147700 : 3.30183482170105\n",
      "Loss at step 147750 : 3.8490688800811768\n",
      "Loss at step 147800 : 3.6846227645874023\n",
      "Loss at step 147850 : 2.7226083278656006\n",
      "Loss at step 147900 : 2.133190155029297\n",
      "Loss at step 147950 : 2.269136428833008\n",
      "Loss at step 148000 : 2.977020740509033\n",
      "Loss at step 148050 : 1.850095272064209\n",
      "Loss at step 148100 : 2.6391232013702393\n",
      "Loss at step 148150 : 2.5052330493927\n",
      "Loss at step 148200 : 3.489068031311035\n",
      "Loss at step 148250 : 2.8683271408081055\n",
      "Loss at step 148300 : 2.6195549964904785\n",
      "Loss at step 148350 : 4.058194637298584\n",
      "Loss at step 148400 : 2.679619550704956\n",
      "Loss at step 148450 : 2.9161016941070557\n",
      "Loss at step 148500 : 2.815415859222412\n",
      "Loss at step 148550 : 2.632970094680786\n",
      "Loss at step 148600 : 2.8811628818511963\n",
      "Loss at step 148650 : 2.8162264823913574\n",
      "Loss at step 148700 : 3.039147138595581\n",
      "Loss at step 148750 : 2.6289401054382324\n",
      "Loss at step 148800 : 3.367141008377075\n",
      "Loss at step 148850 : 2.0621747970581055\n",
      "Loss at step 148900 : 2.901251792907715\n",
      "Loss at step 148950 : 2.739302158355713\n",
      "Loss at step 149000 : 2.818519353866577\n",
      "Loss at step 149050 : 3.3487439155578613\n",
      "Loss at step 149100 : 2.6024250984191895\n",
      "Loss at step 149150 : 2.684253215789795\n",
      "Loss at step 149200 : 2.8611011505126953\n",
      "Loss at step 149250 : 2.7545976638793945\n",
      "Loss at step 149300 : 3.5042166709899902\n",
      "Loss at step 149350 : 2.9764456748962402\n",
      "Loss at step 149400 : 2.9855079650878906\n",
      "Loss at step 149450 : 2.283155918121338\n",
      "Loss at step 149500 : 2.830354928970337\n",
      "Loss at step 149550 : 2.8699288368225098\n",
      "Loss at step 149600 : 2.387099266052246\n",
      "Loss at step 149650 : 2.8799424171447754\n",
      "Loss at step 149700 : 3.6569454669952393\n",
      "Loss at step 149750 : 2.73728609085083\n",
      "Loss at step 149800 : 3.317535400390625\n",
      "Loss at step 149850 : 2.3187966346740723\n",
      "Loss at step 149900 : 3.165024757385254\n",
      "Loss at step 149950 : 2.819719076156616\n",
      "Loss at step 150000 : 2.1167190074920654\n",
      "Nearest to tuna: parmesan, metal, eye, steak, virgin,\n",
      "Nearest to rice: thigh, mandu, lime, spinach, simmer,\n",
      "Nearest to sushi: piece, sashimi, cut, sesame, rice,\n",
      "Nearest to roll: ricotta, sheet, meal, touch, crumb,\n",
      "Nearest to sashimi: seaweed, ring, fryer, gyoza, sushi,\n",
      "Nearest to steak: maple, eye, meat, grate, paste,\n",
      "Nearest to grill: soy, cider, teriyaki, mesh, chicken,\n",
      "Nearest to sauce: jack, wasabi, oregano, appetizer, soy,\n",
      "Nearest to cream: cherry, extract, chocolate, cake, burger,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: mixture, white, baguette, flake, well,\n",
      "Nearest to lasagna: fry, rare, noodle, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, ½, pine, parsley,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 150050 : 2.7460081577301025\n",
      "Loss at step 150100 : 2.1471338272094727\n",
      "Loss at step 150150 : 3.1056599617004395\n",
      "Loss at step 150200 : 2.800114631652832\n",
      "Loss at step 150250 : 3.3136143684387207\n",
      "Loss at step 150300 : 3.4130144119262695\n",
      "Loss at step 150350 : 2.176389217376709\n",
      "Loss at step 150400 : 3.241304636001587\n",
      "Loss at step 150450 : 2.7216827869415283\n",
      "Loss at step 150500 : 3.0318353176116943\n",
      "Loss at step 150550 : 4.005123615264893\n",
      "Loss at step 150600 : 2.5645828247070312\n",
      "Loss at step 150650 : 3.4066872596740723\n",
      "Loss at step 150700 : 2.741467237472534\n",
      "Loss at step 150750 : 2.8462066650390625\n",
      "Loss at step 150800 : 3.2803049087524414\n",
      "Loss at step 150850 : 3.5114922523498535\n",
      "Loss at step 150900 : 4.026699066162109\n",
      "Loss at step 150950 : 2.754478931427002\n",
      "Loss at step 151000 : 2.939044237136841\n",
      "Loss at step 151050 : 3.4708504676818848\n",
      "Loss at step 151100 : 2.1321473121643066\n",
      "Loss at step 151150 : 3.003657341003418\n",
      "Loss at step 151200 : 3.523195266723633\n",
      "Loss at step 151250 : 2.8370518684387207\n",
      "Loss at step 151300 : 2.3858752250671387\n",
      "Loss at step 151350 : 2.0548086166381836\n",
      "Loss at step 151400 : 3.3311002254486084\n",
      "Loss at step 151450 : 1.9981987476348877\n",
      "Loss at step 151500 : 2.357962131500244\n",
      "Loss at step 151550 : 3.5199804306030273\n",
      "Loss at step 151600 : 2.3770809173583984\n",
      "Loss at step 151650 : 3.134215831756592\n",
      "Loss at step 151700 : 2.615790605545044\n",
      "Loss at step 151750 : 2.636488676071167\n",
      "Loss at step 151800 : 3.4471874237060547\n",
      "Loss at step 151850 : 2.72977876663208\n",
      "Loss at step 151900 : 3.3748412132263184\n",
      "Loss at step 151950 : 2.766096591949463\n",
      "Loss at step 152000 : 3.233907699584961\n",
      "Loss at step 152050 : 2.509000778198242\n",
      "Loss at step 152100 : 3.4360175132751465\n",
      "Loss at step 152150 : 4.153198719024658\n",
      "Loss at step 152200 : 2.50077748298645\n",
      "Loss at step 152250 : 3.3466920852661133\n",
      "Loss at step 152300 : 3.3391127586364746\n",
      "Loss at step 152350 : 2.777590036392212\n",
      "Loss at step 152400 : 3.2375288009643555\n",
      "Loss at step 152450 : 2.837141275405884\n",
      "Loss at step 152500 : 3.892137289047241\n",
      "Loss at step 152550 : 2.657569408416748\n",
      "Loss at step 152600 : 1.977573275566101\n",
      "Loss at step 152650 : 2.9469046592712402\n",
      "Loss at step 152700 : 2.736618995666504\n",
      "Loss at step 152750 : 2.146897315979004\n",
      "Loss at step 152800 : 2.7457613945007324\n",
      "Loss at step 152850 : 3.3196401596069336\n",
      "Loss at step 152900 : 3.0429630279541016\n",
      "Loss at step 152950 : 3.3673808574676514\n",
      "Loss at step 153000 : 3.3815510272979736\n",
      "Loss at step 153050 : 3.203551769256592\n",
      "Loss at step 153100 : 3.3774068355560303\n",
      "Loss at step 153150 : 3.8695244789123535\n",
      "Loss at step 153200 : 3.178097724914551\n",
      "Loss at step 153250 : 3.9740777015686035\n",
      "Loss at step 153300 : 3.5363783836364746\n",
      "Loss at step 153350 : 3.0842204093933105\n",
      "Loss at step 153400 : 2.5882797241210938\n",
      "Loss at step 153450 : 3.389352798461914\n",
      "Loss at step 153500 : 3.0239410400390625\n",
      "Loss at step 153550 : 2.6795716285705566\n",
      "Loss at step 153600 : 2.2913150787353516\n",
      "Loss at step 153650 : 2.988417625427246\n",
      "Loss at step 153700 : 2.8853790760040283\n",
      "Loss at step 153750 : 2.7415709495544434\n",
      "Loss at step 153800 : 2.3841867446899414\n",
      "Loss at step 153850 : 3.374950408935547\n",
      "Loss at step 153900 : 2.74615478515625\n",
      "Loss at step 153950 : 2.426563262939453\n",
      "Loss at step 154000 : 3.011476516723633\n",
      "Loss at step 154050 : 2.7675254344940186\n",
      "Loss at step 154100 : 3.8000497817993164\n",
      "Loss at step 154150 : 3.0060532093048096\n",
      "Loss at step 154200 : 3.112919569015503\n",
      "Loss at step 154250 : 1.9604542255401611\n",
      "Loss at step 154300 : 2.640871047973633\n",
      "Loss at step 154350 : 2.779785394668579\n",
      "Loss at step 154400 : 2.1378226280212402\n",
      "Loss at step 154450 : 1.9634380340576172\n",
      "Loss at step 154500 : 3.455364227294922\n",
      "Loss at step 154550 : 3.068528175354004\n",
      "Loss at step 154600 : 3.5760583877563477\n",
      "Loss at step 154650 : 3.41654634475708\n",
      "Loss at step 154700 : 2.5047354698181152\n",
      "Loss at step 154750 : 2.89479923248291\n",
      "Loss at step 154800 : 2.535490036010742\n",
      "Loss at step 154850 : 4.399370193481445\n",
      "Loss at step 154900 : 4.317167282104492\n",
      "Loss at step 154950 : 3.0776941776275635\n",
      "Loss at step 155000 : 1.5176854133605957\n",
      "Nearest to tuna: parmesan, metal, eye, steak, moisture,\n",
      "Nearest to rice: thigh, mandu, lime, spinach, simmer,\n",
      "Nearest to sushi: piece, sashimi, sesame, cut, rice,\n",
      "Nearest to roll: sheet, ricotta, touch, meal, salt,\n",
      "Nearest to sashimi: ring, fryer, seaweed, gyoza, sushi,\n",
      "Nearest to steak: maple, eye, meat, grate, marinade,\n",
      "Nearest to grill: soy, cider, teriyaki, spring, blender,\n",
      "Nearest to sauce: jack, wasabi, appetizer, soy, oregano,\n",
      "Nearest to cream: cherry, extract, chocolate, cake, box,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: mixture, white, baguette, flake, well,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, facing,\n",
      "Nearest to hamburger: herb, ½, meat, pine, parsley,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 155050 : 1.660325288772583\n",
      "Loss at step 155100 : 3.2764368057250977\n",
      "Loss at step 155150 : 3.4740428924560547\n",
      "Loss at step 155200 : 2.5312962532043457\n",
      "Loss at step 155250 : 2.731884002685547\n",
      "Loss at step 155300 : 3.1180336475372314\n",
      "Loss at step 155350 : 3.726508140563965\n",
      "Loss at step 155400 : 2.8130416870117188\n",
      "Loss at step 155450 : 2.94555401802063\n",
      "Loss at step 155500 : 2.617370128631592\n",
      "Loss at step 155550 : 2.437419891357422\n",
      "Loss at step 155600 : 2.4876742362976074\n",
      "Loss at step 155650 : 2.5616159439086914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 155700 : 2.4613022804260254\n",
      "Loss at step 155750 : 2.668509006500244\n",
      "Loss at step 155800 : 2.6610560417175293\n",
      "Loss at step 155850 : 3.38708758354187\n",
      "Loss at step 155900 : 2.685289144515991\n",
      "Loss at step 155950 : 2.374213695526123\n",
      "Loss at step 156000 : 3.3700156211853027\n",
      "Loss at step 156050 : 2.354753255844116\n",
      "Loss at step 156100 : 3.1115245819091797\n",
      "Loss at step 156150 : 3.069352149963379\n",
      "Loss at step 156200 : 2.9018664360046387\n",
      "Loss at step 156250 : 2.8669471740722656\n",
      "Loss at step 156300 : 2.855869770050049\n",
      "Loss at step 156350 : 2.831052780151367\n",
      "Loss at step 156400 : 2.855966329574585\n",
      "Loss at step 156450 : 2.7862534523010254\n",
      "Loss at step 156500 : 2.343982219696045\n",
      "Loss at step 156550 : 2.2147722244262695\n",
      "Loss at step 156600 : 2.529890298843384\n",
      "Loss at step 156650 : 3.3963139057159424\n",
      "Loss at step 156700 : 2.3374271392822266\n",
      "Loss at step 156750 : 3.467467784881592\n",
      "Loss at step 156800 : 3.1990675926208496\n",
      "Loss at step 156850 : 2.7969460487365723\n",
      "Loss at step 156900 : 2.7478976249694824\n",
      "Loss at step 156950 : 2.810084342956543\n",
      "Loss at step 157000 : 2.586164712905884\n",
      "Loss at step 157050 : 2.766144275665283\n",
      "Loss at step 157100 : 2.4479846954345703\n",
      "Loss at step 157150 : 1.4279924631118774\n",
      "Loss at step 157200 : 2.5324506759643555\n",
      "Loss at step 157250 : 2.6188526153564453\n",
      "Loss at step 157300 : 3.324425220489502\n",
      "Loss at step 157350 : 2.265073537826538\n",
      "Loss at step 157400 : 2.959351062774658\n",
      "Loss at step 157450 : 3.3067965507507324\n",
      "Loss at step 157500 : 2.5591683387756348\n",
      "Loss at step 157550 : 2.943934202194214\n",
      "Loss at step 157600 : 2.7023463249206543\n",
      "Loss at step 157650 : 2.644658327102661\n",
      "Loss at step 157700 : 2.5930657386779785\n",
      "Loss at step 157750 : 2.6971864700317383\n",
      "Loss at step 157800 : 2.910560131072998\n",
      "Loss at step 157850 : 3.2179903984069824\n",
      "Loss at step 157900 : 3.394486427307129\n",
      "Loss at step 157950 : 2.7411017417907715\n",
      "Loss at step 158000 : 3.201948642730713\n",
      "Loss at step 158050 : 3.332819700241089\n",
      "Loss at step 158100 : 4.47674560546875\n",
      "Loss at step 158150 : 2.7154054641723633\n",
      "Loss at step 158200 : 2.277937173843384\n",
      "Loss at step 158250 : 3.660921812057495\n",
      "Loss at step 158300 : 3.0604000091552734\n",
      "Loss at step 158350 : 2.95133900642395\n",
      "Loss at step 158400 : 2.4618453979492188\n",
      "Loss at step 158450 : 3.1624138355255127\n",
      "Loss at step 158500 : 2.84098482131958\n",
      "Loss at step 158550 : 3.3442649841308594\n",
      "Loss at step 158600 : 2.374141216278076\n",
      "Loss at step 158650 : 2.690579891204834\n",
      "Loss at step 158700 : 2.743154525756836\n",
      "Loss at step 158750 : 2.020390272140503\n",
      "Loss at step 158800 : 2.914442300796509\n",
      "Loss at step 158850 : 2.985837697982788\n",
      "Loss at step 158900 : 2.614952564239502\n",
      "Loss at step 158950 : 2.5291335582733154\n",
      "Loss at step 159000 : 3.4861602783203125\n",
      "Loss at step 159050 : 3.2825629711151123\n",
      "Loss at step 159100 : 2.0872654914855957\n",
      "Loss at step 159150 : 3.1101484298706055\n",
      "Loss at step 159200 : 2.620844841003418\n",
      "Loss at step 159250 : 3.634697198867798\n",
      "Loss at step 159300 : 2.3656792640686035\n",
      "Loss at step 159350 : 2.4205853939056396\n",
      "Loss at step 159400 : 3.6468374729156494\n",
      "Loss at step 159450 : 3.377727746963501\n",
      "Loss at step 159500 : 3.5825865268707275\n",
      "Loss at step 159550 : 2.4100825786590576\n",
      "Loss at step 159600 : 3.354904890060425\n",
      "Loss at step 159650 : 2.7342610359191895\n",
      "Loss at step 159700 : 2.7099146842956543\n",
      "Loss at step 159750 : 3.1009340286254883\n",
      "Loss at step 159800 : 2.6458311080932617\n",
      "Loss at step 159850 : 2.6269097328186035\n",
      "Loss at step 159900 : 2.6014978885650635\n",
      "Loss at step 159950 : 3.2139170169830322\n",
      "Loss at step 160000 : 2.1983494758605957\n",
      "Nearest to tuna: parmesan, metal, eye, position, sushi,\n",
      "Nearest to rice: thigh, mandu, lime, spinach, simmer,\n",
      "Nearest to sushi: piece, cut, sesame, rice, sashimi,\n",
      "Nearest to roll: sheet, meal, ricotta, touch, crumb,\n",
      "Nearest to sashimi: ring, fryer, seaweed, gyoza, sushi,\n",
      "Nearest to steak: maple, meat, eye, grate, paste,\n",
      "Nearest to grill: soy, cider, teriyaki, spring, blender,\n",
      "Nearest to sauce: jack, wasabi, soy, appetizer, oregano,\n",
      "Nearest to cream: cherry, extract, chocolate, cake, puck,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: mixture, white, baguette, flake, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, pine, meat, ½, confectioner,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 160050 : 2.663787364959717\n",
      "Loss at step 160100 : 3.547879695892334\n",
      "Loss at step 160150 : 2.7401726245880127\n",
      "Loss at step 160200 : 2.4733245372772217\n",
      "Loss at step 160250 : 3.559558868408203\n",
      "Loss at step 160300 : 2.968082904815674\n",
      "Loss at step 160350 : 3.0800087451934814\n",
      "Loss at step 160400 : 2.247809648513794\n",
      "Loss at step 160450 : 2.492389440536499\n",
      "Loss at step 160500 : 3.4540634155273438\n",
      "Loss at step 160550 : 3.049909830093384\n",
      "Loss at step 160600 : 3.3648853302001953\n",
      "Loss at step 160650 : 2.973217010498047\n",
      "Loss at step 160700 : 3.5018489360809326\n",
      "Loss at step 160750 : 2.223534345626831\n",
      "Loss at step 160800 : 2.7246828079223633\n",
      "Loss at step 160850 : 2.627344846725464\n",
      "Loss at step 160900 : 2.604271173477173\n",
      "Loss at step 160950 : 2.077549934387207\n",
      "Loss at step 161000 : 2.2450361251831055\n",
      "Loss at step 161050 : 2.5970141887664795\n",
      "Loss at step 161100 : 2.284817934036255\n",
      "Loss at step 161150 : 2.468543529510498\n",
      "Loss at step 161200 : 3.9989113807678223\n",
      "Loss at step 161250 : 2.0119354724884033\n",
      "Loss at step 161300 : 3.291738510131836\n",
      "Loss at step 161350 : 3.19598388671875\n",
      "Loss at step 161400 : 2.862799882888794\n",
      "Loss at step 161450 : 2.953014373779297\n",
      "Loss at step 161500 : 2.4053163528442383\n",
      "Loss at step 161550 : 2.944336414337158\n",
      "Loss at step 161600 : 3.4585213661193848\n",
      "Loss at step 161650 : 3.0450892448425293\n",
      "Loss at step 161700 : 2.242847442626953\n",
      "Loss at step 161750 : 3.038661003112793\n",
      "Loss at step 161800 : 2.709871768951416\n",
      "Loss at step 161850 : 2.1828413009643555\n",
      "Loss at step 161900 : 2.945481777191162\n",
      "Loss at step 161950 : 3.292184829711914\n",
      "Loss at step 162000 : 2.9593424797058105\n",
      "Loss at step 162050 : 2.027897357940674\n",
      "Loss at step 162100 : 3.4822566509246826\n",
      "Loss at step 162150 : 2.3486924171447754\n",
      "Loss at step 162200 : 3.3739795684814453\n",
      "Loss at step 162250 : 3.0194921493530273\n",
      "Loss at step 162300 : 2.5901918411254883\n",
      "Loss at step 162350 : 3.0686206817626953\n",
      "Loss at step 162400 : 2.664647340774536\n",
      "Loss at step 162450 : 2.6202640533447266\n",
      "Loss at step 162500 : 2.622199535369873\n",
      "Loss at step 162550 : 2.9732000827789307\n",
      "Loss at step 162600 : 2.500884532928467\n",
      "Loss at step 162650 : 3.561323642730713\n",
      "Loss at step 162700 : 3.242382049560547\n",
      "Loss at step 162750 : 3.7081756591796875\n",
      "Loss at step 162800 : 2.9579994678497314\n",
      "Loss at step 162850 : 3.1847023963928223\n",
      "Loss at step 162900 : 2.347931385040283\n",
      "Loss at step 162950 : 2.686052083969116\n",
      "Loss at step 163000 : 2.436044216156006\n",
      "Loss at step 163050 : 3.1380865573883057\n",
      "Loss at step 163100 : 4.347151756286621\n",
      "Loss at step 163150 : 2.688312292098999\n",
      "Loss at step 163200 : 2.4940285682678223\n",
      "Loss at step 163250 : 2.7052316665649414\n",
      "Loss at step 163300 : 2.680377960205078\n",
      "Loss at step 163350 : 2.9477343559265137\n",
      "Loss at step 163400 : 3.0657284259796143\n",
      "Loss at step 163450 : 3.651733875274658\n",
      "Loss at step 163500 : 2.873748540878296\n",
      "Loss at step 163550 : 3.673316478729248\n",
      "Loss at step 163600 : 2.98382830619812\n",
      "Loss at step 163650 : 2.4447340965270996\n",
      "Loss at step 163700 : 3.0167107582092285\n",
      "Loss at step 163750 : 2.4525232315063477\n",
      "Loss at step 163800 : 3.1609015464782715\n",
      "Loss at step 163850 : 3.408885955810547\n",
      "Loss at step 163900 : 2.785698175430298\n",
      "Loss at step 163950 : 2.9246764183044434\n",
      "Loss at step 164000 : 2.95987606048584\n",
      "Loss at step 164050 : 3.0754387378692627\n",
      "Loss at step 164100 : 2.909085512161255\n",
      "Loss at step 164150 : 3.40492844581604\n",
      "Loss at step 164200 : 2.864987373352051\n",
      "Loss at step 164250 : 2.8514256477355957\n",
      "Loss at step 164300 : 3.5004358291625977\n",
      "Loss at step 164350 : 3.005687713623047\n",
      "Loss at step 164400 : 2.4360930919647217\n",
      "Loss at step 164450 : 2.7392637729644775\n",
      "Loss at step 164500 : 2.848292350769043\n",
      "Loss at step 164550 : 2.673553466796875\n",
      "Loss at step 164600 : 2.826176166534424\n",
      "Loss at step 164650 : 3.6493983268737793\n",
      "Loss at step 164700 : 3.1642329692840576\n",
      "Loss at step 164750 : 1.89445161819458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 164800 : 2.964665412902832\n",
      "Loss at step 164850 : 2.1993582248687744\n",
      "Loss at step 164900 : 3.6785531044006348\n",
      "Loss at step 164950 : 4.168036460876465\n",
      "Loss at step 165000 : 2.523951530456543\n",
      "Nearest to tuna: parmesan, metal, eye, position, sushi,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, sesame, sashimi, cut, rice,\n",
      "Nearest to roll: meal, sheet, oil, crumb, ricotta,\n",
      "Nearest to sashimi: ring, fryer, seaweed, gyoza, sushi,\n",
      "Nearest to steak: maple, meat, eye, grate, paste,\n",
      "Nearest to grill: cider, soy, teriyaki, spring, blender,\n",
      "Nearest to sauce: jack, wasabi, appetizer, soy, oregano,\n",
      "Nearest to cream: cherry, chocolate, extract, cake, skinless,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: mixture, white, baguette, flake, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: meat, herb, pine, ½, parsley,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 165050 : 3.1806607246398926\n",
      "Loss at step 165100 : 2.8080039024353027\n",
      "Loss at step 165150 : 3.9223012924194336\n",
      "Loss at step 165200 : 2.8694772720336914\n",
      "Loss at step 165250 : 2.730034589767456\n",
      "Loss at step 165300 : 3.4610490798950195\n",
      "Loss at step 165350 : 3.0991299152374268\n",
      "Loss at step 165400 : 2.3179128170013428\n",
      "Loss at step 165450 : 2.592849016189575\n",
      "Loss at step 165500 : 3.724058151245117\n",
      "Loss at step 165550 : 2.8093886375427246\n",
      "Loss at step 165600 : 3.3568661212921143\n",
      "Loss at step 165650 : 3.5478453636169434\n",
      "Loss at step 165700 : 2.3609585762023926\n",
      "Loss at step 165750 : 3.1131162643432617\n",
      "Loss at step 165800 : 2.2503914833068848\n",
      "Loss at step 165850 : 2.5049920082092285\n",
      "Loss at step 165900 : 2.983445167541504\n",
      "Loss at step 165950 : 2.3680014610290527\n",
      "Loss at step 166000 : 3.338010787963867\n",
      "Loss at step 166050 : 3.282299518585205\n",
      "Loss at step 166100 : 3.278393268585205\n",
      "Loss at step 166150 : 2.4868249893188477\n",
      "Loss at step 166200 : 2.9088339805603027\n",
      "Loss at step 166250 : 4.107239723205566\n",
      "Loss at step 166300 : 2.5300097465515137\n",
      "Loss at step 166350 : 2.082120418548584\n",
      "Loss at step 166400 : 2.6330628395080566\n",
      "Loss at step 166450 : 2.2823519706726074\n",
      "Loss at step 166500 : 3.21514892578125\n",
      "Loss at step 166550 : 2.5346155166625977\n",
      "Loss at step 166600 : 2.3246254920959473\n",
      "Loss at step 166650 : 3.2319788932800293\n",
      "Loss at step 166700 : 2.129244565963745\n",
      "Loss at step 166750 : 2.8284833431243896\n",
      "Loss at step 166800 : 2.763636827468872\n",
      "Loss at step 166850 : 2.351949453353882\n",
      "Loss at step 166900 : 2.987614870071411\n",
      "Loss at step 166950 : 3.1007652282714844\n",
      "Loss at step 167000 : 2.0455002784729004\n",
      "Loss at step 167050 : 2.7441444396972656\n",
      "Loss at step 167100 : 1.8022544384002686\n",
      "Loss at step 167150 : 3.4499120712280273\n",
      "Loss at step 167200 : 3.229936122894287\n",
      "Loss at step 167250 : 3.1995432376861572\n",
      "Loss at step 167300 : 3.281041383743286\n",
      "Loss at step 167350 : 3.0610885620117188\n",
      "Loss at step 167400 : 2.8780324459075928\n",
      "Loss at step 167450 : 2.9856553077697754\n",
      "Loss at step 167500 : 2.7460484504699707\n",
      "Loss at step 167550 : 2.3118174076080322\n",
      "Loss at step 167600 : 3.282553195953369\n",
      "Loss at step 167650 : 2.5070414543151855\n",
      "Loss at step 167700 : 2.9526925086975098\n",
      "Loss at step 167750 : 2.9157278537750244\n",
      "Loss at step 167800 : 3.736438751220703\n",
      "Loss at step 167850 : 2.8118374347686768\n",
      "Loss at step 167900 : 2.6595640182495117\n",
      "Loss at step 167950 : 2.316481113433838\n",
      "Loss at step 168000 : 3.000638961791992\n",
      "Loss at step 168050 : 3.2921929359436035\n",
      "Loss at step 168100 : 2.945164203643799\n",
      "Loss at step 168150 : 2.5428779125213623\n",
      "Loss at step 168200 : 3.297851800918579\n",
      "Loss at step 168250 : 2.6742420196533203\n",
      "Loss at step 168300 : 2.749513626098633\n",
      "Loss at step 168350 : 2.6626148223876953\n",
      "Loss at step 168400 : 2.962388515472412\n",
      "Loss at step 168450 : 2.509453296661377\n",
      "Loss at step 168500 : 3.335008144378662\n",
      "Loss at step 168550 : 2.638827323913574\n",
      "Loss at step 168600 : 4.375734329223633\n",
      "Loss at step 168650 : 3.625310182571411\n",
      "Loss at step 168700 : 2.395763874053955\n",
      "Loss at step 168750 : 3.531022071838379\n",
      "Loss at step 168800 : 3.180603504180908\n",
      "Loss at step 168850 : 2.8318867683410645\n",
      "Loss at step 168900 : 3.2256712913513184\n",
      "Loss at step 168950 : 2.8638916015625\n",
      "Loss at step 169000 : 3.4763031005859375\n",
      "Loss at step 169050 : 2.0089588165283203\n",
      "Loss at step 169100 : 2.6633379459381104\n",
      "Loss at step 169150 : 2.572084426879883\n",
      "Loss at step 169200 : 2.6524901390075684\n",
      "Loss at step 169250 : 4.2019429206848145\n",
      "Loss at step 169300 : 2.9053146839141846\n",
      "Loss at step 169350 : 2.7894399166107178\n",
      "Loss at step 169400 : 2.3964076042175293\n",
      "Loss at step 169450 : 1.9632763862609863\n",
      "Loss at step 169500 : 2.407285690307617\n",
      "Loss at step 169550 : 2.7385239601135254\n",
      "Loss at step 169600 : 2.68129563331604\n",
      "Loss at step 169650 : 3.9187090396881104\n",
      "Loss at step 169700 : 3.057863235473633\n",
      "Loss at step 169750 : 3.043745279312134\n",
      "Loss at step 169800 : 2.9796018600463867\n",
      "Loss at step 169850 : 3.0985589027404785\n",
      "Loss at step 169900 : 2.7871334552764893\n",
      "Loss at step 169950 : 1.9969358444213867\n",
      "Loss at step 170000 : 2.955803871154785\n",
      "Nearest to tuna: parmesan, metal, eye, position, avocado,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, sashimi, sesame, cut, rare,\n",
      "Nearest to roll: meal, sheet, crumb, ricotta, oil,\n",
      "Nearest to sashimi: fryer, ring, seaweed, gyoza, piece,\n",
      "Nearest to steak: maple, meat, eye, grate, marinade,\n",
      "Nearest to grill: cider, soy, teriyaki, spring, blender,\n",
      "Nearest to sauce: jack, wasabi, appetizer, oregano, soy,\n",
      "Nearest to cream: cherry, chocolate, extract, cake, mix,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: mixture, white, baguette, flake, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, ½, parsley,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 170050 : 2.6676025390625\n",
      "Loss at step 170100 : 3.5772757530212402\n",
      "Loss at step 170150 : 2.702909469604492\n",
      "Loss at step 170200 : 2.6265182495117188\n",
      "Loss at step 170250 : 3.195146083831787\n",
      "Loss at step 170300 : 2.7001874446868896\n",
      "Loss at step 170350 : 2.9407811164855957\n",
      "Loss at step 170400 : 2.7561821937561035\n",
      "Loss at step 170450 : 2.683615207672119\n",
      "Loss at step 170500 : 2.7109453678131104\n",
      "Loss at step 170550 : 3.2369203567504883\n",
      "Loss at step 170600 : 2.958041191101074\n",
      "Loss at step 170650 : 2.945899486541748\n",
      "Loss at step 170700 : 3.3875834941864014\n",
      "Loss at step 170750 : 2.9262940883636475\n",
      "Loss at step 170800 : 2.7739312648773193\n",
      "Loss at step 170850 : 2.3351945877075195\n",
      "Loss at step 170900 : 2.1195919513702393\n",
      "Loss at step 170950 : 2.618464708328247\n",
      "Loss at step 171000 : 2.7722830772399902\n",
      "Loss at step 171050 : 3.5950353145599365\n",
      "Loss at step 171100 : 2.683438301086426\n",
      "Loss at step 171150 : 2.839036226272583\n",
      "Loss at step 171200 : 3.22990083694458\n",
      "Loss at step 171250 : 2.5543346405029297\n",
      "Loss at step 171300 : 3.384535789489746\n",
      "Loss at step 171350 : 2.913332462310791\n",
      "Loss at step 171400 : 3.2936861515045166\n",
      "Loss at step 171450 : 2.6459479331970215\n",
      "Loss at step 171500 : 4.3552117347717285\n",
      "Loss at step 171550 : 2.9949045181274414\n",
      "Loss at step 171600 : 2.4095957279205322\n",
      "Loss at step 171650 : 2.810633420944214\n",
      "Loss at step 171700 : 3.733363628387451\n",
      "Loss at step 171750 : 3.4886467456817627\n",
      "Loss at step 171800 : 2.5569210052490234\n",
      "Loss at step 171850 : 2.888392925262451\n",
      "Loss at step 171900 : 2.6889336109161377\n",
      "Loss at step 171950 : 3.721627950668335\n",
      "Loss at step 172000 : 2.876429319381714\n",
      "Loss at step 172050 : 2.309803009033203\n",
      "Loss at step 172100 : 3.2064571380615234\n",
      "Loss at step 172150 : 3.088709831237793\n",
      "Loss at step 172200 : 3.357649087905884\n",
      "Loss at step 172250 : 3.4842114448547363\n",
      "Loss at step 172300 : 2.944821357727051\n",
      "Loss at step 172350 : 2.6381165981292725\n",
      "Loss at step 172400 : 2.9444010257720947\n",
      "Loss at step 172450 : 3.093820571899414\n",
      "Loss at step 172500 : 2.8358922004699707\n",
      "Loss at step 172550 : 2.9000539779663086\n",
      "Loss at step 172600 : 3.5934085845947266\n",
      "Loss at step 172650 : 3.209573984146118\n",
      "Loss at step 172700 : 3.299027681350708\n",
      "Loss at step 172750 : 2.618770122528076\n",
      "Loss at step 172800 : 2.711149215698242\n",
      "Loss at step 172850 : 2.783241033554077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 172900 : 3.27593731880188\n",
      "Loss at step 172950 : 2.137934446334839\n",
      "Loss at step 173000 : 2.7597925662994385\n",
      "Loss at step 173050 : 3.509080648422241\n",
      "Loss at step 173100 : 1.6958684921264648\n",
      "Loss at step 173150 : 3.026787757873535\n",
      "Loss at step 173200 : 2.1360342502593994\n",
      "Loss at step 173250 : 2.6058788299560547\n",
      "Loss at step 173300 : 3.0501904487609863\n",
      "Loss at step 173350 : 1.916808009147644\n",
      "Loss at step 173400 : 2.8586344718933105\n",
      "Loss at step 173450 : 2.7632651329040527\n",
      "Loss at step 173500 : 3.281456232070923\n",
      "Loss at step 173550 : 2.6437649726867676\n",
      "Loss at step 173600 : 3.0593338012695312\n",
      "Loss at step 173650 : 3.211446762084961\n",
      "Loss at step 173700 : 3.139158248901367\n",
      "Loss at step 173750 : 2.772779941558838\n",
      "Loss at step 173800 : 3.262510299682617\n",
      "Loss at step 173850 : 2.234438419342041\n",
      "Loss at step 173900 : 3.856823205947876\n",
      "Loss at step 173950 : 3.57065486907959\n",
      "Loss at step 174000 : 2.5673813819885254\n",
      "Loss at step 174050 : 3.2197928428649902\n",
      "Loss at step 174100 : 2.848834991455078\n",
      "Loss at step 174150 : 2.3694722652435303\n",
      "Loss at step 174200 : 2.628176689147949\n",
      "Loss at step 174250 : 2.7618160247802734\n",
      "Loss at step 174300 : 3.1878223419189453\n",
      "Loss at step 174350 : 2.287182331085205\n",
      "Loss at step 174400 : 2.323486328125\n",
      "Loss at step 174450 : 2.8593251705169678\n",
      "Loss at step 174500 : 2.8842921257019043\n",
      "Loss at step 174550 : 2.7517242431640625\n",
      "Loss at step 174600 : 3.0158848762512207\n",
      "Loss at step 174650 : 2.897277355194092\n",
      "Loss at step 174700 : 2.6590075492858887\n",
      "Loss at step 174750 : 3.0390772819519043\n",
      "Loss at step 174800 : 1.8993384838104248\n",
      "Loss at step 174850 : 2.5963547229766846\n",
      "Loss at step 174900 : 3.5282580852508545\n",
      "Loss at step 174950 : 1.9576791524887085\n",
      "Loss at step 175000 : 2.056339740753174\n",
      "Nearest to tuna: parmesan, metal, eye, avocado, position,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, rice,\n",
      "Nearest to roll: meal, sheet, ricotta, crumb, oil,\n",
      "Nearest to sashimi: fryer, ring, seaweed, gyoza, sushi,\n",
      "Nearest to steak: maple, eye, meat, grate, marinade,\n",
      "Nearest to grill: soy, cider, spring, teriyaki, steak,\n",
      "Nearest to sauce: jack, appetizer, wasabi, oregano, mat,\n",
      "Nearest to cream: cherry, chocolate, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, paper, tin,\n",
      "Nearest to pizza: mixture, white, baguette, run, flake,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, facing,\n",
      "Nearest to hamburger: herb, pine, meat, ½, tomato,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 175050 : 1.9685853719711304\n",
      "Loss at step 175100 : 3.6926181316375732\n",
      "Loss at step 175150 : 2.944255828857422\n",
      "Loss at step 175200 : 2.362851619720459\n",
      "Loss at step 175250 : 2.3943891525268555\n",
      "Loss at step 175300 : 3.173095941543579\n",
      "Loss at step 175350 : 2.548727512359619\n",
      "Loss at step 175400 : 3.0406179428100586\n",
      "Loss at step 175450 : 2.655625820159912\n",
      "Loss at step 175500 : 3.2250304222106934\n",
      "Loss at step 175550 : 2.94399356842041\n",
      "Loss at step 175600 : 2.3980507850646973\n",
      "Loss at step 175650 : 2.62681245803833\n",
      "Loss at step 175700 : 2.403332233428955\n",
      "Loss at step 175750 : 2.632765769958496\n",
      "Loss at step 175800 : 3.0481319427490234\n",
      "Loss at step 175850 : 3.2232511043548584\n",
      "Loss at step 175900 : 2.65822696685791\n",
      "Loss at step 175950 : 2.060636520385742\n",
      "Loss at step 176000 : 3.7498672008514404\n",
      "Loss at step 176050 : 2.5172841548919678\n",
      "Loss at step 176100 : 2.6275198459625244\n",
      "Loss at step 176150 : 3.0141451358795166\n",
      "Loss at step 176200 : 3.3379170894622803\n",
      "Loss at step 176250 : 2.9386706352233887\n",
      "Loss at step 176300 : 2.912193775177002\n",
      "Loss at step 176350 : 3.114856481552124\n",
      "Loss at step 176400 : 3.754303216934204\n",
      "Loss at step 176450 : 3.3884053230285645\n",
      "Loss at step 176500 : 3.5766825675964355\n",
      "Loss at step 176550 : 2.5995185375213623\n",
      "Loss at step 176600 : 2.740957736968994\n",
      "Loss at step 176650 : 2.8365705013275146\n",
      "Loss at step 176700 : 2.613741874694824\n",
      "Loss at step 176750 : 3.310396671295166\n",
      "Loss at step 176800 : 2.848054885864258\n",
      "Loss at step 176850 : 2.1419196128845215\n",
      "Loss at step 176900 : 2.74277925491333\n",
      "Loss at step 176950 : 1.8742239475250244\n",
      "Loss at step 177000 : 4.284789562225342\n",
      "Loss at step 177050 : 2.5695719718933105\n",
      "Loss at step 177100 : 2.984621047973633\n",
      "Loss at step 177150 : 2.6484375\n",
      "Loss at step 177200 : 2.680479049682617\n",
      "Loss at step 177250 : 3.0972423553466797\n",
      "Loss at step 177300 : 3.2290453910827637\n",
      "Loss at step 177350 : 2.9520440101623535\n",
      "Loss at step 177400 : 2.7498414516448975\n",
      "Loss at step 177450 : 2.444876194000244\n",
      "Loss at step 177500 : 3.1077537536621094\n",
      "Loss at step 177550 : 2.4181599617004395\n",
      "Loss at step 177600 : 2.805225133895874\n",
      "Loss at step 177650 : 2.8760910034179688\n",
      "Loss at step 177700 : 3.059413433074951\n",
      "Loss at step 177750 : 3.517927646636963\n",
      "Loss at step 177800 : 2.3775997161865234\n",
      "Loss at step 177850 : 2.9848132133483887\n",
      "Loss at step 177900 : 3.694251537322998\n",
      "Loss at step 177950 : 2.6813855171203613\n",
      "Loss at step 178000 : 3.221496820449829\n",
      "Loss at step 178050 : 3.7484099864959717\n",
      "Loss at step 178100 : 3.1800341606140137\n",
      "Loss at step 178150 : 1.6834900379180908\n",
      "Loss at step 178200 : 3.6374330520629883\n",
      "Loss at step 178250 : 3.1746487617492676\n",
      "Loss at step 178300 : 2.765087842941284\n",
      "Loss at step 178350 : 2.6352484226226807\n",
      "Loss at step 178400 : 2.9927167892456055\n",
      "Loss at step 178450 : 2.659698724746704\n",
      "Loss at step 178500 : 2.73136043548584\n",
      "Loss at step 178550 : 3.3511884212493896\n",
      "Loss at step 178600 : 3.480837821960449\n",
      "Loss at step 178650 : 3.1084136962890625\n",
      "Loss at step 178700 : 2.3277759552001953\n",
      "Loss at step 178750 : 3.2356491088867188\n",
      "Loss at step 178800 : 2.617551326751709\n",
      "Loss at step 178850 : 3.1195149421691895\n",
      "Loss at step 178900 : 1.7242037057876587\n",
      "Loss at step 178950 : 2.944983959197998\n",
      "Loss at step 179000 : 2.4954428672790527\n",
      "Loss at step 179050 : 3.188096523284912\n",
      "Loss at step 179100 : 3.3478424549102783\n",
      "Loss at step 179150 : 2.905519485473633\n",
      "Loss at step 179200 : 3.1526598930358887\n",
      "Loss at step 179250 : 2.4045467376708984\n",
      "Loss at step 179300 : 2.505812168121338\n",
      "Loss at step 179350 : 2.069382667541504\n",
      "Loss at step 179400 : 3.352886199951172\n",
      "Loss at step 179450 : 2.885324478149414\n",
      "Loss at step 179500 : 3.7369446754455566\n",
      "Loss at step 179550 : 3.130096673965454\n",
      "Loss at step 179600 : 2.097719430923462\n",
      "Loss at step 179650 : 2.5950400829315186\n",
      "Loss at step 179700 : 2.8419899940490723\n",
      "Loss at step 179750 : 2.502387523651123\n",
      "Loss at step 179800 : 2.5389623641967773\n",
      "Loss at step 179850 : 2.9560353755950928\n",
      "Loss at step 179900 : 3.2682409286499023\n",
      "Loss at step 179950 : 2.846808910369873\n",
      "Loss at step 180000 : 3.233840227127075\n",
      "Nearest to tuna: parmesan, metal, eye, avocado, cheddar,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, sesame, sashimi, rice, rare,\n",
      "Nearest to roll: meal, sheet, ricotta, heart, oil,\n",
      "Nearest to sashimi: fryer, ring, seaweed, gyoza, sushi,\n",
      "Nearest to steak: maple, eye, grate, meat, marinade,\n",
      "Nearest to grill: soy, cider, spring, steak, teriyaki,\n",
      "Nearest to sauce: jack, appetizer, oregano, wasabi, soy,\n",
      "Nearest to cream: cherry, extract, chocolate, mix, cake,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, paper,\n",
      "Nearest to pizza: white, mixture, baguette, flake, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, pine, meat, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 180050 : 2.6278445720672607\n",
      "Loss at step 180100 : 2.5948827266693115\n",
      "Loss at step 180150 : 2.7097415924072266\n",
      "Loss at step 180200 : 2.7066450119018555\n",
      "Loss at step 180250 : 2.828711748123169\n",
      "Loss at step 180300 : 2.3393783569335938\n",
      "Loss at step 180350 : 2.5156772136688232\n",
      "Loss at step 180400 : 2.5581603050231934\n",
      "Loss at step 180450 : 3.2102224826812744\n",
      "Loss at step 180500 : 2.9680542945861816\n",
      "Loss at step 180550 : 2.8230819702148438\n",
      "Loss at step 180600 : 2.251580238342285\n",
      "Loss at step 180650 : 2.908766269683838\n",
      "Loss at step 180700 : 2.487870216369629\n",
      "Loss at step 180750 : 3.904867649078369\n",
      "Loss at step 180800 : 4.2719831466674805\n",
      "Loss at step 180850 : 2.3335554599761963\n",
      "Loss at step 180900 : 2.731875419616699\n",
      "Loss at step 180950 : 2.585519790649414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 181000 : 2.267538547515869\n",
      "Loss at step 181050 : 2.5891575813293457\n",
      "Loss at step 181100 : 4.177131652832031\n",
      "Loss at step 181150 : 3.3601558208465576\n",
      "Loss at step 181200 : 2.3578286170959473\n",
      "Loss at step 181250 : 3.3736257553100586\n",
      "Loss at step 181300 : 2.7141518592834473\n",
      "Loss at step 181350 : 2.4655075073242188\n",
      "Loss at step 181400 : 3.7307815551757812\n",
      "Loss at step 181450 : 3.2374203205108643\n",
      "Loss at step 181500 : 2.1318483352661133\n",
      "Loss at step 181550 : 4.727985858917236\n",
      "Loss at step 181600 : 3.8297200202941895\n",
      "Loss at step 181650 : 2.935009002685547\n",
      "Loss at step 181700 : 2.915163040161133\n",
      "Loss at step 181750 : 2.756990671157837\n",
      "Loss at step 181800 : 2.3727216720581055\n",
      "Loss at step 181850 : 2.5859179496765137\n",
      "Loss at step 181900 : 2.790215015411377\n",
      "Loss at step 181950 : 2.4818148612976074\n",
      "Loss at step 182000 : 2.9674699306488037\n",
      "Loss at step 182050 : 2.927351951599121\n",
      "Loss at step 182100 : 3.054628849029541\n",
      "Loss at step 182150 : 2.975280523300171\n",
      "Loss at step 182200 : 2.71260142326355\n",
      "Loss at step 182250 : 3.768345832824707\n",
      "Loss at step 182300 : 3.5810389518737793\n",
      "Loss at step 182350 : 2.0817482471466064\n",
      "Loss at step 182400 : 2.7976458072662354\n",
      "Loss at step 182450 : 2.6170055866241455\n",
      "Loss at step 182500 : 3.7782657146453857\n",
      "Loss at step 182550 : 2.6524572372436523\n",
      "Loss at step 182600 : 3.0394604206085205\n",
      "Loss at step 182650 : 3.405975341796875\n",
      "Loss at step 182700 : 3.5356545448303223\n",
      "Loss at step 182750 : 3.304569959640503\n",
      "Loss at step 182800 : 1.4692332744598389\n",
      "Loss at step 182850 : 3.3281702995300293\n",
      "Loss at step 182900 : 2.0294394493103027\n",
      "Loss at step 182950 : 2.9768667221069336\n",
      "Loss at step 183000 : 3.426980972290039\n",
      "Loss at step 183050 : 3.3582098484039307\n",
      "Loss at step 183100 : 2.7958927154541016\n",
      "Loss at step 183150 : 2.5919649600982666\n",
      "Loss at step 183200 : 4.194530010223389\n",
      "Loss at step 183250 : 3.090660572052002\n",
      "Loss at step 183300 : 2.620326042175293\n",
      "Loss at step 183350 : 3.0543835163116455\n",
      "Loss at step 183400 : 2.668137550354004\n",
      "Loss at step 183450 : 2.6689553260803223\n",
      "Loss at step 183500 : 2.949160575866699\n",
      "Loss at step 183550 : 3.700162887573242\n",
      "Loss at step 183600 : 3.222675085067749\n",
      "Loss at step 183650 : 3.51448917388916\n",
      "Loss at step 183700 : 2.6548218727111816\n",
      "Loss at step 183750 : 2.8727707862854004\n",
      "Loss at step 183800 : 3.186282157897949\n",
      "Loss at step 183850 : 2.570927619934082\n",
      "Loss at step 183900 : 3.048847198486328\n",
      "Loss at step 183950 : 1.3968125581741333\n",
      "Loss at step 184000 : 2.506561756134033\n",
      "Loss at step 184050 : 2.6465342044830322\n",
      "Loss at step 184100 : 3.613473415374756\n",
      "Loss at step 184150 : 2.347805976867676\n",
      "Loss at step 184200 : 2.693108320236206\n",
      "Loss at step 184250 : 2.450655937194824\n",
      "Loss at step 184300 : 3.439500093460083\n",
      "Loss at step 184350 : 2.3349366188049316\n",
      "Loss at step 184400 : 2.5870349407196045\n",
      "Loss at step 184450 : 2.870497703552246\n",
      "Loss at step 184500 : 2.521730422973633\n",
      "Loss at step 184550 : 2.6430201530456543\n",
      "Loss at step 184600 : 2.44706392288208\n",
      "Loss at step 184650 : 3.4064269065856934\n",
      "Loss at step 184700 : 1.5716357231140137\n",
      "Loss at step 184750 : 3.159677028656006\n",
      "Loss at step 184800 : 2.2016043663024902\n",
      "Loss at step 184850 : 3.8654470443725586\n",
      "Loss at step 184900 : 2.75480580329895\n",
      "Loss at step 184950 : 3.696859121322632\n",
      "Loss at step 185000 : 3.4855575561523438\n",
      "Nearest to tuna: parmesan, metal, eye, position, avocado,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sesame, rice, sashimi, rare,\n",
      "Nearest to roll: meal, sheet, ricotta, crumb, oil,\n",
      "Nearest to sashimi: fryer, seaweed, gyoza, ring, sushi,\n",
      "Nearest to steak: maple, eye, grate, meat, marinade,\n",
      "Nearest to grill: soy, cider, teriyaki, spring, steak,\n",
      "Nearest to sauce: jack, appetizer, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, mix, extract, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: baguette, mixture, white, flake, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, pine, meat, ½, tomato,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 185050 : 3.2470221519470215\n",
      "Loss at step 185100 : 2.7347145080566406\n",
      "Loss at step 185150 : 2.7310032844543457\n",
      "Loss at step 185200 : 2.5420174598693848\n",
      "Loss at step 185250 : 2.2403476238250732\n",
      "Loss at step 185300 : 3.0467915534973145\n",
      "Loss at step 185350 : 3.168549060821533\n",
      "Loss at step 185400 : 3.4810304641723633\n",
      "Loss at step 185450 : 2.199240207672119\n",
      "Loss at step 185500 : 3.108494520187378\n",
      "Loss at step 185550 : 2.659726619720459\n",
      "Loss at step 185600 : 3.2181556224823\n",
      "Loss at step 185650 : 2.178856372833252\n",
      "Loss at step 185700 : 3.4150686264038086\n",
      "Loss at step 185750 : 2.956132173538208\n",
      "Loss at step 185800 : 2.933504581451416\n",
      "Loss at step 185850 : 2.444382667541504\n",
      "Loss at step 185900 : 2.522303581237793\n",
      "Loss at step 185950 : 2.580106735229492\n",
      "Loss at step 186000 : 3.4496593475341797\n",
      "Loss at step 186050 : 3.75762677192688\n",
      "Loss at step 186100 : 2.7185299396514893\n",
      "Loss at step 186150 : 3.1451306343078613\n",
      "Loss at step 186200 : 2.7294490337371826\n",
      "Loss at step 186250 : 2.950228452682495\n",
      "Loss at step 186300 : 3.4037723541259766\n",
      "Loss at step 186350 : 3.0293078422546387\n",
      "Loss at step 186400 : 3.131791591644287\n",
      "Loss at step 186450 : 2.2816317081451416\n",
      "Loss at step 186500 : 3.056199550628662\n",
      "Loss at step 186550 : 3.792440176010132\n",
      "Loss at step 186600 : 2.879103183746338\n",
      "Loss at step 186650 : 3.1225154399871826\n",
      "Loss at step 186700 : 2.6766343116760254\n",
      "Loss at step 186750 : 3.43990159034729\n",
      "Loss at step 186800 : 2.675793170928955\n",
      "Loss at step 186850 : 2.644608497619629\n",
      "Loss at step 186900 : 3.1180200576782227\n",
      "Loss at step 186950 : 2.961003065109253\n",
      "Loss at step 187000 : 3.201829433441162\n",
      "Loss at step 187050 : 2.89022159576416\n",
      "Loss at step 187100 : 3.7117977142333984\n",
      "Loss at step 187150 : 2.826190710067749\n",
      "Loss at step 187200 : 2.869148015975952\n",
      "Loss at step 187250 : 2.724325180053711\n",
      "Loss at step 187300 : 1.8458701372146606\n",
      "Loss at step 187350 : 3.16264009475708\n",
      "Loss at step 187400 : 2.72879958152771\n",
      "Loss at step 187450 : 3.546940565109253\n",
      "Loss at step 187500 : 3.08789324760437\n",
      "Loss at step 187550 : 2.0650808811187744\n",
      "Loss at step 187600 : 2.1031553745269775\n",
      "Loss at step 187650 : 2.712681770324707\n",
      "Loss at step 187700 : 3.1333320140838623\n",
      "Loss at step 187750 : 2.6843080520629883\n",
      "Loss at step 187800 : 2.868906021118164\n",
      "Loss at step 187850 : 3.864180088043213\n",
      "Loss at step 187900 : 3.1437907218933105\n",
      "Loss at step 187950 : 3.10956072807312\n",
      "Loss at step 188000 : 2.897205114364624\n",
      "Loss at step 188050 : 3.231046676635742\n",
      "Loss at step 188100 : 3.1677231788635254\n",
      "Loss at step 188150 : 2.6349382400512695\n",
      "Loss at step 188200 : 3.0936543941497803\n",
      "Loss at step 188250 : 3.2981345653533936\n",
      "Loss at step 188300 : 2.88944935798645\n",
      "Loss at step 188350 : 3.7603611946105957\n",
      "Loss at step 188400 : 2.3223471641540527\n",
      "Loss at step 188450 : 3.2706387042999268\n",
      "Loss at step 188500 : 3.161116600036621\n",
      "Loss at step 188550 : 2.2959980964660645\n",
      "Loss at step 188600 : 2.578923463821411\n",
      "Loss at step 188650 : 2.7477612495422363\n",
      "Loss at step 188700 : 2.54004168510437\n",
      "Loss at step 188750 : 3.222576856613159\n",
      "Loss at step 188800 : 2.881849765777588\n",
      "Loss at step 188850 : 2.8319830894470215\n",
      "Loss at step 188900 : 3.867032527923584\n",
      "Loss at step 188950 : 2.745800733566284\n",
      "Loss at step 189000 : 2.3077073097229004\n",
      "Loss at step 189050 : 3.1097233295440674\n",
      "Loss at step 189100 : 2.7553110122680664\n",
      "Loss at step 189150 : 3.094564914703369\n",
      "Loss at step 189200 : 2.834357261657715\n",
      "Loss at step 189250 : 2.940380334854126\n",
      "Loss at step 189300 : 3.3549671173095703\n",
      "Loss at step 189350 : 2.8053698539733887\n",
      "Loss at step 189400 : 3.081758975982666\n",
      "Loss at step 189450 : 3.8751721382141113\n",
      "Loss at step 189500 : 2.8895504474639893\n",
      "Loss at step 189550 : 2.4981610774993896\n",
      "Loss at step 189600 : 2.4300379753112793\n",
      "Loss at step 189650 : 2.6712684631347656\n",
      "Loss at step 189700 : 4.4626874923706055\n",
      "Loss at step 189750 : 4.206940174102783\n",
      "Loss at step 189800 : 3.3430936336517334\n",
      "Loss at step 189850 : 3.144138813018799\n",
      "Loss at step 189900 : 3.0581064224243164\n",
      "Loss at step 189950 : 2.490957736968994\n",
      "Loss at step 190000 : 2.3794314861297607\n",
      "Nearest to tuna: parmesan, metal, eye, position, cheddar,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, rice, sesame, cornstarch,\n",
      "Nearest to roll: meal, sheet, salt, ricotta, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, gyoza, ring,\n",
      "Nearest to steak: maple, eye, grate, meat, marinade,\n",
      "Nearest to grill: soy, teriyaki, cider, spring, steak,\n",
      "Nearest to sauce: jack, appetizer, wasabi, soy, oregano,\n",
      "Nearest to cream: chocolate, mix, cherry, extract, cake,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: white, flake, mixture, baguette, run,\n",
      "Nearest to lasagna: fry, rare, noodle, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 190050 : 3.0777182579040527\n",
      "Loss at step 190100 : 2.855478525161743\n",
      "Loss at step 190150 : 2.8233797550201416\n",
      "Loss at step 190200 : 2.8940751552581787\n",
      "Loss at step 190250 : 3.4715194702148438\n",
      "Loss at step 190300 : 2.732646942138672\n",
      "Loss at step 190350 : 2.8079168796539307\n",
      "Loss at step 190400 : 3.376744508743286\n",
      "Loss at step 190450 : 3.5214130878448486\n",
      "Loss at step 190500 : 3.39988374710083\n",
      "Loss at step 190550 : 2.928256034851074\n",
      "Loss at step 190600 : 1.8062334060668945\n",
      "Loss at step 190650 : 2.5178356170654297\n",
      "Loss at step 190700 : 3.526543617248535\n",
      "Loss at step 190750 : 2.54960298538208\n",
      "Loss at step 190800 : 3.069645643234253\n",
      "Loss at step 190850 : 3.3365318775177\n",
      "Loss at step 190900 : 3.4135093688964844\n",
      "Loss at step 190950 : 2.588082790374756\n",
      "Loss at step 191000 : 3.0972256660461426\n",
      "Loss at step 191050 : 2.169795036315918\n",
      "Loss at step 191100 : 2.2765610218048096\n",
      "Loss at step 191150 : 3.243605375289917\n",
      "Loss at step 191200 : 2.745587110519409\n",
      "Loss at step 191250 : 2.7413201332092285\n",
      "Loss at step 191300 : 3.778616189956665\n",
      "Loss at step 191350 : 3.5878796577453613\n",
      "Loss at step 191400 : 2.557396650314331\n",
      "Loss at step 191450 : 2.7134008407592773\n",
      "Loss at step 191500 : 2.833444118499756\n",
      "Loss at step 191550 : 3.4486732482910156\n",
      "Loss at step 191600 : 3.1042861938476562\n",
      "Loss at step 191650 : 2.367485761642456\n",
      "Loss at step 191700 : 3.2261557579040527\n",
      "Loss at step 191750 : 3.6635570526123047\n",
      "Loss at step 191800 : 3.299240827560425\n",
      "Loss at step 191850 : 2.6572132110595703\n",
      "Loss at step 191900 : 3.3236961364746094\n",
      "Loss at step 191950 : 3.4232068061828613\n",
      "Loss at step 192000 : 2.6510350704193115\n",
      "Loss at step 192050 : 2.9918723106384277\n",
      "Loss at step 192100 : 3.0353598594665527\n",
      "Loss at step 192150 : 2.949145793914795\n",
      "Loss at step 192200 : 2.890197992324829\n",
      "Loss at step 192250 : 2.842639684677124\n",
      "Loss at step 192300 : 2.5112786293029785\n",
      "Loss at step 192350 : 3.0057225227355957\n",
      "Loss at step 192400 : 2.072171211242676\n",
      "Loss at step 192450 : 2.9860024452209473\n",
      "Loss at step 192500 : 2.8498337268829346\n",
      "Loss at step 192550 : 3.3156704902648926\n",
      "Loss at step 192600 : 2.855196952819824\n",
      "Loss at step 192650 : 3.5923867225646973\n",
      "Loss at step 192700 : 4.402246475219727\n",
      "Loss at step 192750 : 3.0640645027160645\n",
      "Loss at step 192800 : 2.9306399822235107\n",
      "Loss at step 192850 : 3.066445827484131\n",
      "Loss at step 192900 : 3.016756296157837\n",
      "Loss at step 192950 : 2.873788833618164\n",
      "Loss at step 193000 : 1.7934587001800537\n",
      "Loss at step 193050 : 2.865140914916992\n",
      "Loss at step 193100 : 2.0673000812530518\n",
      "Loss at step 193150 : 2.3913111686706543\n",
      "Loss at step 193200 : 2.6559112071990967\n",
      "Loss at step 193250 : 3.1801681518554688\n",
      "Loss at step 193300 : 3.331045627593994\n",
      "Loss at step 193350 : 2.907214879989624\n",
      "Loss at step 193400 : 2.7464969158172607\n",
      "Loss at step 193450 : 1.7703750133514404\n",
      "Loss at step 193500 : 3.257944345474243\n",
      "Loss at step 193550 : 3.2159714698791504\n",
      "Loss at step 193600 : 3.0837368965148926\n",
      "Loss at step 193650 : 2.7544000148773193\n",
      "Loss at step 193700 : 2.763335704803467\n",
      "Loss at step 193750 : 4.028648376464844\n",
      "Loss at step 193800 : 3.004152774810791\n",
      "Loss at step 193850 : 3.3710086345672607\n",
      "Loss at step 193900 : 3.7591183185577393\n",
      "Loss at step 193950 : 2.674812078475952\n",
      "Loss at step 194000 : 2.025939464569092\n",
      "Loss at step 194050 : 3.218222141265869\n",
      "Loss at step 194100 : 2.723661184310913\n",
      "Loss at step 194150 : 2.0789804458618164\n",
      "Loss at step 194200 : 3.1821136474609375\n",
      "Loss at step 194250 : 3.2351365089416504\n",
      "Loss at step 194300 : 3.264467716217041\n",
      "Loss at step 194350 : 2.941607713699341\n",
      "Loss at step 194400 : 2.2276968955993652\n",
      "Loss at step 194450 : 3.1312389373779297\n",
      "Loss at step 194500 : 2.770327091217041\n",
      "Loss at step 194550 : 2.8979034423828125\n",
      "Loss at step 194600 : 3.2521564960479736\n",
      "Loss at step 194650 : 2.5645639896392822\n",
      "Loss at step 194700 : 2.5993847846984863\n",
      "Loss at step 194750 : 2.717776298522949\n",
      "Loss at step 194800 : 3.3025031089782715\n",
      "Loss at step 194850 : 3.1043477058410645\n",
      "Loss at step 194900 : 3.6336262226104736\n",
      "Loss at step 194950 : 3.1519341468811035\n",
      "Loss at step 195000 : 2.842905282974243\n",
      "Nearest to tuna: parmesan, metal, eye, mat, position,\n",
      "Nearest to rice: thigh, lime, mandu, spinach, simmer,\n",
      "Nearest to sushi: piece, sashimi, sesame, bundt, rice,\n",
      "Nearest to roll: meal, sheet, crumb, ricotta, heart,\n",
      "Nearest to sashimi: fryer, seaweed, ring, sushi, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, teriyaki, cider, spring, steak,\n",
      "Nearest to sauce: jack, appetizer, wasabi, soy, oregano,\n",
      "Nearest to cream: chocolate, cherry, extract, mix, cake,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, mixture, white, baguette, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 195050 : 2.877156972885132\n",
      "Loss at step 195100 : 2.152878522872925\n",
      "Loss at step 195150 : 2.9100677967071533\n",
      "Loss at step 195200 : 3.4294896125793457\n",
      "Loss at step 195250 : 3.001864433288574\n",
      "Loss at step 195300 : 3.502586841583252\n",
      "Loss at step 195350 : 3.0804967880249023\n",
      "Loss at step 195400 : 3.586599588394165\n",
      "Loss at step 195450 : 2.5165536403656006\n",
      "Loss at step 195500 : 2.9038290977478027\n",
      "Loss at step 195550 : 3.223616600036621\n",
      "Loss at step 195600 : 3.477950096130371\n",
      "Loss at step 195650 : 3.677535057067871\n",
      "Loss at step 195700 : 2.6778275966644287\n",
      "Loss at step 195750 : 2.451697587966919\n",
      "Loss at step 195800 : 2.4296388626098633\n",
      "Loss at step 195850 : 4.468172073364258\n",
      "Loss at step 195900 : 2.9147186279296875\n",
      "Loss at step 195950 : 2.944817543029785\n",
      "Loss at step 196000 : 2.881887435913086\n",
      "Loss at step 196050 : 3.059333562850952\n",
      "Loss at step 196100 : 3.5092203617095947\n",
      "Loss at step 196150 : 3.094137668609619\n",
      "Loss at step 196200 : 3.285355806350708\n",
      "Loss at step 196250 : 3.0382676124572754\n",
      "Loss at step 196300 : 3.541339635848999\n",
      "Loss at step 196350 : 2.897742748260498\n",
      "Loss at step 196400 : 3.6488842964172363\n",
      "Loss at step 196450 : 2.9728026390075684\n",
      "Loss at step 196500 : 2.914119243621826\n",
      "Loss at step 196550 : 2.8137927055358887\n",
      "Loss at step 196600 : 2.7758898735046387\n",
      "Loss at step 196650 : 2.473172426223755\n",
      "Loss at step 196700 : 2.6323938369750977\n",
      "Loss at step 196750 : 2.455679416656494\n",
      "Loss at step 196800 : 2.417248487472534\n",
      "Loss at step 196850 : 2.4665184020996094\n",
      "Loss at step 196900 : 2.9114441871643066\n",
      "Loss at step 196950 : 2.791292905807495\n",
      "Loss at step 197000 : 4.5465617179870605\n",
      "Loss at step 197050 : 3.420133590698242\n",
      "Loss at step 197100 : 2.379584312438965\n",
      "Loss at step 197150 : 2.3384313583374023\n",
      "Loss at step 197200 : 3.030808687210083\n",
      "Loss at step 197250 : 2.1458077430725098\n",
      "Loss at step 197300 : 2.8112878799438477\n",
      "Loss at step 197350 : 1.8015167713165283\n",
      "Loss at step 197400 : 2.946950912475586\n",
      "Loss at step 197450 : 2.2991371154785156\n",
      "Loss at step 197500 : 2.215381145477295\n",
      "Loss at step 197550 : 2.339505195617676\n",
      "Loss at step 197600 : 2.8369693756103516\n",
      "Loss at step 197650 : 3.3496804237365723\n",
      "Loss at step 197700 : 2.647244691848755\n",
      "Loss at step 197750 : 2.8465747833251953\n",
      "Loss at step 197800 : 2.714951992034912\n",
      "Loss at step 197850 : 1.9059431552886963\n",
      "Loss at step 197900 : 3.180781841278076\n",
      "Loss at step 197950 : 2.984292507171631\n",
      "Loss at step 198000 : 2.5198302268981934\n",
      "Loss at step 198050 : 2.8658127784729004\n",
      "Loss at step 198100 : 2.9469728469848633\n",
      "Loss at step 198150 : 2.211719036102295\n",
      "Loss at step 198200 : 2.7069931030273438\n",
      "Loss at step 198250 : 3.97623872756958\n",
      "Loss at step 198300 : 2.708134651184082\n",
      "Loss at step 198350 : 3.358670473098755\n",
      "Loss at step 198400 : 1.9737002849578857\n",
      "Loss at step 198450 : 2.2422337532043457\n",
      "Loss at step 198500 : 3.444632053375244\n",
      "Loss at step 198550 : 3.0396547317504883\n",
      "Loss at step 198600 : 2.7615723609924316\n",
      "Loss at step 198650 : 2.7126944065093994\n",
      "Loss at step 198700 : 3.247556686401367\n",
      "Loss at step 198750 : 2.7027111053466797\n",
      "Loss at step 198800 : 2.1165857315063477\n",
      "Loss at step 198850 : 2.8163514137268066\n",
      "Loss at step 198900 : 3.944127321243286\n",
      "Loss at step 198950 : 1.8203215599060059\n",
      "Loss at step 199000 : 2.8122756481170654\n",
      "Loss at step 199050 : 3.4576563835144043\n",
      "Loss at step 199100 : 2.8990602493286133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 199150 : 3.0190587043762207\n",
      "Loss at step 199200 : 3.462486982345581\n",
      "Loss at step 199250 : 3.077979326248169\n",
      "Loss at step 199300 : 3.108485698699951\n",
      "Loss at step 199350 : 2.6826486587524414\n",
      "Loss at step 199400 : 4.088657855987549\n",
      "Loss at step 199450 : 2.8000659942626953\n",
      "Loss at step 199500 : 3.6551871299743652\n",
      "Loss at step 199550 : 3.023803234100342\n",
      "Loss at step 199600 : 2.169670581817627\n",
      "Loss at step 199650 : 2.759066104888916\n",
      "Loss at step 199700 : 2.30612850189209\n",
      "Loss at step 199750 : 2.341580629348755\n",
      "Loss at step 199800 : 3.1883344650268555\n",
      "Loss at step 199850 : 2.194319009780884\n",
      "Loss at step 199900 : 3.228804111480713\n",
      "Loss at step 199950 : 2.555026054382324\n",
      "Loss at step 200000 : 3.11266827583313\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name=\"word_embeddings\")\n",
    "doc_embeddings = tf.Variable(tf.random_uniform([len(preprocessed_texts), doc_embedding_size], -1.0, 1.0), name=\"doc_embeddings\")\n",
    "decoder_weights = tf.Variable(tf.truncated_normal([vocabulary_size, concatenated_size],\n",
    "                                               stddev=1.0 / np.sqrt(concatenated_size)),\n",
    "                                               name=\"decoder_weights\")\n",
    "decoder_biases = tf.Variable(tf.zeros([vocabulary_size]), name=\"decoder_biases\")\n",
    "\n",
    "\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[None, 2]) #2:  1 for word index and 1 for doc index\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embed= tf.nn.embedding_lookup(embeddings, x_inputs[:, 0])\n",
    "    \n",
    "doc_indices = tf.slice(x_inputs, [0,1],[batch_size,1])\n",
    "doc_embed = tf.nn.embedding_lookup(doc_embeddings,doc_indices)\n",
    "final_embed = tf.concat([embed, tf.squeeze(doc_embed)],1)\n",
    "\n",
    "logits = tf.matmul(final_embed, tf.transpose(decoder_weights)) + decoder_biases\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_target))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=model_learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "#cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True, name=\"cosine_similarity\")\n",
    "\n",
    "with tf.name_scope(\"performance\"):\n",
    "    loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "    loss_summary = tf.summary.scalar('loss', loss_ph)\n",
    "    performance_summaries = tf.summary.merge([loss_summary])\n",
    "\n",
    "saver = tf.train.Saver({\"embeddings\": embeddings, \"doc_embeddings\": doc_embeddings, \"decoder_weights\":decoder_weights, \"decoder_biases\":decoder_biases})\n",
    "summ_writer = tf.summary.FileWriter(summaries_folder_name, sess.graph)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "print('Starting Training')\n",
    "\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = create_batch_data(text_data)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        summ = sess.run(performance_summaries, feed_dict={loss_ph:loss_val})\n",
    "        summ_writer.add_summary(summ, i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "        \n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        #save vocabulary\n",
    "        with open(os.path.join(models_folder_name,'doc2vec_recipes_dict_words_integers.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        #save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),models_folder_name,'doc2vec_recipes_checkpoint.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))\n",
    "        \n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
