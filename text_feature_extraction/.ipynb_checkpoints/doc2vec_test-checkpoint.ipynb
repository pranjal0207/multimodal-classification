{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deviled_eggs': 6, 'macaroni_and_cheese': 15, 'sushi': 23, 'pancakes': 19, 'lasagna': 14, 'macarons': 16, 'tacos': 24, 'waffles': 26, 'hummus': 13, 'chicken_curry': 2, 'sashimi': 21, 'french_toast': 8, 'apple_pie': 0, 'frozen_yogurt': 10, 'tiramisu': 25, 'steak': 22, 'chicken_wings': 3, 'chocolate_cake': 4, 'chocolate_mousse': 5, 'garlic_bread': 11, 'fried_rice': 9, 'cheesecake': 1, 'hamburger': 12, 'onion_rings': 18, 'pizza': 20, 'dumplings': 7, 'nachos': 17}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>number_of_important_words</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>text_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>58</td>\n",
       "      <td>[apple, pie, recipe, dessert, cinnamon, roll, ...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>66</td>\n",
       "      <td>[apple, pie, dessert, apple, powder, butter, s...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>28</td>\n",
       "      <td>[apple, pie, dessert, puff, pastry, butter, su...</td>\n",
       "      <td>11.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>78</td>\n",
       "      <td>[caramel, apple, pie, recipe, dessert, granny,...</td>\n",
       "      <td>12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>28</td>\n",
       "      <td>[cooker, apple, recipe, dessert, apple, pie, f...</td>\n",
       "      <td>13.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>46</td>\n",
       "      <td>[cooker, cranberry, apple, pie, recipe, desser...</td>\n",
       "      <td>14.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>82</td>\n",
       "      <td>[apple, pie, recipe, dessert, flour, sugar, sa...</td>\n",
       "      <td>15.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>60</td>\n",
       "      <td>[apple, pie, bread, recipe, dessert, bread, eg...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>69</td>\n",
       "      <td>[apple, pie, recipe, bakery, pie, crust, cream...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>85</td>\n",
       "      <td>[apple, pie, recipe, bakery, vanilla, ice, cre...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>54</td>\n",
       "      <td>[apple, pie, dessert, graham, cracker, sugar, ...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>41</td>\n",
       "      <td>[apple, pie, recipe, dessert, granny, smith, a...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>99</td>\n",
       "      <td>[apple, pie, dumpling, recipe, bakery, apple, ...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>70</td>\n",
       "      <td>[caramel, apple, pie, recipe, bakery, apple, l...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>apple_pie</td>\n",
       "      <td>43</td>\n",
       "      <td>[cinnamon, roll, apple, pie, recipe, bakery, s...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>39</td>\n",
       "      <td>[recipe, dessert, butter, graham, cracker, cre...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>54</td>\n",
       "      <td>[bake, cherry, cheesecake, recipe, dessert, gr...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>46</td>\n",
       "      <td>[box, brownie, cheesecake, recipe, brownie, mi...</td>\n",
       "      <td>11.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>49</td>\n",
       "      <td>[cheesecake, recipe, dessert, mix, cream, chee...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>55</td>\n",
       "      <td>[cherry, cheesecake, recipe, dessert, dessert,...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>44</td>\n",
       "      <td>[chocolate, recipe, dessert, brownie, mix, cre...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>54</td>\n",
       "      <td>[mini, cheesecake, dessert, biscuit, butter, c...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>32</td>\n",
       "      <td>[mini, cheesecake, dessert, cream, cheese, cre...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>51</td>\n",
       "      <td>[mini, bake, cheesecake, dessert, butter, milk...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>40</td>\n",
       "      <td>[mini, peanut, butter, cheesecake, cracker, cr...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>55</td>\n",
       "      <td>[nibble, brownie, dessert, sugar, butter, choc...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>136</td>\n",
       "      <td>[chicken, curry, recipe, dinner, salt, ground,...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>43</td>\n",
       "      <td>[chicken, curry, recipe, dinner, oil, chicken,...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>72</td>\n",
       "      <td>[coconut, chicken, curry, recipe, chicken, oni...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>110</td>\n",
       "      <td>[chicken, curry, dinner, coconut, oil, cinnamo...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>tacos</td>\n",
       "      <td>30</td>\n",
       "      <td>[cauliflower, tortilla, taco, recipe, dinner, ...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>tacos</td>\n",
       "      <td>35</td>\n",
       "      <td>[bean, potato, taco, recipe, lunch, potato, oi...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>tacos</td>\n",
       "      <td>61</td>\n",
       "      <td>[chicken, taco, recipe, lunch, chicken, bell, ...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>tacos</td>\n",
       "      <td>42</td>\n",
       "      <td>[cilantro, lime, chicken, taco, recipe, chicke...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>tacos</td>\n",
       "      <td>54</td>\n",
       "      <td>[potato, taco, recipe, dinner, potato, cream, ...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>tacos</td>\n",
       "      <td>59</td>\n",
       "      <td>[decker, potato, taco, recipe, dinner, potato,...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>tacos</td>\n",
       "      <td>75</td>\n",
       "      <td>[taco, cilantro, sauce, recipe, lunch, shrimp,...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>tacos</td>\n",
       "      <td>24</td>\n",
       "      <td>[chicken, taco, recipe, tortilla, butter, chic...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>tacos</td>\n",
       "      <td>41</td>\n",
       "      <td>[chicken, taco, recipe, dinner, chicken, onion...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>tacos</td>\n",
       "      <td>38</td>\n",
       "      <td>[chicken, taco, recipe, lunch, chicken, onion,...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>34</td>\n",
       "      <td>[egg, yolk, sugar, mascarpone, egg, white, cre...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>46</td>\n",
       "      <td>[strawberry, confectioner, sugar, mascarpone, ...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>31</td>\n",
       "      <td>[egg, yolk, sugar, cream, vanilla, mascarpone,...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>36</td>\n",
       "      <td>[egg, yolk, sugar, cream, ladyfinger, coffee, ...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>46</td>\n",
       "      <td>[sugar, egg, espresso, sugar, ladyfinger, powd...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>28</td>\n",
       "      <td>[coffee, liqueur, cream, cheese, cream, ladyfi...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>22</td>\n",
       "      <td>[egg, yolk, sugar, mascarpone, ladyfinger, cof...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>46</td>\n",
       "      <td>[strawberry, confectioner, sugar, mascarpone, ...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>70</td>\n",
       "      <td>[cake, cake, coffee, powder, coffee, coffee, c...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>38</td>\n",
       "      <td>[vanilla, wafer, coffee, water, cream, cheese,...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>waffles</td>\n",
       "      <td>25</td>\n",
       "      <td>[brownie, waffle, recipe, brownie, mix, vanill...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>waffles</td>\n",
       "      <td>24</td>\n",
       "      <td>[waffle, recipe, breakfast, egg, salt, pepper,...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>waffles</td>\n",
       "      <td>49</td>\n",
       "      <td>[chicken, breakfast, syrup, salt, pepper, pepp...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>waffles</td>\n",
       "      <td>15</td>\n",
       "      <td>[cinnamon, roll, waffle, cinnamon, roll, dough...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>waffles</td>\n",
       "      <td>39</td>\n",
       "      <td>[pancake, mix, recipe, breakfast, oat, milk, e...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>waffles</td>\n",
       "      <td>22</td>\n",
       "      <td>[waffle, recipe, breakfast, bread, egg, slice,...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>waffles</td>\n",
       "      <td>42</td>\n",
       "      <td>[mozzarella, waffle, mozzarella, cheese, flour...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>waffles</td>\n",
       "      <td>30</td>\n",
       "      <td>[pizza, waffle, pizza, dough, marinara, sauce,...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>waffles</td>\n",
       "      <td>40</td>\n",
       "      <td>[waffle, breakfast, mix, powder, oil, egg, pec...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>waffles</td>\n",
       "      <td>27</td>\n",
       "      <td>[waffle, recipe, mix, egg, oil, milk, egg, oil...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            labels  number_of_important_words  \\\n",
       "0        apple_pie                         58   \n",
       "1        apple_pie                         66   \n",
       "2        apple_pie                         28   \n",
       "3        apple_pie                         78   \n",
       "4        apple_pie                         28   \n",
       "5        apple_pie                         46   \n",
       "6        apple_pie                         82   \n",
       "7        apple_pie                         60   \n",
       "8        apple_pie                         69   \n",
       "9        apple_pie                         85   \n",
       "10       apple_pie                         54   \n",
       "11       apple_pie                         41   \n",
       "12       apple_pie                         99   \n",
       "13       apple_pie                         70   \n",
       "14       apple_pie                         43   \n",
       "15      cheesecake                         39   \n",
       "16      cheesecake                         54   \n",
       "17      cheesecake                         46   \n",
       "18      cheesecake                         49   \n",
       "19      cheesecake                         55   \n",
       "20      cheesecake                         44   \n",
       "21      cheesecake                         54   \n",
       "22      cheesecake                         32   \n",
       "23      cheesecake                         51   \n",
       "24      cheesecake                         40   \n",
       "25      cheesecake                         55   \n",
       "26   chicken_curry                        136   \n",
       "27   chicken_curry                         43   \n",
       "28   chicken_curry                         72   \n",
       "29   chicken_curry                        110   \n",
       "..             ...                        ...   \n",
       "286          tacos                         30   \n",
       "287          tacos                         35   \n",
       "288          tacos                         61   \n",
       "289          tacos                         42   \n",
       "290          tacos                         54   \n",
       "291          tacos                         59   \n",
       "292          tacos                         75   \n",
       "293          tacos                         24   \n",
       "294          tacos                         41   \n",
       "295          tacos                         38   \n",
       "296       tiramisu                         34   \n",
       "297       tiramisu                         46   \n",
       "298       tiramisu                         31   \n",
       "299       tiramisu                         36   \n",
       "300       tiramisu                         46   \n",
       "301       tiramisu                         28   \n",
       "302       tiramisu                         22   \n",
       "303       tiramisu                         46   \n",
       "304       tiramisu                         70   \n",
       "305       tiramisu                         38   \n",
       "306        waffles                         25   \n",
       "307        waffles                         24   \n",
       "308        waffles                         49   \n",
       "309        waffles                         15   \n",
       "310        waffles                         39   \n",
       "311        waffles                         22   \n",
       "312        waffles                         42   \n",
       "313        waffles                         30   \n",
       "314        waffles                         40   \n",
       "315        waffles                         27   \n",
       "\n",
       "                                    preprocessed_texts text_names  \n",
       "0    [apple, pie, recipe, dessert, cinnamon, roll, ...      1.txt  \n",
       "1    [apple, pie, dessert, apple, powder, butter, s...     10.txt  \n",
       "2    [apple, pie, dessert, puff, pastry, butter, su...     11.txt  \n",
       "3    [caramel, apple, pie, recipe, dessert, granny,...     12.txt  \n",
       "4    [cooker, apple, recipe, dessert, apple, pie, f...     13.txt  \n",
       "5    [cooker, cranberry, apple, pie, recipe, desser...     14.txt  \n",
       "6    [apple, pie, recipe, dessert, flour, sugar, sa...     15.txt  \n",
       "7    [apple, pie, bread, recipe, dessert, bread, eg...      2.txt  \n",
       "8    [apple, pie, recipe, bakery, pie, crust, cream...      3.txt  \n",
       "9    [apple, pie, recipe, bakery, vanilla, ice, cre...      4.txt  \n",
       "10   [apple, pie, dessert, graham, cracker, sugar, ...      5.txt  \n",
       "11   [apple, pie, recipe, dessert, granny, smith, a...      6.txt  \n",
       "12   [apple, pie, dumpling, recipe, bakery, apple, ...      7.txt  \n",
       "13   [caramel, apple, pie, recipe, bakery, apple, l...      8.txt  \n",
       "14   [cinnamon, roll, apple, pie, recipe, bakery, s...      9.txt  \n",
       "15   [recipe, dessert, butter, graham, cracker, cre...      1.txt  \n",
       "16   [bake, cherry, cheesecake, recipe, dessert, gr...     10.txt  \n",
       "17   [box, brownie, cheesecake, recipe, brownie, mi...     11.txt  \n",
       "18   [cheesecake, recipe, dessert, mix, cream, chee...      2.txt  \n",
       "19   [cherry, cheesecake, recipe, dessert, dessert,...      3.txt  \n",
       "20   [chocolate, recipe, dessert, brownie, mix, cre...      4.txt  \n",
       "21   [mini, cheesecake, dessert, biscuit, butter, c...      5.txt  \n",
       "22   [mini, cheesecake, dessert, cream, cheese, cre...      6.txt  \n",
       "23   [mini, bake, cheesecake, dessert, butter, milk...      7.txt  \n",
       "24   [mini, peanut, butter, cheesecake, cracker, cr...      8.txt  \n",
       "25   [nibble, brownie, dessert, sugar, butter, choc...      9.txt  \n",
       "26   [chicken, curry, recipe, dinner, salt, ground,...      1.txt  \n",
       "27   [chicken, curry, recipe, dinner, oil, chicken,...      2.txt  \n",
       "28   [coconut, chicken, curry, recipe, chicken, oni...      3.txt  \n",
       "29   [chicken, curry, dinner, coconut, oil, cinnamo...      4.txt  \n",
       "..                                                 ...        ...  \n",
       "286  [cauliflower, tortilla, taco, recipe, dinner, ...      1.txt  \n",
       "287  [bean, potato, taco, recipe, lunch, potato, oi...     10.txt  \n",
       "288  [chicken, taco, recipe, lunch, chicken, bell, ...      2.txt  \n",
       "289  [cilantro, lime, chicken, taco, recipe, chicke...      3.txt  \n",
       "290  [potato, taco, recipe, dinner, potato, cream, ...      4.txt  \n",
       "291  [decker, potato, taco, recipe, dinner, potato,...      5.txt  \n",
       "292  [taco, cilantro, sauce, recipe, lunch, shrimp,...      6.txt  \n",
       "293  [chicken, taco, recipe, tortilla, butter, chic...      7.txt  \n",
       "294  [chicken, taco, recipe, dinner, chicken, onion...      8.txt  \n",
       "295  [chicken, taco, recipe, lunch, chicken, onion,...      9.txt  \n",
       "296  [egg, yolk, sugar, mascarpone, egg, white, cre...      1.txt  \n",
       "297  [strawberry, confectioner, sugar, mascarpone, ...     10.txt  \n",
       "298  [egg, yolk, sugar, cream, vanilla, mascarpone,...      2.txt  \n",
       "299  [egg, yolk, sugar, cream, ladyfinger, coffee, ...      3.txt  \n",
       "300  [sugar, egg, espresso, sugar, ladyfinger, powd...      4.txt  \n",
       "301  [coffee, liqueur, cream, cheese, cream, ladyfi...      5.txt  \n",
       "302  [egg, yolk, sugar, mascarpone, ladyfinger, cof...      6.txt  \n",
       "303  [strawberry, confectioner, sugar, mascarpone, ...      7.txt  \n",
       "304  [cake, cake, coffee, powder, coffee, coffee, c...      8.txt  \n",
       "305  [vanilla, wafer, coffee, water, cream, cheese,...      9.txt  \n",
       "306  [brownie, waffle, recipe, brownie, mix, vanill...      1.txt  \n",
       "307  [waffle, recipe, breakfast, egg, salt, pepper,...     10.txt  \n",
       "308  [chicken, breakfast, syrup, salt, pepper, pepp...      2.txt  \n",
       "309  [cinnamon, roll, waffle, cinnamon, roll, dough...      3.txt  \n",
       "310  [pancake, mix, recipe, breakfast, oat, milk, e...      4.txt  \n",
       "311  [waffle, recipe, breakfast, bread, egg, slice,...      5.txt  \n",
       "312  [mozzarella, waffle, mozzarella, cheese, flour...      6.txt  \n",
       "313  [pizza, waffle, pizza, dough, marinara, sauce,...      7.txt  \n",
       "314  [waffle, breakfast, mix, powder, oil, egg, pec...      8.txt  \n",
       "315  [waffle, recipe, mix, egg, oil, milk, egg, oil...      9.txt  \n",
       "\n",
       "[316 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_crossvalidation_run=1\n",
    "models_folder_name_test = os.path.join(os.getcwd(),'models','test',str(number_of_crossvalidation_run))\n",
    "models_folder_name_train = os.path.join(os.getcwd(),'models','train',str(number_of_crossvalidation_run))\n",
    "summaries_folder_name = os.path.join(os.getcwd(),'summaries','test', str(number_of_crossvalidation_run))\n",
    "\n",
    "path_to_preprocessed_texts_test = os.path.join(models_folder_name_test,'recipes_test_dataset.pkl')\n",
    "path_to_preprocessed_texts_train = os.path.join(models_folder_name_train,'recipes_train_dataset.pkl')\n",
    "\n",
    "df_preprocessed_texts_test = pd.read_pickle(path_to_preprocessed_texts_test)\n",
    "df_preprocessed_texts_train = pd.read_pickle(path_to_preprocessed_texts_train)\n",
    "df_preprocessed_texts_all=pd.concat([df_preprocessed_texts_test,df_preprocessed_texts_train]).sort_index()\n",
    "words_to_ints = pd.read_pickle(os.path.join(models_folder_name_train, 'doc2vec_recipes_dict_words_integers.pkl'))\n",
    "\n",
    "preprocessed_texts_test = df_preprocessed_texts_test.preprocessed_texts.values.tolist()\n",
    "preprocessed_texts_train = df_preprocessed_texts_train.preprocessed_texts.values.tolist()\n",
    "preprocessed_texts_all = df_preprocessed_texts_all.preprocessed_texts.values.tolist()\n",
    "labels = df_preprocessed_texts_train['labels'].values.tolist()\n",
    "\n",
    "unique_labels=sorted(set(labels))\n",
    "number_categories=len(unique_labels)\n",
    "categories_indices=np.linspace(0,number_categories-1,number_categories,dtype=int)\n",
    "labels2integers=dict(zip(unique_labels,categories_indices))\n",
    "\n",
    "print(labels2integers)\n",
    "df_preprocessed_texts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "generations = 200000\n",
    "model_learning_rate = 0.0005\n",
    "\n",
    "embedding_size = 27   #word embedding size\n",
    "doc_embedding_size = 27  #document embedding size\n",
    "concatenated_size = embedding_size + doc_embedding_size\n",
    "\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace each word in texts with integer value\n",
    "def text_to_numbers(preprocessed_texts, word_dict):\n",
    "    data = []\n",
    "    for prepr_text in preprocessed_texts:\n",
    "        text_data = []\n",
    "        for word in prepr_text:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "                text_data.append(word_ix)\n",
    "        data.append(text_data)\n",
    "    return (data)\n",
    "\n",
    "\n",
    "def create_batch_data(text_with_words_conv_to_numbers, batch_size=batch_size):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    rand_text_ix = int(np.random.choice(len(text_with_words_conv_to_numbers), size=1))\n",
    "    rand_text = text_with_words_conv_to_numbers[rand_text_ix]\n",
    "    word_to_predict_label=np.random.choice(list(set(rand_text)), size=1,replace=False)\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        item_in_batch=[]        \n",
    "        \n",
    "        label_words=np.random.choice(rand_text, size=1,replace=False)\n",
    "\n",
    "        item_in_batch.extend(word_to_predict_label)\n",
    "        item_in_batch.append(rand_text_ix)     \n",
    "        label_data.extend(label_words)\n",
    "        batch_data.append(item_in_batch)\n",
    "\n",
    "        \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array(label_data))\n",
    "\n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'springform': 386, 'cumin': 121, 'sugar': 401, 'cardamom': 70, 'camembert': 68, 'none': 261, 'sprinkle': 387, 'saucepan': 349, 'coating': 94, 'cajun': 66, 'heart': 193, 'round': 338, 'shrimp': 361, 'facing': 151, 'cauliflower': 73, 'work': 452, 'block': 41, 'peanut': 287, 'decker': 129, 'box': 48, 'baste': 24, 'ground': 186, 'foil': 162, 'pizza': 300, 'pesto': 291, 'handful': 191, 'sea': 352, 'air': 0, 'cling': 91, 'mat': 236, 'mini': 248, 'lasagna': 210, 'cocoa': 95, 'sieve': 362, 'packet': 272, 'pot': 307, 'truffle': 427, 'wing': 448, 'seed': 356, 'wrap': 454, 'cornstarch': 112, 'seasoning': 354, 'mesh': 243, 'spatula': 377, 'breast': 52, 'chive': 83, 'herb': 194, 'speed': 378, 'macaron': 224, 'baking': 17, 'tuna': 428, 'roll': 337, 'pipe': 298, 'coloring': 99, 'confectioner': 102, 'cone': 101, 'skewer': 366, 'base': 22, 'frosting': 167, 'area': 8, 'blend': 39, 'cider': 87, 'salmon': 343, 'chickpea': 80, 'salt': 345, 'salsa': 344, 'fryer': 169, 'fusion': 170, 'core': 108, 'bun': 60, 'zip': 459, 'marinade': 230, 'turkey': 429, 'dunk': 141, 'drop': 138, 'flour': 161, 'spread': 383, 'carrot': 71, 'iron': 202, 'boil': 44, 'sauce': 348, 'buttercream': 64, 'border': 46, 'grate': 183, 'eye': 149, 'mix': 249, 'griddle': 184, 'macaroni': 225, 'lime': 217, 'flesh': 158, 'glaze': 177, 'yolk': 458, 'rectangle': 331, 'marshmallow': 232, 'bell': 33, 'puree': 321, 'cranberry': 115, 'cooky': 106, 'pork': 304, 'circle': 90, 'processor': 315, 'bakery': 16, 'thickness': 411, 'roe': 336, 'mozzarella': 254, 'egg': 142, 'honey': 196, 'breakfast': 51, 'batter': 25, 'ring': 335, 'oat': 266, 'mousse': 253, 'lunch': 222, 'oven': 271, 'sake': 342, 'brush': 57, 'thigh': 412, 'juice': 207, 'cool': 107, 'blackberry': 37, 'fry': 168, 'beaten': 29, 'everything': 147, 'oil': 267, 'chill': 82, 'tempura': 407, 'rainbow': 324, 'nibble': 260, 'muffin': 255, 'liner': 218, 'tofu': 416, 'meatball': 241, 'lid': 216, 'banana': 20, 'garnish': 174, 'matchstick': 237, 'lettuce': 215, 'bacon': 12, 'mandu': 227, 'patty': 284, 'beat': 28, 'coffee': 98, 'veggie': 434, 'dough': 135, 'nori': 263, 'asparagus': 10, 'ranch': 326, 'potato': 308, 'whip': 444, 'amp': 3, 'spring': 385, 'ramekin': 325, 'guacamole': 187, 'ravioli': 329, 'cheddar': 75, 'butter': 63, 'texture': 410, 'pepper': 290, 'pasta': 280, 'beer': 31, 'moisture': 252, 'form': 164, 'cod': 97, 'mayonnaise': 238, 'boneless': 45, 'touch': 424, 'onion': 268, 'hummus': 197, 'garbanzo': 172, 'pat': 283, 'pecan': 288, 'spray': 382, 'jack': 203, 'cream': 116, 'toast': 415, 'graham': 179, 'skin': 368, 'jasmine': 206, 'strip': 399, 'fill': 153, 'rare': 327, 'baguette': 14, 'bottom': 47, 'syrup': 404, 'dumpling': 140, 'orange': 269, 'mash': 235, 'sandwich': 346, 'wine': 447, 'provolone': 316, 'cheesecake': 77, 'almond': 1, 'jam': 205, 'filling': 154, 'cilantro': 88, 'brown': 55, 'enjoy': 145, 'hazelnut': 192, '½': 461, 'bake': 15, 'mushroom': 256, 'cheese': 76, 'buffalo': 59, 'floret': 160, 'tender': 408, 'toss': 422, 'dome': 134, 'wafer': 437, 'chopstick': 86, 'recipe': 330, 'peak': 286, 'ice': 198, 'wok': 450, 'popsicle': 303, 'crumb': 118, 'quantity': 322, 'espresso': 146, 'press': 313, 'bundt': 61, 'naan': 258, 'waffle': 438, 'bag': 13, 'slice': 370, 'biscuit': 35, 'garlic': 173, 'pine': 296, 'bubble': 58, 'freezer': 166, 'instruction': 201, 'ball': 18, 'stuffed': 400, 'breadcrumb': 50, 'dente': 130, 'coriander': 109, 'total': 423, 'fish': 156, 'mango': 228, 'sauté': 350, 'scallion': 351, 'pumpkin': 320, 'sodium': 373, 'bit': 36, 'avocado': 11, 'grill': 185, 'filet': 152, 'layer': 211, 'wash': 440, 'stick': 393, 'masala': 233, 'parmesan': 277, 'pastry': 282, 'finger': 155, 'jalapeño': 204, 'face': 150, 'pinch': 295, 'burger': 62, 'raspberry': 328, 'cucumber': 120, 'squash': 388, 'cherry': 78, 'snack': 372, 'ladyfinger': 209, 'melt': 242, 'nut': 264, 'bbq': 26, 'skillet': 367, 'pudding': 318, 'chicken': 79, 'paprika': 276, 'sprig': 384, 'basil': 23, 'position': 306, 'wire': 449, 'panko': 274, 'tortilla': 421, 'middle': 245, 'clove': 92, 'dinner': 132, 'shiitake': 360, 'rack': 323, 'inside': 200, 'maple': 229, 'cut': 124, 'barbecue': 21, 'combine': 100, 'gelatin': 175, 'teriyaki': 409, 'amount': 2, 'grain': 180, 'sashimi': 347, 'meat': 240, 'hamburger': 190, 'curry': 123, 'liquid': 220, 'pressure': 314, 'rice': 333, 'marinara': 231, 'sheet': 358, 'turmeric': 430, 'tray': 426, 'dry': 139, 'mascarpone': 234, 'puff': 319, 'stock': 395, 'apple': 6, 'tongs': 418, 'vegetable': 433, 'steam': 392, 'coconut': 96, 'worm': 453, 'sit': 365, 'spinach': 380, 'granola': 182, 'sesame': 357, 'drain': 136, 'chocolate': 84, 'ham': 189, 'angle': 4, 'lemon': 214, 'bamboo': 19, 'yeast': 456, 'macarons': 226, 'spoonful': 381, 'pico': 292, 'elbow': 144, 'curl': 122, 'noodle': 262, 'cracker': 114, 'liqueur': 219, 'broccoli': 53, 'vinegar': 435, 'paste': 281, 'gouda': 178, 'bean': 27, 'tahini': 406, 'loaf': 221, 'simmer': 363, 'dripping': 137, 'mac': 223, 'stir': 394, 'ginger': 176, 'granny': 181, 'vanilla': 432, 'wedge': 442, 'yogurt': 457, 'brownie': 56, 'mustard': 257, 'tin': 414, 'blueberry': 42, 'leg': 213, 'crust': 119, 'sushi': 403, 'kosher': 208, 'cinnamon': 89, 'season': 353, 'space': 376, 'portion': 305, 'towel': 425, 'garam': 171, 'puck': 317, 'hole': 195, 'oregano': 270, 'food': 163, 'pancake': 273, 'preheat': 312, 'nacho': 259, 'worcestershire': 451, 'thumb': 413, 'running': 341, 'arrange': 9, 'leaf': 212, 'coat': 93, 'meal': 239, 'whisk': 445, 'wrapper': 455, 'shell': 359, 'choice': 85, 'parsley': 278, 'spicy': 379, 'berry': 34, 'strainer': 397, 'water': 441, 'pie': 293, 'cashew': 72, 'upwards': 431, 'metal': 244, 'mixer': 250, 'rest': 332, 'pod': 301, 'poke': 302, 'appetizer': 5, 'paper': 275, 'sriracha': 389, 'caramel': 69, 'prawn': 311, 'run': 340, 'beet': 32, 'cabbage': 65, 'seaweed': 355, 'cookie': 105, 'wasabi': 439, 'white': 446, 'milk': 246, 'piece': 294, 'imitation': 199, 'taco': 405, 'row': 339, 'crab': 113, 'well': 443, 'crispy': 117, 'flake': 157, 'eggplant': 143, 'bread': 49, 'soup': 374, 'cake': 67, 'strawberry': 398, 'zucchini': 460, 'pour': 309, 'dipping': 133, 'ricotta': 334, 'extract': 148, 'corn': 110, 'beef': 30, 'topping': 419, 'powder': 310, 'starch': 390, 'blade': 38, 'virgin': 436, 'dark': 128, 'peel': 289, 'surface': 402, 'pea': 285, 'blender': 40, 'chili': 81, 'cayenne': 74, 'sirloin': 364, 'soy': 375, 'nutmeg': 265, 'cooker': 104, 'gyoza': 188, 'torch': 420, 'aquafaba': 7, 'mixture': 251, 'milliliter': 247, 'boat': 43, 'daikon': 127, 'smith': 371, 'stone': 396, 'cutting': 125, 'freeze': 165, 'dessert': 131, 'cylinder': 126, 'piping': 299, 'tomato': 417, 'flip': 159, 'part': 279, 'skinless': 369, 'pineapple': 297, 'steak': 391, 'corner': 111, 'broth': 54, 'consistency': 103}\n",
      "462\n"
     ]
    }
   ],
   "source": [
    "word_dictionary=words_to_ints\n",
    "vocabulary_size=len(word_dictionary)\n",
    "print(word_dictionary)\n",
    "print(vocabulary_size)\n",
    "\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52,)\n",
      "(264,)\n",
      "(316,)\n",
      "[[6, 293, 131, 6, 310, 63, 401, 89, 345, 282, 142, 275, 6, 63, 401, 89, 265, 345, 358, 319, 282, 142, 142, 275, 289, 6, 441, 310, 310, 441, 251, 63, 401, 89, 265, 345, 445, 251, 6, 394, 282, 312, 282, 331, 6, 251, 331, 441, 331, 6, 331, 331, 154, 445, 142, 293, 358, 293, 15, 55, 331, 275, 331, 6, 293, 145], [6, 293, 131, 319, 282, 63, 401, 6, 89, 186, 358, 282, 63, 401, 6, 249, 319, 282, 370, 6, 63, 282, 401, 282, 249, 6, 282, 145], [69, 6, 293, 330, 131, 181, 371, 6, 207, 161, 401, 186, 89, 186, 293, 119, 142, 116, 55, 401, 63, 181, 371, 6, 207, 401, 186, 89, 186, 293, 119, 142, 116, 63, 345, 312, 271, 6, 370, 207, 161, 401, 89, 6, 293, 119, 293, 6, 370, 47, 119, 293, 119, 6, 370, 135, 119, 293, 142, 293, 119, 55, 282, 349, 116, 401, 401, 445, 63, 44, 69, 69, 348, 107, 293, 345, 370, 145], [104, 115, 6, 293, 330, 131, 115, 348, 6, 293, 154, 67, 249, 63, 432, 198, 116, 115, 6, 293, 67, 249, 63, 432, 198, 116, 115, 348, 6, 293, 104, 47, 67, 249, 104, 6, 115, 370, 63, 370, 249, 432, 198, 116, 116, 145], [6, 293, 330, 131, 161, 401, 345, 63, 198, 441, 181, 371, 6, 401, 214, 112, 214, 267, 345, 161, 401, 345, 63, 441, 181, 371, 6, 401, 207, 267, 345, 119, 161, 401, 345, 63, 161, 294, 198, 441, 249, 135, 135, 271, 154, 289, 108, 6, 370, 401, 214, 112, 207, 267, 345, 6, 402, 337, 135, 338, 293, 47, 135, 47, 293, 119, 6, 119, 119, 135, 119, 119, 267, 401, 89, 119, 15, 119, 55, 107, 370, 145], [6, 293, 49, 330, 131, 49, 142, 246, 432, 148, 89, 371, 6, 401, 112, 432, 198, 116, 69, 348, 370, 142, 246, 432, 89, 181, 371, 6, 401, 432, 198, 116, 69, 312, 49, 142, 246, 432, 89, 49, 394, 49, 246, 251, 49, 338, 6, 401, 112, 6, 49, 15, 107, 370, 432, 198, 116, 69, 348, 145], [6, 293, 330, 16, 432, 198, 116, 161, 345, 63, 198, 441, 181, 371, 6, 401, 161, 345, 89, 214, 142, 432, 198, 116, 345, 63, 441, 371, 6, 401, 345, 89, 265, 214, 142, 401, 161, 345, 249, 63, 161, 251, 441, 135, 441, 135, 135, 135, 402, 6, 108, 370, 6, 401, 161, 345, 89, 265, 207, 214, 249, 6, 271, 402, 293, 135, 337, 338, 135, 135, 6, 154, 251, 135, 135, 293, 142, 401, 293, 293, 119, 55, 282, 116, 145], [6, 293, 131, 179, 114, 401, 89, 63, 6, 207, 89, 175, 116, 186, 179, 114, 401, 89, 63, 6, 207, 310, 116, 89, 312, 271, 179, 114, 401, 89, 63, 179, 114, 251, 47, 15, 63, 179, 114, 119, 349, 6, 207, 89, 175, 175, 179, 114, 175, 251, 175, 116, 89, 145], [6, 293, 330, 131, 181, 371, 6, 401, 112, 89, 345, 293, 6, 6, 401, 345, 119, 289, 108, 6, 6, 401, 112, 89, 345, 275, 293, 119, 293, 119, 358, 6, 251, 119, 293, 119, 399, 119, 15, 116, 145], [6, 293, 140, 330, 16, 6, 401, 89, 63, 112, 319, 282, 142, 6, 87, 441, 401, 89, 63, 207, 6, 401, 89, 63, 112, 358, 282, 142, 6, 87, 441, 401, 89, 214, 207, 214, 289, 6, 6, 294, 108, 47, 108, 6, 294, 6, 6, 294, 401, 89, 112, 63, 249, 6, 294, 282, 402, 282, 358, 338, 348, 282, 358, 108, 6, 6, 294, 282, 142, 282, 6, 282, 140, 142, 140, 140, 15, 271, 282, 55, 154, 87, 348, 140, 441, 6, 87, 401, 89, 349, 44, 44, 348, 445, 63, 207, 348, 140, 145], [69, 6, 293, 330, 16, 6, 214, 401, 401, 89, 293, 135, 116, 6, 401, 55, 401, 89, 265, 293, 116, 6, 441, 207, 441, 6, 6, 108, 108, 401, 401, 89, 265, 394, 6, 394, 293, 135, 135, 6, 89, 401, 251, 6, 69, 348, 271, 6, 370, 293, 135, 370, 6, 370, 15, 15, 55, 271, 107, 349, 89, 401, 44, 116, 394, 69, 348, 6, 293, 145], [89, 337, 6, 293, 330, 16, 371, 6, 89, 337, 401, 89, 181, 371, 6, 337, 401, 89, 112, 312, 267, 289, 6, 370, 401, 89, 112, 6, 394, 89, 337, 161, 47, 89, 337, 119, 6, 337, 119, 15, 15, 293, 145], [330, 131, 63, 179, 114, 116, 76, 401, 432, 148, 63, 179, 114, 116, 76, 63, 325, 325, 179, 114, 118, 325, 63, 249, 119, 47, 116, 76, 401, 432, 154, 179, 114, 119, 77, 325, 82, 77, 145], [15, 78, 77, 330, 131, 179, 114, 118, 401, 63, 345, 116, 76, 432, 148, 116, 401, 310, 345, 179, 114, 401, 63, 345, 116, 401, 310, 345, 154, 179, 114, 118, 401, 345, 63, 386, 82, 116, 76, 28, 116, 401, 432, 345, 28, 310, 116, 251, 28, 119, 165, 154, 77, 145], [48, 56, 77, 330, 56, 249, 116, 76, 401, 432, 148, 441, 175, 116, 48, 56, 249, 432, 441, 116, 312, 271, 56, 249, 48, 309, 414, 15, 271, 77, 154, 116, 76, 401, 432, 148, 175, 441, 249, 77, 154, 56, 22, 82, 116, 145], [77, 330, 131, 249, 116, 76, 401, 432, 148, 246, 310, 116, 387, 48, 48, 76, 401, 432, 246, 175, 310, 116, 387, 312, 249, 48, 25, 386, 15, 116, 76, 401, 432, 249, 246, 445, 175, 310, 175, 251, 116, 251, 445, 77, 251, 82, 116, 387, 145], [78, 77, 330, 131, 131, 78, 77, 77, 78, 154, 28, 330, 401, 441, 78, 84, 63, 116, 76, 432, 116, 401, 445, 394, 441, 401, 78, 394, 148, 386, 78, 165, 63, 251, 386, 22, 165, 116, 76, 401, 432, 148, 116, 28, 77, 251, 119, 165, 78, 154, 77, 77, 25, 82, 145], [248, 77, 131, 35, 63, 116, 76, 116, 401, 142, 432, 148, 35, 63, 116, 401, 142, 432, 148, 401, 77, 312, 271, 13, 118, 309, 63, 35, 22, 426, 426, 35, 22, 15, 77, 154, 116, 76, 116, 445, 401, 142, 432, 426, 35, 22, 154, 15, 401, 77, 401, 401, 401, 145], [248, 15, 77, 131, 63, 246, 246, 116, 76, 401, 116, 179, 114, 63, 246, 246, 116, 76, 401, 179, 114, 118, 114, 63, 394, 246, 246, 251, 251, 251, 116, 76, 63, 401, 116, 249, 251, 394, 179, 114, 63, 414, 218, 119, 47, 218, 251, 165, 414, 218, 145], [248, 287, 63, 77, 114, 116, 76, 401, 287, 63, 116, 432, 114, 401, 287, 63, 116, 432, 142, 287, 63, 114, 47, 414, 28, 116, 76, 401, 287, 63, 116, 432, 142, 28, 251, 287, 63, 114, 15, 145], [260, 56, 131, 401, 63, 84, 142, 246, 84, 76, 401, 142, 432, 148, 246, 84, 84, 13, 246, 84, 260, 116, 76, 142, 13, 260, 271, 401, 63, 394, 84, 394, 142, 394, 13, 260, 251, 56, 426, 15, 77, 445, 116, 76, 401, 445, 142, 432, 148, 251, 56, 13, 260, 15, 145], [79, 123, 330, 132, 345, 186, 290, 186, 121, 186, 430, 109, 186, 70, 74, 45, 369, 79, 412, 457, 176, 267, 71, 268, 308, 417, 281, 417, 79, 54, 333, 88, 442, 441, 310, 345, 457, 246, 267, 63, 186, 121, 186, 109, 186, 70, 74, 45, 369, 79, 412, 457, 92, 176, 267, 71, 268, 308, 281, 417, 206, 333, 88, 442, 441, 310, 345, 457, 246, 267, 393, 63, 345, 290, 121, 109, 70, 74, 394, 79, 412, 457, 92, 173, 176, 79, 441, 310, 457, 246, 394, 394, 135, 135, 267, 267, 307, 79, 267, 307, 71, 268, 308, 92, 173, 394, 394, 417, 281, 417, 54, 394, 363, 79, 394, 363, 308, 79, 394, 457, 258, 135, 135, 135, 258, 135, 258, 63, 258, 79, 123, 333, 457, 88, 442, 145], [79, 123, 330, 132, 267, 79, 345, 290, 268, 71, 308, 441, 285, 281, 267, 79, 345, 268, 71, 308, 441, 281, 267, 79, 353, 345, 290, 79, 307, 267, 268, 71, 308, 79, 441, 44, 363, 216, 285, 123, 281, 333, 145], [79, 123, 132, 96, 267, 89, 393, 70, 301, 333, 441, 345, 267, 268, 176, 281, 186, 430, 186, 121, 186, 109, 74, 457, 45, 369, 79, 412, 345, 290, 441, 442, 88, 96, 267, 89, 393, 301, 333, 441, 345, 96, 267, 268, 176, 92, 417, 281, 186, 186, 186, 109, 74, 457, 45, 369, 79, 412, 345, 290, 441, 88, 333, 96, 267, 307, 89, 393, 70, 301, 333, 441, 345, 333, 333, 89, 393, 70, 301, 96, 267, 268, 268, 176, 173, 417, 281, 430, 121, 109, 74, 394, 457, 79, 412, 353, 345, 290, 79, 441, 216, 79, 123, 348, 79, 348, 333, 442, 88, 145], [79, 123, 330, 132, 45, 369, 79, 52, 310, 186, 121, 171, 233, 186, 176, 267, 81, 417, 441, 345, 290, 333, 109, 45, 369, 79, 81, 310, 121, 171, 233, 186, 176, 267, 81, 417, 441, 345, 290, 333, 109, 79, 353, 79, 310, 121, 171, 233, 176, 267, 307, 79, 81, 417, 441, 345, 290, 44, 363, 348, 333, 109, 145], [79, 123, 330, 132, 308, 441, 267, 123, 310, 45, 369, 79, 52, 268, 285, 33, 290, 71, 290, 345, 206, 333, 278, 308, 267, 123, 310, 45, 369, 79, 52, 268, 285, 33, 290, 71, 290, 345, 206, 333, 278, 308, 441, 308, 308, 308, 441, 307, 267, 123, 310, 79, 268, 307, 79, 285, 33, 290, 71, 290, 308, 441, 308, 307, 345, 290, 123, 310, 123, 44, 123, 206, 333, 278, 145], [63, 330, 52, 345, 290, 123, 310, 171, 233, 81, 310, 268, 92, 457, 96, 246, 63, 333, 109, 79, 52, 345, 290, 123, 310, 171, 233, 81, 310, 268, 92, 457, 246, 63, 333, 109, 216, 333, 109, 145], [96, 123, 330, 267, 45, 369, 79, 52, 345, 290, 54, 268, 173, 123, 281, 96, 246, 186, 430, 81, 310, 186, 109, 186, 176, 345, 290, 33, 290, 160, 160, 88, 267, 45, 369, 79, 52, 345, 290, 79, 268, 92, 123, 281, 246, 186, 81, 310, 186, 109, 186, 176, 345, 290, 33, 290, 160, 160, 88, 307, 267, 267, 52, 353, 345, 290, 79, 79, 79, 54, 268, 173, 307, 268, 123, 281, 394, 96, 246, 310, 109, 176, 345, 290, 394, 79, 33, 290, 394, 44, 216, 363, 123, 88, 145], [448, 5, 79, 448, 310, 345, 348, 448, 310, 348, 312, 252, 448, 275, 425, 394, 310, 345, 79, 17, 323, 55, 117, 323, 358, 275, 162, 137, 367, 59, 348, 79, 448, 348, 145], [196, 26, 79, 448, 330, 161, 81, 310, 345, 290, 310, 448, 26, 348, 196, 26, 81, 310, 345, 290, 310, 79, 448, 348, 196, 312, 161, 81, 310, 345, 290, 310, 448, 161, 448, 275, 358, 211, 15, 55, 26, 348, 196, 448, 348, 448, 358, 211, 348, 145], [196, 173, 448, 330, 5, 161, 310, 290, 345, 267, 63, 310, 375, 348, 196, 401, 357, 356, 161, 310, 290, 345, 267, 267, 63, 375, 348, 401, 357, 356, 161, 310, 290, 345, 79, 211, 251, 267, 79, 448, 267, 275, 425, 63, 348, 196, 401, 448, 348, 394, 448, 348, 357, 356, 145], [290, 448, 330, 5, 79, 448, 310, 345, 63, 207, 290, 448, 310, 345, 63, 207, 290, 345, 312, 448, 283, 139, 17, 310, 345, 448, 394, 17, 323, 382, 358, 448, 261, 150, 448, 448, 271, 348, 63, 207, 290, 345, 394, 448, 394, 145], [375, 79, 448, 330, 5, 79, 448, 345, 290, 390, 267, 375, 348, 401, 357, 356, 448, 345, 290, 390, 267, 401, 173, 357, 312, 267, 79, 448, 345, 290, 390, 448, 267, 448, 55, 275, 425, 448, 357, 145], [448, 330, 5, 79, 448, 310, 345, 375, 348, 401, 196, 357, 356, 448, 310, 345, 375, 348, 357, 356, 448, 283, 139, 17, 310, 345, 448, 394, 17, 323, 382, 358, 448, 261, 150, 448, 448, 271, 348, 375, 348, 394, 401, 196, 356, 394, 448, 394, 145], [117, 59, 448, 330, 5, 112, 448, 161, 310, 290, 310, 345, 290, 267, 59, 348, 112, 448, 283, 161, 310, 310, 290, 267, 59, 348, 112, 79, 448, 323, 112, 161, 310, 310, 345, 290, 267, 79, 448, 448, 267, 55, 79, 448, 323, 275, 425, 358, 79, 59, 348, 145], [194, 448, 330, 79, 448, 310, 345, 267, 173, 278, 345, 448, 310, 345, 267, 278, 345, 312, 252, 448, 275, 425, 394, 310, 345, 79, 17, 323, 55, 117, 323, 358, 275, 162, 137, 367, 267, 173, 394, 194, 448, 394, 348, 145], [196, 257, 448, 330, 5, 79, 448, 310, 345, 196, 257, 345, 448, 310, 345, 196, 257, 345, 290, 312, 252, 448, 275, 425, 394, 310, 345, 79, 17, 323, 55, 117, 323, 358, 275, 162, 137, 367, 196, 257, 348, 394, 79, 448, 348, 145], [345, 448, 5, 79, 448, 267, 112, 290, 448, 267, 290, 271, 358, 275, 79, 448, 267, 390, 290, 345, 448, 358, 15, 117, 145], [448, 79, 448, 310, 345, 375, 348, 196, 401, 357, 356, 448, 310, 345, 375, 348, 401, 357, 356, 252, 448, 275, 425, 394, 310, 345, 79, 17, 323, 55, 117, 323, 358, 275, 162, 137, 367, 375, 348, 196, 401, 356, 348, 394, 79, 448, 348, 145], [66, 26, 448, 330, 5, 79, 448, 310, 345, 26, 348, 66, 448, 310, 345, 26, 348, 66, 312, 448, 283, 139, 17, 310, 345, 448, 394, 17, 323, 382, 358, 448, 261, 150, 448, 448, 271, 348, 26, 348, 66, 448, 394, 448, 145], [194, 448, 330, 5, 79, 448, 310, 345, 63, 194, 39, 278, 448, 310, 345, 63, 173, 194, 39, 278, 312, 448, 283, 139, 17, 310, 345, 448, 394, 17, 323, 382, 358, 448, 261, 150, 448, 448, 271, 348, 63, 394, 173, 394, 194, 39, 345, 278, 448, 394, 145], [196, 448, 330, 448, 112, 345, 310, 290, 207, 196, 26, 79, 448, 112, 345, 310, 290, 348, 207, 196, 283, 79, 112, 310, 345, 79, 211, 358, 323, 207, 196, 345, 312, 448, 448, 448, 348, 145], [84, 330, 131, 401, 95, 310, 310, 246, 267, 432, 148, 84, 192, 383, 310, 310, 267, 432, 84, 192, 383, 84, 192, 383, 84, 192, 383, 25, 84, 192, 383, 401, 145], [453, 302, 48, 67, 330, 131, 67, 249, 267, 142, 249, 246, 116, 105, 453, 84, 48, 84, 67, 249, 267, 142, 249, 246, 116, 106, 453, 271, 84, 67, 249, 267, 142, 445, 25, 249, 246, 445, 116, 106, 67, 195, 67, 453, 67, 84, 116, 105, 251, 67, 67, 453, 106, 145], [84, 287, 63, 330, 131, 67, 249, 116, 76, 287, 63, 246, 246, 419, 287, 63, 48, 84, 67, 249, 76, 287, 63, 246, 246, 84, 67, 25, 15, 116, 76, 287, 63, 246, 246, 195, 67, 195, 287, 63, 251, 67, 67, 251, 251, 67, 419, 67, 67, 63, 145], [84, 67, 330, 84, 67, 84, 167, 84, 69, 84, 84, 302, 195, 67, 195, 67, 195, 246, 84, 67, 84, 167, 69, 67], [106, 116, 302, 67, 330, 84, 67, 106, 116, 318, 249, 246, 84, 106, 246, 272, 116, 246, 84, 106, 246, 302, 195, 67, 195, 272, 106, 116, 318, 249, 246, 445, 419, 84, 106, 394, 246, 67, 195, 251, 318, 251, 251, 106, 67], [106, 61, 330, 131, 116, 76, 401, 432, 148, 67, 249, 84, 105, 84, 116, 116, 76, 432, 148, 48, 84, 67, 249, 201, 106, 84, 95, 116, 271, 445, 116, 76, 401, 432, 148, 67, 25, 61, 67, 116, 116, 251, 67, 25, 67, 84, 105, 116, 76, 67, 25, 106, 67, 177, 84, 116, 177, 177, 67, 177, 67], [84, 269, 67, 330, 131, 84, 63, 142, 269, 142, 401, 116, 269, 148, 84, 63, 142, 142, 401, 116, 269, 84, 63, 394, 84, 142, 269, 84, 251, 394, 142, 401, 142, 251, 84, 251, 142, 251, 142, 394, 15, 271, 67, 269, 116, 116, 269, 148, 401, 116, 269, 145], [84, 48, 67, 330, 131, 67, 249, 116, 84, 95, 310, 48, 84, 67, 249, 116, 84, 95, 310, 271, 67, 249, 201, 25, 61, 15, 67, 116, 84, 116, 249, 84, 61, 67, 84, 195, 67, 67, 95, 310, 145], [84, 84, 330, 131, 441, 128, 84, 198, 116, 84, 441, 198, 128, 84, 198, 116, 84, 84, 441, 198, 441, 198, 441, 84, 251, 198, 84, 251, 116, 116, 84, 145], [84, 253, 330, 131, 116, 401, 84, 116, 105, 116, 401, 84, 116, 106, 116, 401, 250, 286, 164, 116, 84, 116, 251, 116, 84, 116, 251, 116, 84, 105, 145], [106, 116, 253, 330, 131, 84, 346, 105, 116, 84, 346, 106, 116, 401, 116, 84, 346, 106, 106, 13, 116, 106, 116, 84, 346, 106, 116, 250, 116, 106, 401, 28, 286, 164, 84, 346, 105, 13, 13, 84, 346, 105, 84, 346, 105, 145], [84, 96, 116, 330, 131, 96, 345, 404, 96, 116, 95, 310, 96, 246, 404, 345, 96, 246, 404, 432, 148, 96, 157, 96, 345, 96, 116, 95, 310, 96, 246, 345, 96, 246, 432, 148, 96, 157, 96, 157, 345, 96, 404, 251, 96, 116, 95, 310, 96, 246, 404, 345, 251, 96, 116, 96, 246, 250, 96, 246, 148, 404, 28, 96, 157, 145], [84, 253, 330, 131, 401, 432, 148, 84, 398, 7, 401, 432, 148, 128, 84, 398, 7, 250, 401, 286, 164, 432, 286, 164, 84, 253, 398, 145], [84, 253, 330, 131, 84, 96, 116, 96, 246, 432, 84, 96, 116, 96, 246, 432, 148, 84, 84, 96, 116, 96, 246, 432, 84, 145], [84, 253, 330, 131, 84, 346, 105, 175, 441, 401, 84, 310, 116, 432, 148, 116, 84, 346, 106, 175, 441, 401, 128, 84, 95, 310, 116, 432, 148, 116, 106, 13, 175, 441, 175, 401, 95, 116, 432, 250, 28, 251, 28, 106, 253, 253, 116, 286, 164, 401, 286, 164, 116, 253, 95, 310, 145], [84, 253, 330, 131, 441, 84, 401, 432, 441, 7, 128, 84, 401, 432, 28, 7, 286, 164, 84, 401, 432, 253, 398, 145], [11, 142, 330, 5, 142, 11, 457, 257, 207, 345, 290, 142, 11, 457, 257, 207, 345, 290, 124, 142, 458, 458, 446, 11, 457, 257, 207, 345, 290, 142, 458, 299, 13, 13, 111, 142, 446, 276, 145], [66, 142, 330, 5, 142, 238, 257, 66, 290, 348, 268, 142, 238, 257, 66, 348, 268, 142, 307, 153, 441, 142, 307, 44, 365, 142, 198, 441, 142, 458, 142, 446, 238, 257, 66, 290, 348, 458, 299, 13, 459, 13, 111, 124, 251, 142, 276, 268, 145], [142, 330, 142, 238, 257, 345, 290, 278, 142, 238, 257, 345, 290, 276, 278, 142, 307, 153, 441, 142, 307, 44, 365, 142, 198, 441, 142, 458, 142, 446, 238, 257, 345, 290, 458, 299, 13, 459, 13, 111, 124, 251, 142, 446, 276, 278, 145], [142, 330, 5, 142, 11, 207, 345, 290, 417, 268, 88, 204, 421, 142, 207, 345, 290, 417, 268, 204, 142, 307, 441, 44, 365, 441, 198, 441, 142, 458, 446, 458, 251, 142, 446, 421, 88, 145], [142, 330, 5, 142, 11, 88, 204, 268, 417, 207, 345, 421, 88, 142, 11, 88, 417, 207, 345, 88, 142, 307, 153, 441, 142, 307, 44, 365, 142, 198, 441, 142, 458, 142, 446, 11, 88, 204, 268, 417, 207, 345, 458, 299, 13, 459, 13, 111, 124, 251, 142, 88, 421, 145], [142, 330, 5, 142, 257, 238, 345, 290, 83, 238, 345, 290, 83, 142, 458, 257, 238, 345, 290, 458, 299, 13, 458, 251, 142, 446, 276, 145], [142, 330, 5, 142, 12, 83, 76, 345, 290, 12, 83, 142, 12, 83, 76, 345, 290, 12, 83, 142, 307, 153, 441, 142, 307, 44, 365, 142, 198, 441, 142, 458, 142, 446, 12, 83, 76, 345, 290, 458, 299, 13, 459, 13, 111, 124, 251, 142, 12, 83, 145], [255, 414, 142, 330, 5, 142, 457, 348, 345, 290, 278, 162, 142, 457, 348, 345, 290, 278, 142, 458, 142, 446, 255, 414, 458, 142, 255, 414, 414, 255, 414, 142, 162, 162, 142, 162, 441, 255, 414, 162, 142, 142, 446, 458, 162, 255, 414, 142, 458, 142, 446, 414, 142, 142, 458, 457, 348, 345, 290, 251, 299, 13, 458, 251, 142, 446, 276, 278, 145], [79, 140, 267, 79, 268, 71, 92, 345, 63, 161, 79, 116, 161, 310, 345, 116, 267, 268, 71, 92, 345, 63, 116, 310, 345, 116, 267, 79, 294, 71, 268, 394, 63, 161, 249, 116, 374, 161, 310, 345, 290, 116, 394, 251, 135, 116, 135, 338, 135, 251, 140, 374, 140, 140, 145], [227, 330, 227, 251, 416, 304, 351, 227, 455, 140, 186, 304, 186, 351, 268, 92, 176, 357, 267, 375, 348, 345, 441, 455, 348, 375, 348, 357, 267, 333, 447, 435, 290, 416, 275, 275, 441, 252, 416, 416, 294, 304, 351, 268, 176, 267, 375, 348, 345, 290, 249, 441, 245, 455, 155, 455, 441, 251, 455, 227, 455, 441, 227, 227, 441, 140, 348, 145], [374, 140, 330, 140, 455, 441, 441, 175, 310, 375, 348, 395, 186, 304, 268, 360, 256, 176, 267, 455, 247, 441, 175, 310, 375, 348, 186, 304, 268, 186, 256, 186, 294, 176, 92, 173, 357, 267, 342, 374, 441, 175, 310, 375, 348, 395, 394, 374, 154, 186, 304, 268, 256, 176, 267, 249, 381, 245, 455, 381, 374, 140, 140, 455, 294, 275, 140, 247, 441, 392, 145], [140, 304, 342, 345, 401, 173, 176, 375, 348, 267, 308, 390, 65, 83, 188, 368, 441, 308, 390, 267, 375, 348, 435, 267, 188, 348, 304, 345, 401, 92, 173, 176, 375, 348, 357, 267, 308, 390, 65, 83, 247, 441, 390, 357, 267, 375, 348, 435, 81, 267, 188, 348, 304, 342, 345, 401, 249, 173, 176, 375, 348, 267, 308, 390, 65, 381, 245, 188, 368, 441, 368, 155, 267, 188, 188, 216, 392, 216, 267, 145], [140, 330, 161, 345, 441, 65, 268, 176, 375, 348, 267, 186, 304, 290, 256, 71, 361, 375, 348, 333, 447, 435, 267, 290, 345, 441, 65, 268, 92, 176, 267, 186, 304, 290, 256, 71, 361, 375, 348, 333, 447, 435, 357, 267, 290, 161, 345, 441, 249, 135, 135, 294, 135, 294, 140, 135, 294, 294, 140, 455, 294, 275, 135, 65, 268, 173, 176, 375, 348, 267, 249, 304, 154, 186, 304, 345, 290, 65, 251, 394, 154, 256, 71, 65, 251, 394, 361, 154, 361, 65, 251, 394, 140, 455, 155, 455, 441, 455, 154, 155, 154, 455, 267, 140, 47, 140, 441, 216, 392, 140, 441, 140, 275, 252, 375, 348, 333, 435, 267, 290, 394, 140, 348, 145], [330, 65, 345, 186, 304, 360, 256, 351, 267, 176, 290, 338, 455, 267, 441, 348, 375, 348, 267, 333, 447, 65, 345, 186, 304, 360, 256, 351, 267, 290, 338, 455, 267, 441, 348, 375, 348, 357, 267, 333, 447, 435, 65, 345, 252, 65, 252, 65, 441, 65, 186, 304, 360, 351, 267, 176, 345, 290, 249, 245, 455, 155, 441, 455, 140, 455, 267, 140, 47, 441, 154, 140, 348, 145], [140, 361, 186, 304, 342, 345, 375, 348, 176, 83, 401, 455, 188, 368, 441, 267, 333, 435, 375, 348, 267, 186, 304, 345, 176, 92, 83, 401, 455, 247, 441, 357, 267, 267, 361, 186, 304, 342, 345, 375, 348, 176, 173, 401, 83, 249, 155, 455, 441, 455, 455, 381, 245, 455, 455, 455, 140, 140, 455, 154, 140, 267, 140, 47, 441, 216, 392, 216, 267, 348, 333, 435, 375, 348, 267, 140, 348, 145], [330, 186, 79, 268, 351, 88, 176, 81, 310, 267, 455, 65, 267, 417, 81, 176, 290, 345, 357, 356, 88, 186, 79, 268, 351, 81, 310, 63, 267, 140, 455, 65, 275, 267, 417, 290, 356, 88, 154, 186, 79, 268, 351, 88, 176, 310, 267, 249, 245, 455, 155, 441, 455, 155, 455, 441, 65, 392, 455, 348, 267, 417, 173, 176, 290, 345, 417, 417, 251, 357, 356, 88, 145], [415, 51, 49, 142, 63, 116, 84, 170, 370, 49, 142, 63, 116, 84, 118, 49, 370, 294, 142, 142, 118, 63, 116, 84, 145], [415, 330, 51, 142, 401, 432, 148, 345, 49, 63, 401, 186, 89, 186, 345, 404, 142, 432, 295, 345, 49, 63, 401, 186, 295, 186, 295, 345, 229, 404, 142, 401, 89, 295, 345, 445, 49, 142, 251, 49, 162, 63, 401, 89, 345, 162, 415, 162, 162, 404, 145], [334, 84, 415, 404, 49, 246, 334, 76, 84, 142, 246, 401, 398, 401, 370, 246, 334, 248, 84, 246, 295, 398, 401, 207, 142, 246, 401, 49, 370, 334, 76, 49, 370, 84, 84, 334, 142, 251, 367, 63, 49, 142, 404, 251, 404, 145], [415, 330, 51, 49, 63, 142, 246, 432, 89, 84, 232, 84, 404, 84, 170, 370, 63, 246, 432, 84, 248, 232, 84, 404, 248, 84, 445, 142, 89, 432, 246, 142, 440, 118, 141, 49, 142, 440, 118, 242, 63, 367, 49, 118, 49, 370, 84, 313, 248, 232, 84, 404, 145], [415, 330, 51, 49, 142, 432, 246, 89, 116, 76, 432, 148, 116, 398, 401, 102, 404, 170, 432, 246, 89, 116, 76, 432, 207, 398, 229, 404, 445, 142, 89, 432, 246, 142, 440, 249, 116, 76, 432, 401, 207, 116, 370, 49, 251, 313, 398, 370, 313, 141, 142, 440, 242, 63, 367, 49, 118, 49, 398, 102, 401, 404, 145], [415, 330, 51, 328, 37, 404, 328, 219, 116, 76, 49, 246, 142, 328, 219, 345, 63, 116, 404, 328, 37, 404, 328, 219, 76, 370, 49, 246, 142, 345, 116, 229, 404, 328, 37, 328, 219, 404, 34, 116, 76, 404, 370, 49, 34, 370, 49, 370, 49, 313, 294, 445, 246, 142, 328, 219, 345, 63, 367, 246, 251, 55, 116, 404, 116, 415, 229, 116, 34, 145], [42, 415, 43, 330, 51, 116, 76, 401, 89, 142, 116, 49, 42, 116, 76, 401, 89, 142, 116, 221, 49, 221, 42, 404, 116, 76, 401, 89, 142, 116, 445, 221, 49, 221, 49, 43, 49, 294, 49, 294, 42, 142, 251, 49, 43, 358, 275, 49, 142, 251, 55, 43, 162, 370, 404, 145], [42, 415, 330, 51, 49, 142, 246, 89, 432, 148, 42, 370, 49, 142, 89, 432, 42, 370, 49, 142, 246, 89, 432, 249, 49, 42, 49, 34, 142, 251, 142, 404, 145], [89, 415, 330, 51, 49, 63, 142, 246, 432, 89, 116, 76, 116, 401, 89, 432, 170, 63, 246, 432, 89, 116, 76, 116, 432, 445, 142, 89, 432, 246, 142, 440, 249, 116, 76, 116, 401, 89, 432, 141, 370, 49, 142, 440, 242, 63, 367, 49, 118, 49, 63, 89, 116, 251, 145], [415, 415, 142, 246, 63, 432, 89, 401, 345, 170, 221, 415, 142, 246, 63, 432, 89, 401, 345, 358, 415, 445, 142, 246, 63, 432, 89, 345, 401, 49, 251, 358, 358, 145], [34, 415, 330, 51, 49, 142, 246, 432, 89, 34, 401, 102, 170, 370, 49, 142, 246, 432, 89, 42, 328, 37, 207, 401, 102, 401, 445, 142, 89, 432, 246, 142, 440, 141, 49, 142, 440, 242, 63, 367, 49, 249, 34, 401, 207, 34, 370, 370, 102, 145], [415, 330, 51, 84, 232, 49, 142, 116, 63, 170, 49, 142, 116, 63, 370, 49, 232, 49, 142, 116, 251, 63, 55, 145], [12, 142, 333, 330, 132, 142, 12, 268, 267, 333, 345, 290, 401, 375, 348, 268, 170, 142, 268, 267, 333, 345, 290, 401, 268, 142, 345, 12, 268, 267, 450, 142, 142, 450, 12, 450, 12, 450, 142, 268, 450, 168, 333, 333, 168, 345, 290, 401, 348, 394, 333, 142, 12, 268, 145], [297, 333, 330, 297, 267, 142, 311, 173, 268, 333, 433, 348, 385, 268, 297, 267, 142, 311, 92, 268, 333, 433, 348, 385, 268, 297, 297, 297, 297, 450, 267, 29, 450, 142, 311, 168, 311, 142, 311, 450, 450, 267, 173, 268, 394, 333, 433, 333, 434, 142, 311, 348, 297, 394, 147, 297, 385, 268, 145], [357, 79, 73, 333, 330, 132, 73, 52, 285, 71, 375, 348, 290, 142, 357, 356, 79, 52, 71, 290, 142, 356, 73, 160, 160, 79, 285, 71, 375, 348, 290, 220, 333, 433, 79, 73, 333, 443, 29, 443, 142, 142, 333, 249, 357, 356, 145], [409, 79, 333, 134, 330, 132, 409, 79, 333, 134, 333, 333, 170, 401, 348, 79, 267, 71, 268, 33, 290, 345, 290, 285, 160, 267, 333, 142, 357, 356, 351, 220, 401, 375, 348, 79, 79, 332, 79, 267, 450, 367, 267, 71, 268, 290, 345, 290, 285, 160, 367, 53, 408, 433, 367, 267, 367, 173, 333, 267, 333, 367, 29, 434, 394, 333, 375, 348, 394, 333, 333, 267, 367, 79, 409, 348, 356, 333, 134, 409, 79, 134, 351, 145], [434, 333, 330, 132, 267, 71, 268, 33, 290, 53, 160, 285, 110, 142, 333, 375, 348, 267, 290, 170, 267, 71, 173, 268, 33, 290, 53, 160, 285, 110, 142, 333, 357, 267, 290, 450, 367, 267, 71, 268, 268, 33, 290, 53, 433, 142, 142, 249, 332, 433, 285, 110, 333, 375, 348, 267, 290, 249, 333, 145], [333, 330, 132, 73, 53, 160, 345, 290, 375, 348, 401, 170, 73, 160, 290, 401, 73, 160, 160, 53, 345, 290, 375, 348, 401, 220, 333, 145], [79, 333, 330, 267, 173, 268, 71, 110, 285, 333, 79, 142, 375, 348, 267, 345, 290, 268, 357, 356, 170, 267, 92, 173, 268, 71, 110, 285, 333, 79, 142, 375, 348, 357, 267, 345, 290, 268, 357, 356, 367, 267, 268, 71, 110, 285, 71, 408, 333, 79, 142, 375, 348, 267, 345, 290, 394, 147, 357, 356, 268, 145], [79, 409, 333, 330, 132, 79, 52, 409, 348, 267, 268, 71, 53, 160, 142, 333, 375, 348, 267, 290, 170, 79, 52, 409, 348, 267, 268, 173, 71, 53, 160, 333, 357, 267, 290, 79, 409, 348, 450, 367, 79, 409, 267, 268, 173, 71, 268, 433, 142, 142, 249, 332, 433, 333, 79, 375, 348, 267, 290, 249, 333, 145], [189, 333, 330, 132, 267, 268, 71, 33, 290, 189, 142, 333, 375, 348, 290, 170, 267, 268, 173, 71, 33, 290, 189, 142, 333, 290, 297, 450, 367, 267, 268, 71, 33, 290, 189, 268, 189, 189, 433, 142, 142, 249, 332, 433, 333, 375, 348, 290, 297, 249, 333, 145], [333, 330, 132, 267, 173, 52, 345, 290, 71, 53, 160, 333, 285, 373, 375, 348, 170, 267, 92, 173, 52, 345, 290, 71, 53, 160, 333, 285, 373, 375, 348, 267, 367, 79, 345, 290, 71, 53, 408, 333, 375, 348, 285, 145], [434, 333, 330, 333, 54, 71, 267, 268, 285, 345, 290, 142, 351, 333, 71, 267, 268, 92, 285, 345, 290, 142, 351, 333, 54, 71, 307, 394, 307, 333, 307, 307, 267, 267, 268, 173, 285, 345, 290, 249, 394, 147, 443, 333, 142, 29, 443, 142, 147, 351, 145], [333, 330, 333, 441, 375, 348, 267, 433, 142, 345, 333, 441, 375, 348, 357, 267, 433, 142, 351, 333, 441, 267, 348, 394, 333, 433, 333, 142, 142, 333, 142, 142, 333, 351, 145], [457, 330, 131, 432, 457, 196, 1, 324, 387, 432, 457, 1, 324, 387, 163, 315, 324, 387, 165, 145], [457, 182, 20, 330, 131, 20, 457, 1, 246, 20, 457, 1, 246, 182, 20, 303, 393, 245, 20, 20, 275, 358, 165, 457, 1, 246, 410, 246, 94, 182, 20, 457, 94, 337, 387, 419, 20, 166, 94, 145], [457, 182, 20, 330, 372, 20, 432, 457, 182, 303, 432, 457, 182, 303, 393, 20, 303, 393, 20, 275, 165, 20, 20, 457, 387, 182, 165, 145], [457, 20, 330, 131, 20, 457, 1, 246, 20, 457, 246, 20, 303, 393, 245, 20, 20, 275, 358, 165, 457, 1, 246, 410, 246, 94, 20, 457, 94, 337, 387, 419, 20, 166, 94, 145], [457, 387, 20, 131, 20, 457, 1, 246, 324, 387, 20, 457, 1, 246, 324, 387, 20, 303, 393, 245, 20, 20, 275, 358, 165, 457, 1, 246, 410, 246, 94, 20, 457, 94, 337, 387, 419, 20, 166, 94, 145], [84, 457, 330, 372, 20, 457, 84, 20, 457, 84, 84, 163, 315, 84, 165, 419, 145], [228, 457, 330, 372, 228, 246, 457, 196, 228, 246, 457, 196, 163, 315, 165, 419, 145], [457, 330, 131, 398, 432, 457, 398, 432, 457, 163, 315, 165, 145], [398, 457, 330, 131, 398, 457, 196, 398, 457, 196, 163, 315, 165, 145], [457, 330, 131, 228, 20, 457, 228, 20, 457, 163, 315, 165, 145], [457, 84, 20, 330, 131, 20, 457, 1, 246, 84, 20, 457, 1, 246, 84, 20, 303, 393, 245, 20, 20, 275, 358, 165, 457, 1, 246, 410, 246, 94, 84, 20, 457, 94, 337, 387, 419, 20, 166, 94, 145], [49, 330, 63, 76, 76, 75, 76, 268, 14, 170, 63, 92, 76, 75, 76, 268, 14, 312, 14, 14, 63, 251, 14, 370, 145], [417, 254, 49, 5, 49, 173, 63, 254, 76, 417, 23, 345, 290, 170, 221, 49, 92, 173, 63, 254, 76, 417, 23, 345, 290, 63, 173, 221, 49, 63, 49, 254, 417, 23, 345, 290, 348, 267, 354, 370, 399, 145], [68, 49, 330, 5, 221, 76, 63, 278, 170, 63, 278, 92, 68, 49, 49, 49, 63, 278, 173, 68, 49, 49, 63, 49, 15, 68, 49, 145], [30, 400, 49, 330, 5, 391, 345, 290, 142, 267, 14, 254, 76, 11, 63, 278, 170, 30, 345, 290, 142, 267, 14, 370, 11, 399, 63, 92, 278, 312, 345, 290, 30, 30, 30, 142, 49, 118, 49, 118, 30, 399, 30, 267, 30, 399, 55, 14, 294, 200, 14, 294, 14, 294, 254, 370, 30, 399, 370, 11, 76, 30, 11, 294, 400, 14, 294, 370, 14, 370, 358, 162, 63, 173, 278, 277, 63, 251, 14, 370, 162, 49, 173, 55, 145], [49, 330, 49, 63, 354, 76, 417, 267, 345, 290, 23, 221, 49, 63, 92, 354, 76, 417, 92, 173, 267, 345, 290, 23, 63, 92, 173, 221, 49, 63, 354, 76, 55, 417, 173, 345, 290, 23, 267, 267, 49, 251, 76, 145], [189, 49, 330, 5, 300, 135, 63, 278, 345, 254, 76, 75, 76, 189, 63, 92, 278, 345, 254, 76, 75, 76, 189, 370, 312, 300, 135, 63, 278, 345, 251, 135, 254, 75, 76, 135, 294, 189, 76, 135, 76, 49, 15, 145], [49, 330, 5, 14, 186, 30, 49, 118, 278, 345, 290, 142, 254, 76, 231, 348, 63, 278, 170, 14, 186, 30, 49, 118, 345, 290, 142, 370, 231, 348, 63, 92, 277, 312, 186, 30, 49, 118, 278, 142, 345, 290, 251, 241, 241, 14, 14, 294, 241, 200, 14, 294, 370, 254, 241, 254, 241, 241, 76, 14, 294, 400, 14, 294, 370, 358, 162, 63, 63, 14, 370, 370, 162, 14, 15, 162, 231, 145], [291, 49, 330, 300, 135, 291, 254, 76, 79, 63, 310, 268, 310, 290, 278, 170, 291, 254, 76, 79, 63, 310, 268, 310, 312, 300, 135, 291, 135, 254, 291, 79, 254, 135, 400, 135, 400, 135, 221, 63, 310, 268, 310, 290, 278, 63, 135, 135, 63, 76, 221, 15, 162, 278, 370, 145], [62, 330, 132, 186, 30, 345, 290, 268, 310, 267, 75, 76, 60, 12, 417, 186, 30, 345, 290, 268, 310, 267, 75, 76, 190, 12, 370, 30, 75, 30, 75, 30, 76, 267, 62, 62, 60, 12, 417, 215, 145], [12, 75, 330, 12, 116, 76, 268, 75, 76, 186, 30, 345, 290, 75, 76, 12, 116, 76, 268, 75, 186, 30, 345, 290, 370, 190, 60, 12, 12, 116, 76, 268, 75, 249, 186, 30, 251, 251, 186, 30, 353, 345, 290, 62, 62, 370, 76, 62, 62, 76, 62, 62, 60, 145], [190, 330, 132, 186, 30, 208, 345, 186, 290, 267, 76, 60, 186, 30, 267, 370, 76, 75, 190, 30, 284, 284, 284, 353, 345, 290, 367, 267, 284, 367, 284, 370, 76, 284, 76, 284, 62, 284, 60, 348, 145], [3, 62, 330, 80, 173, 267, 214, 406, 348, 186, 121, 345, 267, 310, 290, 157, 345, 290, 60, 215, 268, 92, 267, 207, 186, 121, 345, 267, 310, 157, 345, 290, 190, 215, 268, 80, 173, 207, 406, 121, 345, 370, 370, 370, 345, 267, 310, 290, 157, 345, 290, 370, 267, 251, 370, 62, 370, 145], [268, 62, 330, 132, 186, 30, 345, 290, 267, 190, 60, 76, 268, 186, 30, 345, 290, 267, 190, 60, 268, 249, 30, 284, 30, 267, 62, 370, 62, 60, 268, 145], [330, 132, 190, 60, 186, 30, 284, 76, 268, 186, 30, 284, 370, 76, 268, 63, 63, 190, 60, 60, 30, 76, 284, 284, 284, 76, 184, 207, 60, 268, 62, 60, 76, 145], [132, 267, 268, 290, 310, 121, 345, 417, 348, 348, 348, 401, 257, 190, 267, 268, 290, 92, 310, 121, 345, 417, 348, 348, 348, 60, 367, 267, 267, 268, 290, 268, 173, 310, 121, 345, 173, 353, 345, 417, 348, 348, 348, 401, 257, 249, 190, 60, 145], [330, 190, 60, 63, 186, 30, 345, 249, 76, 215, 417, 257, 290, 208, 345, 186, 190, 60, 63, 186, 30, 317, 345, 3, 249, 370, 215, 370, 417, 257, 208, 290, 208, 345, 186, 184, 190, 60, 63, 60, 184, 348, 348, 60, 184, 345, 3, 317, 317, 184, 377, 317, 284, 377, 377, 62, 345, 3, 249, 62, 207, 377, 62, 184, 377, 62, 62, 76, 62, 215, 370, 417, 60, 145], [197, 330, 372, 80, 173, 406, 207, 121, 290, 345, 290, 436, 267, 441, 278, 92, 406, 207, 121, 345, 290, 267, 441, 278, 80, 173, 406, 207, 163, 315, 39, 267, 441, 197, 290, 278, 267, 145], [380, 330, 372, 80, 380, 193, 207, 345, 290, 436, 267, 380, 193, 207, 345, 290, 267, 80, 380, 193, 173, 406, 207, 163, 315, 39, 267, 197, 380, 193, 267, 145], [434, 197, 330, 197, 290, 32, 197, 290, 32, 163, 315, 197, 321, 163, 315, 197, 290, 321, 163, 315, 197, 32, 321, 145], [27, 115, 197, 330, 27, 406, 214, 115, 267, 345, 441, 27, 207, 214, 115, 267, 345, 2, 441, 163, 315, 39, 2, 267, 441, 103, 267, 115, 145], [197, 330, 372, 172, 27, 345, 186, 121, 173, 207, 436, 267, 267, 441, 27, 345, 186, 92, 214, 207, 436, 267, 267, 441, 80, 8, 425, 425, 27, 368, 27, 163, 315, 345, 121, 315, 251, 163, 315, 341, 309, 214, 207, 267, 267, 441, 315, 340, 251, 85, 434, 145], [291, 197, 330, 372, 172, 27, 345, 186, 121, 291, 207, 441, 27, 345, 92, 186, 121, 291, 214, 207, 441, 80, 8, 425, 425, 27, 368, 27, 163, 315, 345, 121, 173, 291, 315, 251, 163, 315, 341, 309, 207, 441, 315, 340, 251, 85, 434, 145], [290, 197, 330, 372, 172, 27, 345, 186, 121, 173, 290, 207, 436, 267, 27, 345, 186, 92, 290, 214, 207, 436, 267, 80, 8, 425, 425, 27, 368, 27, 163, 315, 345, 121, 173, 290, 315, 251, 163, 315, 341, 309, 207, 267, 315, 340, 251, 85, 434, 145], [173, 197, 330, 172, 27, 173, 406, 214, 267, 345, 296, 264, 278, 441, 172, 27, 92, 207, 267, 345, 296, 264, 278, 441, 172, 27, 368, 368, 163, 315, 39, 2, 267, 441, 103, 296, 264, 267, 145], [197, 330, 372, 80, 321, 267, 173, 345, 207, 267, 92, 173, 345, 214, 207, 80, 267, 173, 345, 207, 163, 315, 267, 441, 103, 145], [197, 330, 372, 27, 406, 214, 345, 290, 290, 121, 267, 296, 264, 441, 172, 207, 92, 290, 121, 267, 296, 264, 441, 172, 27, 368, 368, 163, 315, 39, 2, 267, 441, 103, 330, 434, 145], [53, 79, 210, 330, 132, 63, 173, 290, 76, 262, 79, 53, 254, 63, 173, 290, 277, 76, 210, 262, 79, 53, 254, 312, 63, 173, 290, 44, 76, 348, 348, 47, 210, 262, 211, 79, 53, 254, 348, 211, 211, 348, 254, 162, 15, 162, 76, 145], [380, 210, 337, 132, 210, 262, 334, 76, 380, 267, 173, 231, 348, 254, 76, 76, 345, 290, 278, 170, 210, 262, 380, 267, 92, 173, 254, 277, 345, 290, 278, 210, 262, 380, 267, 345, 334, 380, 173, 461, 254, 461, 277, 345, 290, 251, 210, 262, 337, 231, 47, 210, 337, 332, 254, 277, 15, 76, 278, 145], [460, 210, 330, 132, 210, 330, 210, 460, 262, 186, 429, 240, 348, 460, 267, 268, 92, 186, 417, 345, 290, 417, 281, 142, 278, 345, 290, 460, 460, 262, 358, 267, 268, 186, 429, 429, 417, 23, 278, 270, 345, 290, 417, 281, 312, 334, 76, 142, 278, 277, 345, 290, 211, 429, 348, 251, 277, 211, 429, 348, 76, 332, 145], [329, 330, 132, 329, 348, 170, 329, 348, 426, 329, 348, 426, 211, 329, 211, 348, 211, 348, 211, 329, 211, 348, 76, 426, 162, 15, 162, 15, 76, 36, 145], [134, 330, 132, 267, 268, 186, 30, 345, 290, 417, 348, 417, 281, 334, 76, 23, 142, 76, 262, 254, 76, 63, 161, 345, 186, 290, 76, 23, 170, 267, 268, 92, 186, 30, 345, 290, 348, 281, 23, 142, 210, 262, 254, 63, 461, 345, 186, 290, 76, 23, 76, 312, 267, 268, 173, 30, 345, 290, 240, 240, 417, 348, 417, 281, 348, 251, 334, 23, 142, 277, 267, 262, 47, 262, 210, 262, 211, 240, 251, 254, 47, 262, 262, 240, 251, 254, 348, 334, 251, 262, 211, 332, 262, 332, 254, 332, 240, 348, 210, 262, 162, 63, 161, 281, 348, 44, 353, 345, 290, 277, 134, 348, 134, 23, 134, 277, 23, 145], [210, 337, 132, 210, 262, 186, 30, 186, 268, 345, 290, 231, 348, 334, 76, 380, 142, 254, 76, 170, 210, 262, 186, 30, 186, 268, 345, 290, 231, 380, 142, 210, 262, 367, 30, 268, 345, 290, 380, 231, 348, 334, 142, 251, 210, 251, 231, 254, 277, 145], [210, 330, 132, 380, 278, 76, 345, 290, 231, 348, 44, 210, 262, 254, 170, 334, 380, 278, 345, 290, 231, 44, 210, 262, 254, 334, 380, 278, 277, 345, 290, 211, 417, 348, 417, 348, 210, 262, 262, 211, 334, 251, 254, 211, 262, 334, 417, 348, 254, 211, 262, 417, 348, 211, 254, 277, 76, 145], [330, 132, 267, 186, 30, 345, 186, 290, 268, 231, 348, 358, 334, 76, 254, 76, 23, 170, 267, 186, 30, 345, 290, 173, 92, 268, 231, 348, 358, 254, 312, 267, 367, 30, 345, 290, 30, 268, 268, 231, 348, 348, 281, 30, 251, 358, 30, 358, 30, 251, 332, 358, 332, 231, 348, 358, 334, 348, 254, 76, 334, 23, 15, 76, 145], [210, 330, 267, 268, 290, 345, 290, 143, 388, 460, 417, 281, 270, 417, 334, 76, 254, 76, 23, 278, 142, 143, 460, 388, 417, 267, 268, 290, 92, 345, 290, 143, 388, 460, 281, 254, 23, 278, 143, 388, 417, 262, 312, 267, 367, 268, 290, 92, 345, 290, 143, 388, 460, 417, 281, 270, 345, 290, 433, 417, 281, 47, 417, 251, 332, 210, 211, 334, 254, 23, 278, 345, 290, 142, 143, 388, 460, 417, 173, 345, 290, 267, 211, 47, 262, 211, 334, 251, 433, 211, 262, 334, 433, 15, 76, 348, 433, 145], [223, 3, 330, 246, 225, 75, 246, 144, 225, 75, 76, 307, 246, 280, 394, 280, 75, 394, 76, 280, 145], [223, 330, 225, 441, 246, 345, 290, 75, 76, 83, 144, 225, 441, 246, 345, 290, 75, 76, 83, 225, 441, 345, 394, 246, 76, 345, 290, 394, 394, 387, 83, 145], [225, 330, 132, 267, 186, 30, 268, 225, 441, 30, 75, 417, 268, 267, 186, 268, 225, 441, 30, 75, 75, 417, 268, 267, 268, 394, 186, 30, 186, 30, 394, 441, 30, 280, 417, 394, 280, 280, 76, 394, 76, 268, 145], [380, 3, 330, 132, 63, 380, 246, 345, 290, 225, 75, 76, 254, 246, 290, 75, 76, 254, 312, 63, 380, 380, 246, 345, 290, 394, 246, 225, 246, 225, 75, 254, 76, 254, 145], [223, 330, 132, 12, 83, 278, 63, 161, 246, 345, 290, 310, 290, 225, 75, 76, 76, 76, 76, 316, 76, 254, 76, 75, 12, 118, 83, 278, 63, 246, 345, 290, 310, 225, 75, 76, 76, 76, 178, 76, 316, 76, 254, 76, 75, 76, 307, 12, 12, 118, 83, 278, 394, 118, 307, 307, 63, 161, 394, 246, 353, 345, 290, 290, 348, 312, 280, 348, 394, 75, 178, 316, 76, 76, 280, 246, 251, 254, 75, 223, 76, 387, 118, 251, 145], [223, 330, 132, 225, 63, 161, 246, 186, 290, 345, 380, 254, 76, 254, 76, 417, 23, 225, 63, 246, 186, 290, 345, 380, 254, 254, 417, 23, 312, 271, 307, 441, 280, 136, 63, 161, 394, 246, 348, 353, 276, 290, 345, 380, 394, 254, 394, 280, 394, 348, 254, 280, 254, 417, 23, 145], [76, 223, 330, 132, 63, 161, 246, 441, 144, 225, 116, 76, 75, 76, 76, 76, 76, 290, 345, 290, 23, 63, 246, 441, 116, 76, 75, 76, 178, 345, 290, 23, 312, 63, 161, 246, 441, 225, 394, 280, 130, 116, 76, 75, 76, 178, 353, 345, 290, 394, 251, 63, 394, 118, 118, 225, 76, 271, 23, 145], [307, 223, 330, 246, 225, 63, 75, 76, 345, 290, 278, 246, 144, 225, 63, 345, 290, 278, 307, 246, 144, 225, 280, 408, 63, 75, 345, 290, 280, 76, 63, 246, 348, 387, 278, 145], [320, 223, 330, 320, 267, 345, 290, 63, 161, 246, 186, 310, 290, 75, 76, 76, 225, 280, 75, 76, 254, 76, 388, 267, 345, 290, 186, 310, 280, 271, 320, 267, 345, 290, 186, 271, 320, 348, 63, 161, 246, 348, 348, 394, 186, 310, 290, 75, 394, 320, 394, 320, 225, 249, 75, 254, 76, 145], [223, 330, 380, 278, 246, 441, 345, 225, 63, 161, 310, 268, 310, 290, 345, 75, 76, 254, 380, 278, 246, 441, 345, 225, 63, 310, 268, 310, 290, 345, 75, 254, 380, 278, 246, 307, 441, 345, 144, 130, 441, 136, 307, 63, 161, 394, 251, 246, 310, 268, 310, 290, 345, 307, 394, 75, 76, 249, 380, 251, 394, 249, 280, 225, 254, 145], [223, 330, 132, 308, 71, 268, 72, 345, 290, 310, 268, 310, 456, 225, 308, 71, 268, 72, 345, 290, 310, 268, 310, 456, 225, 308, 71, 268, 307, 441, 408, 441, 433, 72, 345, 290, 310, 268, 310, 456, 441, 441, 249, 280, 276, 145], [223, 3, 330, 132, 225, 441, 388, 71, 246, 75, 76, 116, 144, 225, 130, 441, 388, 71, 246, 441, 388, 71, 307, 433, 408, 441, 307, 246, 76, 116, 76, 394, 225, 249, 145], [106, 116, 226, 142, 401, 401, 1, 161, 84, 105, 95, 310, 163, 99, 401, 63, 432, 246, 105, 116, 154, 142, 446, 401, 401, 84, 106, 95, 310, 163, 63, 432, 246, 106, 116, 154, 142, 401, 286, 164, 401, 105, 161, 95, 310, 142, 446, 251, 142, 226, 25, 163, 99, 249, 312, 251, 13, 13, 298, 358, 275, 25, 275, 106, 424, 368, 164, 154, 63, 401, 105, 116, 154, 432, 246, 299, 13, 106, 424, 15, 64, 224, 226, 145], [226, 16, 401, 161, 345, 142, 401, 432, 163, 99, 63, 401, 432, 116, 345, 142, 446, 401, 432, 138, 163, 63, 432, 116, 226, 163, 401, 161, 345, 378, 1, 161, 251, 243, 142, 446, 345, 250, 286, 164, 401, 286, 164, 432, 28, 163, 99, 28, 1, 251, 142, 446, 1, 161, 25, 224, 25, 299, 13, 338, 25, 17, 358, 294, 275, 25, 358, 226, 275, 358, 402, 226, 424, 271, 226, 226, 275, 226, 64, 63, 28, 250, 401, 28, 432, 28, 116, 28, 64, 299, 13, 338, 64, 224, 359, 224, 359, 346, 224, 359, 64, 145], [226, 16, 142, 401, 401, 1, 161, 310, 63, 401, 432, 95, 310, 142, 446, 401, 401, 1, 161, 1, 161, 163, 310, 63, 401, 432, 95, 310, 17, 426, 275, 142, 250, 378, 286, 164, 401, 250, 142, 446, 286, 401, 28, 142, 446, 286, 401, 446, 243, 397, 1, 161, 310, 142, 446, 294, 397, 251, 142, 446, 25, 286, 299, 13, 338, 25, 17, 426, 17, 426, 402, 25, 106, 368, 164, 402, 106, 312, 154, 63, 105, 155, 368, 25, 155, 106, 271, 106, 105, 359, 64, 359, 105, 95, 310, 145], [226, 142, 401, 161, 401, 116, 76, 401, 246, 142, 76, 246, 142, 250, 378, 286, 164, 401, 250, 142, 446, 286, 401, 446, 243, 397, 1, 161, 401, 142, 446, 294, 397, 312, 299, 13, 224, 251, 298, 275, 358, 106, 424, 368, 164, 106, 424, 15, 154, 116, 76, 401, 246, 299, 13, 116, 251, 346, 145], [226, 16, 142, 401, 401, 1, 161, 163, 99, 116, 76, 401, 246, 205, 142, 446, 401, 138, 163, 116, 246, 205, 142, 401, 286, 164, 401, 1, 161, 142, 446, 251, 142, 446, 25, 163, 99, 249, 25, 13, 13, 25, 13, 13, 358, 275, 25, 275, 298, 358, 17, 358, 25, 106, 424, 368, 164, 271, 154, 116, 76, 401, 246, 299, 13, 106, 106, 424, 15, 116, 251, 105, 205, 346, 105, 226, 145], [224, 198, 116, 346, 161, 401, 142, 401, 163, 99, 198, 116, 161, 1, 161, 1, 161, 163, 401, 142, 446, 401, 138, 163, 198, 116, 17, 426, 275, 243, 1, 161, 401, 294, 142, 250, 378, 286, 164, 401, 250, 142, 446, 286, 401, 28, 142, 446, 286, 401, 446, 161, 401, 142, 446, 138, 163, 99, 251, 142, 446, 25, 286, 249, 25, 299, 13, 338, 25, 17, 426, 17, 426, 402, 25, 226, 368, 164, 402, 224, 155, 368, 25, 155, 106, 271, 226, 224, 359, 198, 116, 198, 116, 224, 359, 198, 116, 224, 145], [287, 63, 226, 16, 401, 1, 239, 95, 310, 116, 401, 287, 63, 7, 95, 310, 116, 401, 287, 63, 312, 401, 1, 239, 95, 310, 7, 28, 286, 116, 401, 7, 28, 286, 164, 401, 1, 239, 95, 310, 7, 25, 299, 13, 298, 275, 358, 426, 402, 15, 226, 287, 63, 226, 226, 145], [26, 330, 5, 421, 186, 26, 348, 75, 76, 76, 417, 268, 12, 278, 26, 186, 26, 348, 417, 268, 12, 278, 312, 367, 421, 186, 26, 348, 75, 417, 268, 12, 278, 26, 348, 15, 145], [308, 259, 330, 5, 308, 267, 345, 290, 310, 369, 79, 52, 204, 290, 76, 11, 268, 308, 267, 345, 290, 310, 369, 52, 204, 76, 11, 268, 312, 308, 267, 310, 345, 290, 358, 358, 52, 267, 345, 290, 79, 308, 79, 79, 207, 79, 367, 308, 79, 204, 76, 11, 268, 145], [26, 79, 259, 330, 79, 26, 348, 421, 75, 76, 268, 187, 116, 170, 79, 421, 75, 76, 187, 116, 79, 21, 348, 421, 76, 268, 76, 187, 116, 145], [31, 330, 5, 116, 76, 75, 76, 290, 257, 268, 268, 170, 31, 116, 76, 75, 76, 290, 257, 421, 268, 268, 417, 367, 31, 367, 367, 31, 116, 76, 75, 76, 367, 251, 257, 31, 76, 31, 76, 211, 421, 31, 76, 211, 145], [73, 330, 5, 73, 267, 345, 186, 290, 348, 203, 76, 75, 76, 11, 116, 88, 170, 73, 160, 267, 345, 186, 290, 207, 203, 75, 76, 11, 116, 88, 312, 358, 73, 160, 267, 345, 290, 348, 207, 73, 421, 358, 73, 203, 76, 75, 76, 358, 15, 76, 11, 116, 88, 145], [79, 259, 330, 5, 79, 348, 421, 203, 76, 417, 268, 170, 348, 203, 417, 268, 312, 79, 348, 367, 421, 79, 203, 417, 268, 15, 145], [73, 330, 5, 73, 256, 267, 268, 186, 121, 81, 310, 348, 345, 290, 421, 78, 417, 204, 39, 88, 73, 256, 267, 268, 186, 310, 348, 345, 290, 78, 417, 39, 88, 73, 160, 160, 256, 267, 367, 268, 268, 73, 256, 121, 81, 310, 348, 345, 290, 73, 256, 251, 256, 421, 73, 76, 76, 88, 145], [26, 79, 259, 330, 5, 267, 79, 52, 345, 186, 290, 310, 81, 310, 26, 348, 421, 76, 268, 417, 12, 278, 116, 170, 267, 79, 52, 290, 310, 81, 310, 21, 421, 39, 268, 417, 278, 116, 11, 312, 267, 79, 345, 290, 310, 81, 310, 21, 348, 348, 367, 421, 211, 21, 79, 76, 268, 417, 12, 278, 15, 116, 11, 145], [79, 330, 5, 267, 417, 81, 310, 121, 310, 345, 290, 79, 421, 39, 116, 417, 88, 267, 79, 81, 310, 121, 310, 345, 290, 421, 39, 116, 88, 267, 417, 81, 310, 121, 310, 345, 290, 348, 79, 348, 312, 211, 421, 367, 211, 76, 251, 348, 211, 421, 211, 76, 79, 348, 15, 76, 116, 417, 88, 145], [79, 259, 330, 5, 369, 79, 52, 345, 290, 81, 310, 310, 121, 267, 268, 290, 421, 290, 203, 76, 75, 76, 116, 369, 79, 52, 290, 310, 310, 267, 268, 290, 207, 421, 203, 75, 116, 312, 79, 52, 345, 81, 310, 310, 121, 367, 267, 79, 79, 79, 367, 267, 268, 268, 290, 290, 79, 207, 421, 367, 251, 203, 75, 76, 15, 76, 145], [26, 268, 335, 330, 268, 26, 348, 26, 268, 370, 26, 348, 289, 268, 335, 335, 268, 335, 268, 335, 275, 358, 26, 348, 335, 55, 145], [31, 268, 335, 330, 268, 441, 161, 345, 290, 310, 31, 441, 267, 268, 441, 161, 345, 290, 310, 31, 441, 267, 267, 289, 268, 370, 335, 441, 161, 345, 290, 310, 31, 441, 161, 335, 441, 335, 161, 335, 267, 55, 275, 425, 387, 345, 145], [59, 79, 268, 335, 330, 116, 76, 59, 348, 79, 76, 326, 268, 161, 142, 274, 50, 267, 116, 76, 59, 348, 79, 326, 268, 142, 267, 116, 76, 59, 348, 79, 326, 268, 289, 268, 268, 335, 370, 211, 335, 275, 358, 426, 79, 251, 268, 335, 251, 267, 161, 142, 49, 118, 335, 161, 142, 118, 335, 267, 55, 335, 76, 326, 348, 145], [268, 116, 374, 268, 268, 374, 268, 268, 335, 374, 268, 358, 382, 268, 335, 374, 251, 268, 335, 268, 268, 335, 335, 335, 251, 145], [254, 268, 335, 330, 268, 254, 76, 161, 142, 50, 267, 268, 370, 142, 267, 348, 289, 268, 335, 335, 254, 268, 335, 335, 254, 211, 76, 268, 335, 161, 142, 50, 335, 161, 142, 50, 142, 50, 268, 335, 166, 267, 335, 55, 76, 275, 425, 145], [268, 335, 330, 382, 268, 50, 76, 310, 270, 345, 290, 142, 161, 382, 268, 49, 118, 274, 49, 118, 345, 290, 142, 348, 358, 382, 275, 268, 370, 211, 335, 49, 118, 310, 270, 345, 290, 142, 161, 268, 335, 161, 142, 49, 118, 251, 268, 335, 358, 55, 348, 145], [300, 268, 335, 330, 268, 254, 76, 300, 348, 76, 270, 267, 142, 274, 50, 161, 270, 268, 254, 300, 348, 348, 267, 142, 268, 370, 211, 335, 426, 275, 268, 335, 370, 268, 211, 300, 348, 254, 387, 270, 426, 166, 142, 50, 270, 300, 335, 166, 161, 142, 50, 267, 268, 335, 275, 425, 267, 387, 270, 145], [20, 273, 51, 20, 401, 401, 432, 148, 246, 345, 310, 89, 63, 63, 20, 55, 401, 246, 345, 310, 63, 63, 20, 401, 401, 432, 445, 246, 445, 345, 310, 89, 445, 63, 273, 25, 55, 159, 25, 63, 145], [20, 273, 51, 20, 142, 432, 148, 20, 142, 432, 148, 89, 235, 20, 249, 142, 432, 89, 273, 25, 367, 58, 273, 159, 145], [42, 20, 273, 51, 20, 142, 20, 142, 42, 20, 445, 142, 63, 367, 25, 367, 42, 159, 145], [84, 273, 51, 142, 458, 142, 401, 246, 63, 432, 148, 161, 95, 310, 310, 84, 63, 84, 142, 458, 446, 401, 246, 25, 63, 432, 148, 95, 310, 310, 84, 63, 458, 401, 246, 142, 446, 445, 63, 432, 161, 95, 310, 310, 246, 25, 249, 84, 25, 249, 84, 273, 25, 58, 273, 159, 273, 84, 273, 145], [20, 273, 51, 20, 246, 432, 148, 161, 310, 345, 20, 246, 432, 310, 345, 20, 142, 235, 20, 142, 249, 249, 310, 345, 25, 367, 58, 273, 25, 145], [273, 51, 161, 310, 246, 63, 142, 458, 142, 161, 246, 63, 142, 458, 446, 404, 445, 161, 310, 63, 246, 142, 458, 142, 246, 142, 446, 367, 273, 25, 367, 55, 273, 273, 25, 404, 145], [20, 273, 51, 20, 142, 432, 148, 266, 20, 432, 148, 266, 235, 20, 249, 142, 432, 249, 266, 89, 367, 273, 25, 58, 25, 159, 55, 273, 20, 404, 145, 25, 273, 273], [84, 273, 51, 20, 142, 266, 95, 310, 84, 20, 142, 266, 95, 310, 84, 345, 235, 20, 249, 142, 367, 273, 25, 58, 25, 159, 55, 273, 404, 145], [300, 330, 161, 300, 348, 254, 76, 370, 161, 300, 254, 370, 161, 18, 135, 18, 452, 402, 338, 135, 161, 135, 358, 348, 135, 76, 15, 76, 119, 55, 370, 145], [26, 79, 300, 330, 132, 79, 26, 348, 49, 254, 76, 268, 26, 79, 26, 348, 49, 254, 268, 348, 49, 358, 26, 348, 79, 254, 268, 370, 145], [49, 300, 330, 132, 14, 417, 254, 76, 417, 254, 18, 23, 290, 26, 348, 79, 254, 76, 268, 14, 294, 417, 348, 254, 417, 18, 23, 290, 157, 26, 79, 254, 268, 268, 271, 358, 14, 294, 358, 300, 14, 294, 417, 348, 254, 417, 254, 18, 23, 26, 79, 300, 14, 294, 26, 348, 79, 254, 268, 15, 76, 300, 23, 290, 157, 26, 79, 300, 268, 330, 300, 26, 79, 300, 79, 254, 145], [300, 119, 330, 132, 300, 73, 119, 330, 73, 119, 55, 73, 254, 345, 73, 294, 163, 315, 73, 163, 315, 73, 100, 254, 270, 23, 345, 358, 73, 251, 251, 90, 73, 251, 119, 119, 55, 300, 348, 300, 15, 76, 145], [79, 300, 330, 267, 345, 290, 270, 157, 300, 119, 254, 76, 268, 417, 267, 267, 345, 290, 270, 157, 254, 268, 417, 267, 345, 290, 270, 157, 348, 271, 267, 300, 119, 119, 348, 300, 119, 254, 268, 271, 119, 300, 417, 145], [300, 330, 300, 135, 267, 417, 348, 254, 76, 23, 300, 267, 417, 254, 23, 185, 300, 267, 267, 135, 90, 300, 185, 267, 185, 300, 300, 135, 135, 348, 76, 23, 300, 185, 300, 76, 119, 300, 76, 119, 145], [300, 330, 161, 345, 401, 441, 456, 436, 267, 417, 254, 76, 23, 401, 441, 456, 436, 267, 417, 254, 23, 100, 161, 345, 401, 441, 456, 267, 332, 251, 251, 100, 135, 332, 452, 402, 135, 18, 18, 358, 332, 163, 315, 100, 417, 267, 345, 300, 396, 452, 402, 119, 18, 135, 135, 135, 90, 135, 90, 348, 135, 119, 370, 254, 348, 23, 300, 290, 268, 300, 358, 300, 396, 271, 76, 119, 145], [343, 11, 268, 439, 356, 333, 262, 294, 343, 345, 343, 441, 156, 187, 268, 417, 345, 345, 345, 345, 345, 345, 345, 345, 124, 11, 11, 11, 11, 294, 268, 251, 11, 251, 187, 337, 217, 207, 217, 217, 207, 217, 207, 11, 207, 217, 187, 251, 439, 439, 36, 439, 187, 36, 36, 357, 356, 375, 348, 417, 417, 417, 11, 109, 212, 212, 251, 109, 187, 91, 343, 345, 343, 343, 156, 347, 124, 370, 91, 36, 390, 91, 370, 343, 390, 91, 390, 343, 343, 390, 343, 343, 343, 169, 370, 169, 343, 267, 343, 343, 393, 169, 343, 343, 343, 343, 343, 343, 169, 267, 343, 267, 343, 174, 343, 333, 262, 262, 343, 187, 109, 333, 262, 187], [428, 439, 375, 348, 176, 180, 306, 155, 180, 370], [403, 152, 41, 403, 152, 41, 109, 212, 357, 356, 267, 428, 428, 152, 370, 370, 152, 124, 428, 370, 41, 41, 428, 356, 267, 155, 267, 428, 109, 152, 402, 152, 41, 38, 156, 428, 41, 156, 156, 306, 152, 41, 4, 124, 4, 38, 370, 156, 370, 428, 41, 38, 124, 370, 174, 174, 155, 71, 18, 306, 18, 370, 403, 71, 18, 38, 306, 370, 294, 370, 370, 343, 155, 155, 403, 18, 306, 18, 120, 71, 18, 428, 370, 403, 370, 4, 306, 4, 370, 174, 370, 155, 370, 428, 370, 370, 174, 370, 212, 120, 18, 428, 370, 120, 174, 11, 370, 11, 155, 11, 101, 306, 370, 403, 101, 18, 176, 428, 370, 399, 347, 370, 375, 348, 439], [109, 212, 357, 356, 267, 403, 358, 109, 109, 428, 267, 109, 294, 403, 428, 267, 428, 109, 428, 399, 109, 399, 428, 428, 357, 356, 267, 399, 109, 428, 156, 428, 428, 347, 156, 428, 358, 358, 109, 428, 358, 263, 337, 428, 263, 337, 337, 337, 337, 294, 124, 212, 71, 428, 375, 348, 133], [403, 152, 180, 441, 345, 401, 267, 152, 403, 152, 180, 343, 152, 441, 345, 401, 152, 156, 343, 152, 343, 152, 13, 267, 13, 13, 0, 267, 13, 441, 156, 13, 13, 402, 343, 13, 156, 399, 343, 306, 399, 152, 399, 347, 347, 11, 375, 0, 375, 348, 0, 375, 348, 0, 306, 370, 11, 375, 348, 155, 343, 370, 11, 343], [403, 109, 428, 347, 294, 428, 156, 428, 156, 428, 428, 41, 109, 428, 109, 428, 393, 267, 370], [428, 11, 124, 370, 267, 217, 207, 217, 268, 375, 348, 428, 370, 370, 11, 267, 217, 207, 11, 428, 268, 345, 375, 348, 133, 217], [403, 333, 333, 435, 435, 401, 345, 343, 11, 294, 263, 399, 348, 375, 439, 357, 333, 333, 441, 333, 333, 435, 401, 345, 401, 345, 333, 435, 251, 333, 343, 11, 263, 375, 348, 439, 348, 439, 439, 375, 348, 348], [435, 375, 348, 267, 186, 290, 451, 348, 268, 310, 345, 290, 149, 391, 435, 375, 348, 267, 186, 290, 451, 348, 268, 310, 345, 290, 391, 230, 240, 312, 185, 267, 185, 183, 230], [81, 217, 391, 330, 132, 364, 391, 88, 207, 441, 345, 81, 310, 186, 121, 345, 290, 207, 364, 88, 207, 441, 345, 81, 310, 186, 121, 345, 290, 92, 173, 207, 217, 88, 207, 441, 345, 40, 81, 310, 121, 345, 290, 173, 207, 81, 217, 391, 391, 185, 185, 185, 391, 185, 391, 185, 391, 391, 240, 399, 240, 88, 217, 348, 88, 145], [391, 330, 222, 391, 375, 348, 290, 267, 391, 375, 348, 290, 173, 92, 267, 391, 375, 348, 290, 173, 13, 13, 391, 230, 391, 230, 93, 267, 391, 391, 145], [63, 391, 330, 132, 391, 345, 290, 267, 63, 391, 345, 267, 92, 384, 384, 312, 353, 391, 345, 290, 449, 323, 358, 327, 391, 267, 267, 267, 391, 159, 63, 173, 391, 24, 391, 63, 24, 159, 24, 391, 391, 449, 323, 391, 207, 391, 399, 370, 145], [149, 391, 330, 222, 391, 345, 290, 267, 63, 149, 391, 345, 290, 267, 63, 384, 384, 312, 353, 391, 345, 290, 449, 323, 358, 327, 391, 267, 391, 159, 63, 173, 391, 24, 391, 63, 24, 159, 24, 391, 391, 449, 323, 391, 207, 391, 399, 370, 145], [391, 132, 53, 268, 391, 267, 333, 345, 290, 310, 268, 268, 267, 333, 310, 268, 310, 312, 345, 290, 310, 268, 310, 53, 268, 391, 93, 53, 268, 267, 391, 53, 268, 391, 333, 145], [358, 391, 222, 267, 345, 290, 364, 391, 63, 267, 345, 290, 92, 364, 391, 63, 312, 358, 267, 353, 345, 290, 353, 391, 345, 290, 358, 391, 327, 63, 145], [104, 391, 222, 364, 391, 268, 375, 348, 267, 176, 33, 290, 333, 364, 268, 375, 348, 267, 176, 92, 33, 290, 333, 364, 391, 399, 375, 348, 267, 176, 268, 240, 104, 104, 268, 104, 309, 290, 333, 145], [391, 345, 186, 290, 267, 353, 391, 345, 290, 185, 312, 185, 391, 185, 391, 267, 185, 391, 391], [391, 366, 391, 10, 345, 290, 267, 19, 391, 294, 267, 19, 366, 294, 391, 10, 19, 366, 391, 10, 267, 366, 345, 290, 159, 353, 345, 290, 145], [391, 366, 132, 391, 121, 81, 310, 310, 345, 290, 33, 290, 33, 290, 268, 19, 391, 81, 310, 310, 345, 290, 33, 290, 294, 33, 290, 294, 268, 294, 19, 366, 441, 185, 391, 121, 81, 310, 310, 345, 290, 391, 294, 366, 294, 290, 290, 268, 366, 366, 402, 185, 183, 366, 145], [391, 330, 132, 399, 391, 345, 290, 267, 399, 391, 391, 345, 267, 10, 353, 391, 345, 290, 267, 367, 391, 391, 327, 391, 10, 345, 290, 391, 370, 10, 145], [63, 92, 364, 290, 312, 185, 63, 310, 391, 345, 290, 391, 63], [375, 348, 435, 186, 176, 310, 267, 40, 375, 348, 435, 176, 310, 267, 391, 391, 309, 230, 391, 312, 185, 183, 267, 391, 185, 230, 391], [267, 451, 348, 268, 290, 348, 149, 267, 451, 348, 375, 348, 268, 345, 290, 391, 348, 40, 391, 309, 230, 391, 185, 391, 230, 230, 267, 402, 391, 185, 391], [364, 391, 401, 290, 310, 312, 185, 402, 391, 391, 348, 401, 309, 348, 391, 345, 290, 310, 391, 345, 290, 310, 391, 230, 309, 230, 267, 185, 183, 391, 24, 391, 230], [435, 290, 208, 345, 267, 391, 63, 83, 290, 435, 290, 345, 267, 391, 13, 230, 13, 185, 267, 183, 391, 230, 230, 391, 185, 63, 83, 290, 391, 185, 327, 391, 63], [63, 83, 290, 345, 268, 310, 186, 290, 391, 267, 312, 185, 63, 83, 290, 345, 268, 310, 290, 391, 267, 93, 240, 267, 185, 183, 240, 185, 185, 185, 370, 399, 63], [403, 333, 435, 401, 345, 333, 441, 358, 154, 120, 120, 237, 237, 290, 237, 268, 237, 348, 294, 127, 237, 127, 11, 207, 19, 403, 236, 333, 435, 401, 345, 333, 362, 136, 333, 441, 349, 44, 425, 349, 425, 441, 333, 333, 251, 333, 275, 333, 333, 425, 337, 403, 236, 376, 358, 236, 155, 441, 313, 333, 263, 154, 333, 333, 236, 263, 333, 154, 126, 236, 337, 236, 337, 236, 263, 36, 441, 403, 403, 337, 333, 154, 403, 337, 294, 375, 348], [375, 348, 333, 447, 435, 176, 356, 356, 267, 238, 427, 267, 348, 375, 348, 333, 435, 176, 220, 251, 207, 220, 348, 403, 337, 357, 356, 251, 357, 356, 267, 249, 403, 337, 348, 419, 238, 267, 403, 337, 391, 364, 391, 155, 391, 155, 391, 411, 124, 211, 391, 411, 357, 356, 267, 364, 352, 345, 387, 402, 391, 244, 418, 152, 433, 357, 356, 267, 10, 256, 10, 256, 352, 345, 418, 256, 267, 403, 337, 267, 403, 337, 358, 155, 403, 333, 358, 333, 358, 19, 236, 459, 13, 333, 236, 358, 155, 333, 236, 306, 358, 244, 418, 211, 256, 10, 358, 337, 236, 155, 155, 314, 19, 236, 358, 413, 155, 337, 314, 364, 364, 306, 158, 370, 411, 391, 294, 370, 403, 337, 403, 337, 155, 370, 364, 306, 337, 403, 337, 337, 155, 236, 403, 337, 155, 441, 38, 124, 403, 337, 124, 294, 403, 91, 294, 403, 337, 306, 294, 403, 4, 322, 306, 18, 176, 18, 439, 348, 403, 337, 370, 420, 348, 420, 402, 402, 370, 295, 385, 268, 427, 267, 238, 295, 357, 356, 370], [403, 343, 191, 328, 127, 343, 336, 11, 11, 396, 11, 396, 396, 396, 11, 413, 11, 294, 11, 306, 11, 4, 11, 155, 370, 411, 38, 155, 11, 11, 370, 339, 91, 339, 370, 339, 370, 339, 403, 337, 337, 306, 294, 403, 343, 152, 339, 339, 91, 11, 152, 91, 314, 155, 403, 337, 403, 337, 294, 328, 306, 328, 362, 244, 328, 362, 207, 362, 207, 57, 57, 328, 402, 86, 11, 337, 294, 328, 57, 306, 370, 86, 343, 124, 294, 127, 322, 38, 322, 11, 337, 86, 343, 336, 370, 86, 314], [403, 333, 358, 120, 370, 11, 427, 154, 358, 403, 333, 263, 155, 333, 333, 358, 263, 402, 425, 333, 358, 152, 358, 120, 120, 358, 337, 403, 337, 19, 236, 413, 358, 337, 358, 154, 337, 19, 236, 459, 13, 337, 236, 337, 313, 155, 337, 337, 337, 370, 11, 403, 337, 376, 370, 337, 91, 155, 91, 441, 337, 337, 337, 124, 19, 236, 91, 419, 63, 370, 403, 370, 420, 427, 420, 402, 370, 370, 336, 370, 370, 279, 267, 279, 267, 36, 251, 403, 370, 306, 370, 403, 4, 376, 370, 433, 120, 71, 370, 424, 294, 439], [113, 393, 333, 358, 370, 370, 357, 356, 113, 113, 393, 393, 337, 238, 113, 393, 238, 249, 333, 358, 358, 403, 333, 358, 155, 333, 358, 387, 356, 333, 333, 333, 19, 236, 459, 13, 358, 236, 113, 251, 358, 370, 120, 370, 376, 358, 337, 236, 154, 154, 413, 337, 314, 236, 337, 236, 36, 337, 337, 236, 403, 337, 337, 91, 337, 337, 91, 236, 91, 337, 91, 337, 333, 337, 403, 294, 403, 337, 333, 38, 337, 337, 370, 403, 337, 38, 314, 403, 337, 337, 403, 294, 176, 439, 375, 348], [403, 333, 358, 10, 393, 336, 407, 25, 403, 358, 263, 13, 236, 403, 333, 358, 155, 333, 358, 393, 10, 358, 211, 10, 156, 142, 403, 337, 10, 399, 347, 343, 403, 337, 358, 19, 236, 155, 358, 393, 337, 403, 403, 337, 403, 337, 407, 25, 25, 403, 337, 25, 403, 337, 333, 407, 25, 337, 337, 25, 337, 169, 343, 418, 275, 267, 403, 337, 403, 337, 441, 38, 333, 25, 124, 337, 294, 403, 403, 337, 403, 337, 403, 337, 439, 176, 348], [337, 330, 132, 403, 333, 333, 435, 403, 263, 357, 356, 199, 113, 120, 333, 333, 435, 358, 357, 356, 294, 199, 120, 237, 353, 403, 333, 333, 435, 236, 358, 263, 151, 431, 191, 333, 263, 333, 263, 333, 353, 333, 295, 357, 356, 263, 431, 9, 339, 47, 113, 339, 11, 339, 120, 236, 236, 154, 376, 424, 337, 337, 337, 275, 425, 337, 305, 145], [403, 330, 132, 142, 401, 345, 333, 435, 333, 120, 343, 428, 375, 348, 439, 176, 142, 401, 345, 333, 435, 403, 333, 358, 120, 343, 399, 428, 399, 375, 348, 439, 176, 142, 142, 47, 211, 142, 220, 401, 345, 333, 435, 403, 435, 333, 333, 358, 263, 441, 403, 333, 263, 333, 358, 399, 358, 263, 403, 236, 399, 120, 263, 120, 399, 403, 333, 120, 399, 343, 399, 263, 343, 399, 428, 337, 294, 343, 399, 333, 263, 343, 399, 120, 294, 403, 294, 120, 403, 263, 337, 337, 370, 348, 439, 176, 145], [403, 337, 330, 403, 333, 357, 356, 379, 97, 336, 355, 120, 435, 401, 333, 357, 356, 379, 97, 336, 358, 120, 435, 401, 435, 401, 345, 333, 394, 333, 357, 356, 394, 333, 379, 97, 336, 394, 358, 355, 124, 358, 19, 236, 97, 336, 333, 355, 337, 126, 337, 337, 19, 236, 19, 236, 337, 126, 294, 120, 97, 337, 124, 358, 126, 358, 355, 357, 333, 337, 333, 358, 403, 145], [403, 330, 132, 333, 441, 401, 345, 333, 435, 428, 238, 389, 348, 199, 113, 238, 79, 409, 348, 120, 71, 11, 403, 263, 375, 348, 439, 176, 333, 441, 401, 345, 333, 435, 428, 238, 389, 348, 199, 113, 238, 79, 348, 120, 71, 11, 403, 355, 375, 348, 439, 176, 333, 441, 307, 333, 441, 401, 345, 435, 401, 333, 435, 333, 435, 124, 333, 333, 370, 428, 240, 249, 238, 389, 348, 79, 409, 348, 79, 370, 199, 113, 294, 249, 238, 434, 370, 294, 333, 403, 19, 236, 441, 333, 337, 358, 263, 191, 333, 263, 333, 263, 333, 376, 47, 263, 333, 339, 333, 236, 236, 376, 424, 337, 236, 333, 337, 337, 337, 425, 333, 337, 454, 236, 454, 337, 434, 337, 409, 79, 337, 337, 145], [337, 330, 132, 403, 333, 333, 435, 403, 263, 343, 116, 76, 120, 333, 333, 435, 358, 116, 76, 237, 120, 237, 353, 403, 333, 333, 435, 358, 263, 151, 431, 191, 333, 263, 333, 263, 333, 9, 339, 47, 343, 116, 76, 120, 236, 236, 154, 376, 424, 337, 337, 337, 275, 425, 337, 305, 145], [434, 337, 330, 132, 403, 333, 333, 435, 403, 263, 11, 120, 33, 290, 356, 333, 333, 435, 358, 11, 120, 237, 33, 290, 237, 353, 403, 333, 333, 435, 358, 263, 151, 431, 191, 333, 263, 387, 356, 333, 263, 333, 263, 431, 9, 339, 47, 33, 290, 120, 370, 236, 236, 154, 376, 424, 337, 337, 337, 275, 425, 337, 305, 145], [434, 403, 132, 333, 435, 401, 345, 333, 333, 267, 360, 256, 375, 348, 310, 290, 120, 71, 267, 161, 310, 345, 290, 198, 441, 308, 11, 11, 120, 228, 409, 416, 33, 290, 71, 439, 176, 333, 435, 401, 345, 333, 333, 358, 267, 360, 256, 348, 310, 290, 120, 393, 71, 393, 267, 310, 345, 290, 198, 441, 308, 393, 370, 370, 120, 393, 228, 393, 416, 399, 33, 393, 71, 393, 176, 375, 348, 307, 333, 435, 401, 345, 445, 44, 345, 401, 307, 403, 333, 333, 333, 435, 251, 403, 333, 333, 249, 435, 251, 360, 434, 337, 357, 267, 267, 360, 256, 375, 348, 310, 290, 256, 207, 358, 263, 403, 236, 403, 333, 263, 441, 155, 333, 155, 333, 211, 46, 337, 263, 120, 71, 256, 155, 433, 263, 19, 236, 337, 403, 337, 403, 337, 423, 403, 294, 308, 407, 337, 267, 307, 161, 310, 345, 290, 249, 198, 441, 249, 308, 393, 25, 155, 308, 393, 267, 275, 425, 358, 263, 403, 236, 403, 333, 263, 155, 333, 211, 46, 337, 263, 308, 407, 11, 155, 433, 263, 19, 236, 337, 403, 337, 403, 337, 423, 403, 294, 120, 228, 337, 358, 263, 403, 236, 333, 263, 155, 333, 211, 46, 337, 263, 11, 120, 228, 155, 154, 263, 19, 236, 337, 403, 337, 403, 337, 423, 403, 294, 409, 416, 337, 358, 263, 403, 236, 333, 263, 155, 333, 211, 46, 337, 337, 409, 416, 33, 290, 71, 155, 154, 263, 19, 236, 337, 403, 337, 403, 337, 423, 403, 294, 403, 439, 176, 348, 145], [333, 333, 435, 401, 345, 333, 441, 358, 154, 120, 120, 237, 11, 207, 375, 348, 19, 403, 236, 337, 333, 435, 401, 345, 333, 441, 136, 333, 441, 349, 44, 333, 425, 349, 425, 333, 441, 333, 333, 251, 333, 275, 358, 333, 333, 425, 337, 403, 236, 376, 358, 236, 155, 441, 313, 333, 263, 154, 333, 333, 236, 263, 333, 154, 126, 236, 337, 236, 337, 236, 263, 36, 441, 403, 403, 275, 425, 333, 154, 403, 337, 294, 375, 348], [361, 407, 357, 356, 345, 267, 361, 379, 348, 238, 348, 333, 447, 435, 207, 361, 407, 445, 161, 198, 25, 445, 357, 356, 345, 207, 25, 155, 161, 169, 267, 267, 349, 267, 361, 25, 169, 136, 275, 425, 358, 387, 345, 361, 169, 379, 348, 445, 238, 389, 333, 447, 435, 207, 389, 361, 379, 348], [333, 403, 333, 333, 447, 435, 401, 238, 238, 333, 447, 435, 375, 348, 403, 13, 355, 358, 154, 399, 343, 428, 290, 11, 385, 268, 403, 439, 176, 375, 348, 337, 333, 358, 236, 441, 191, 333, 211, 238, 211, 238, 333, 154, 238, 154, 428, 120, 337, 236, 333, 314, 337, 333, 57, 441, 337, 454, 236, 337, 403, 370, 91, 403, 211, 343, 91, 211, 91, 333, 313, 313, 333, 156, 91, 403, 155, 91, 18, 419, 419, 294, 343, 18, 333, 419, 18, 91, 18], [358, 333, 403, 343, 156, 116, 76, 399, 348, 355, 403, 355, 211, 403, 333, 333, 333, 333, 403, 294, 343, 294, 156, 337, 156, 156, 403, 343, 403, 337, 152, 294, 399, 333, 11, 116, 76, 337, 337, 19, 236, 275, 454, 403, 337, 294, 403, 337, 375, 348, 176], [333, 347, 343, 11, 71, 10, 127, 336, 124, 294, 127, 358, 124, 124, 425, 244, 366, 425, 366, 433, 366, 47, 433, 125, 358, 124, 47, 10, 441, 352, 345, 10, 441, 71, 71, 124, 294, 294, 399, 402, 370, 370, 370, 399, 124, 370, 370, 399, 11, 11, 158, 11, 294, 347, 343, 124, 294, 343, 294, 124, 337, 294, 263, 236, 441, 435, 333, 263, 36, 439, 10, 36, 370, 71, 336, 337, 337, 36, 279, 333, 358, 337, 358, 358, 294, 337, 294, 337, 279, 294, 145], [113, 63, 176, 11, 403, 333, 358, 113, 113, 213, 155, 113, 240, 213, 240, 113, 213, 294, 113, 240, 353, 213, 352, 345, 213, 345, 290, 63, 63, 113, 240, 418, 418, 113, 176, 362, 220, 251, 11, 356, 356, 11, 370, 124, 38, 207, 370, 403, 337, 358, 403, 333, 358, 155, 333, 19, 236, 459, 13, 358, 236, 333, 176, 399, 358, 113, 213, 176, 413, 19, 236, 358, 314, 155, 403, 337, 236, 306, 124, 370, 403, 337, 306, 155, 11, 370, 403, 337, 38, 155, 337, 125, 403, 337, 236, 91, 403, 337, 236, 155, 11, 337, 236, 11, 337, 314, 11, 337, 236, 91, 403, 337, 294, 337, 294, 236, 370, 91, 370, 4, 322, 379, 238, 370, 336], [73, 421, 405, 330, 132, 73, 217, 270, 81, 310, 345, 290, 73, 81, 310, 345, 290, 73, 73, 345, 73, 425, 73, 249, 207, 290, 270, 310, 251, 145], [79, 405, 330, 222, 79, 33, 290, 33, 290, 33, 290, 268, 267, 81, 310, 121, 345, 290, 110, 421, 116, 170, 79, 33, 290, 33, 290, 33, 290, 268, 267, 81, 310, 121, 345, 290, 110, 421, 116, 367, 267, 422, 268, 33, 290, 345, 81, 310, 121, 290, 350, 268, 290, 79, 422, 251, 421, 251, 421, 116, 145], [88, 217, 79, 405, 330, 79, 116, 88, 267, 345, 110, 421, 170, 116, 217, 88, 92, 267, 345, 292, 344, 110, 421, 367, 267, 422, 173, 350, 79, 345, 116, 207, 249, 79, 251, 88, 251, 421, 251, 421, 292, 145], [308, 405, 330, 132, 308, 116, 173, 121, 345, 270, 110, 421, 267, 116, 344, 76, 170, 308, 116, 92, 121, 345, 270, 110, 421, 267, 116, 344, 88, 308, 308, 308, 249, 116, 173, 121, 345, 290, 270, 421, 425, 425, 421, 308, 251, 421, 421, 267, 308, 405, 267, 267, 425, 145], [129, 308, 405, 330, 132, 308, 63, 173, 116, 345, 290, 421, 76, 110, 405, 359, 207, 170, 308, 63, 92, 116, 290, 421, 76, 110, 405, 359, 207, 292, 308, 173, 63, 116, 308, 251, 251, 308, 345, 290, 129, 405, 359, 421, 76, 76, 110, 405, 359, 421, 421, 129, 405, 308, 207, 129, 405, 292, 145], [405, 88, 348, 330, 222, 361, 310, 270, 121, 290, 345, 366, 267, 110, 421, 116, 88, 345, 207, 65, 65, 88, 170, 310, 121, 345, 366, 267, 110, 421, 116, 217, 345, 207, 65, 65, 361, 310, 270, 121, 290, 345, 422, 366, 361, 366, 361, 366, 361, 361, 366, 267, 267, 366, 405, 88, 348, 65, 65, 116, 251, 65, 348, 405, 405, 110, 421, 65, 251, 361, 88, 348, 88, 207, 145], [79, 405, 330, 132, 79, 268, 81, 310, 345, 268, 88, 110, 421, 170, 268, 92, 81, 310, 345, 268, 88, 110, 421, 367, 267, 422, 268, 345, 350, 81, 310, 79, 249, 79, 251, 421, 251, 421, 268, 88, 145], [79, 405, 330, 222, 79, 268, 270, 186, 121, 290, 345, 110, 421, 170, 79, 268, 92, 186, 121, 290, 345, 110, 367, 267, 422, 268, 345, 350, 270, 121, 79, 79, 251, 251, 421, 251, 421, 145], [142, 458, 401, 234, 142, 446, 116, 98, 209, 95, 310, 28, 142, 458, 401, 234, 76, 142, 446, 116, 401, 98, 209, 98, 251, 209, 211, 211, 251, 209, 211, 251, 387, 95], [398, 102, 401, 234, 116, 219, 209, 310, 398, 34, 34, 398, 102, 76, 116, 401, 219, 28, 250, 378, 209, 146, 98, 383, 251, 209, 211, 398, 209, 146, 251, 398, 209, 116, 401, 28, 250, 378, 116, 211, 209, 95, 116, 398, 309, 398], [142, 458, 401, 116, 432, 234, 98, 95, 310, 142, 458, 401, 251, 116, 432, 286, 234, 251, 98, 209, 98, 251, 209, 47, 234, 251, 209, 116, 211, 387, 95], [142, 458, 401, 116, 209, 98, 95, 310, 84, 142, 458, 401, 441, 444, 458, 234, 458, 28, 444, 116, 286, 251, 47, 98, 219, 116, 209, 98, 219, 211, 95, 84, 122, 84, 122, 84], [401, 142, 146, 401, 209, 310, 28, 401, 142, 458, 250, 142, 251, 28, 28, 142, 446, 401, 250, 286, 142, 446, 234, 251, 309, 146, 209, 146, 209, 383, 234, 251, 209, 211, 95, 310, 209, 146, 234, 251, 95, 310, 209, 146, 234, 251], [98, 219, 116, 76, 116, 209, 95, 310, 98, 116, 76, 98, 251, 116, 432, 286, 116, 116, 76, 209, 47, 98, 251, 116, 251, 211, 387, 95], [67, 67, 98, 310, 98, 98, 102, 401, 98, 116, 102, 401, 98, 95, 310, 84, 67, 25, 98, 25, 67, 98, 98, 219, 250, 378, 234, 102, 401, 219, 28, 250, 378, 116, 102, 401, 219, 116, 251, 251, 67, 67, 211, 67, 98, 251, 67, 251, 98, 67, 211, 67, 98, 251, 211, 383, 67, 211, 67, 309, 98, 251, 383, 67, 95, 67, 122, 84, 122, 84], [432, 437, 98, 441, 116, 76, 401, 116, 142, 444, 95, 310, 383, 437, 47, 98, 441, 437, 98, 28, 116, 76, 401, 250, 116, 142, 378, 25, 98, 441, 25, 437, 437, 98, 25, 444, 95, 310], [56, 438, 330, 56, 249, 432, 116, 69, 348, 56, 249, 432, 116, 69, 348, 56, 25, 438, 202, 56, 432, 116, 69, 348, 145], [438, 330, 51, 142, 345, 290, 382, 142, 345, 290, 382, 142, 345, 290, 249, 438, 202, 382, 142, 142, 142, 438, 202, 145], [79, 51, 404, 345, 290, 290, 142, 246, 432, 267, 161, 310, 79, 294, 345, 142, 246, 432, 267, 310, 401, 404, 394, 79, 345, 290, 142, 246, 432, 267, 161, 310, 401, 345, 394, 79, 438, 25, 438, 202, 294, 79, 25, 79, 438, 55, 25, 348, 145], [89, 337, 438, 89, 337, 135, 89, 337, 135, 89, 337, 438, 202, 55, 145], [273, 249, 330, 51, 266, 246, 142, 432, 310, 345, 404, 401, 266, 246, 142, 432, 310, 345, 404, 266, 246, 142, 432, 266, 310, 345, 273, 25, 273, 438, 25, 438, 202, 438, 202, 273, 438, 404, 145], [438, 330, 51, 49, 142, 370, 49, 142, 370, 370, 370, 49, 438, 202, 142, 142, 76, 370, 49, 49, 55, 145], [254, 438, 254, 76, 161, 142, 382, 231, 348, 246, 254, 142, 118, 382, 231, 348, 438, 254, 370, 370, 161, 142, 118, 254, 161, 142, 49, 118, 254, 254, 438, 202, 382, 254, 438, 202, 49, 118, 55, 76, 231, 145], [438, 51, 249, 310, 267, 142, 288, 116, 401, 116, 310, 267, 142, 288, 116, 55, 401, 116, 345, 249, 142, 267, 310, 394, 69, 348, 438, 202, 249, 202, 288, 69, 249, 438, 202, 438, 288, 69, 116, 145], [6, 293, 330, 131, 89, 337, 142, 246, 89, 432, 148, 63, 371, 6, 401, 432, 198, 116, 337, 135, 142, 246, 89, 432, 371, 6, 401, 432, 198, 116, 312, 89, 337, 135, 399, 399, 294, 294, 89, 337, 142, 246, 89, 148, 63, 6, 401, 401, 89, 337, 135, 294, 142, 251, 6, 55, 116, 145], [104, 6, 330, 131, 6, 293, 154, 67, 249, 63, 432, 198, 116, 6, 293, 63, 6, 293, 154, 249, 63, 104, 15, 432, 198, 116, 116, 145], [6, 293, 330, 16, 293, 119, 116, 401, 116, 432, 148, 186, 89, 186, 265, 142, 6, 441, 401, 186, 89, 186, 293, 119, 116, 401, 116, 432, 148, 186, 89, 186, 142, 6, 441, 401, 186, 89, 186, 312, 293, 119, 116, 401, 432, 89, 116, 142, 251, 15, 271, 107, 349, 6, 401, 89, 112, 441, 6, 6, 293, 154, 293, 119, 399, 142, 15, 55, 145], [84, 330, 131, 56, 249, 116, 76, 401, 142, 432, 148, 116, 48, 56, 249, 76, 401, 142, 432, 116, 312, 56, 25, 309, 386, 15, 116, 76, 401, 142, 432, 28, 56, 77, 25, 47, 77, 116, 309, 394, 84, 77, 77, 145], [248, 77, 131, 116, 76, 116, 401, 432, 142, 116, 76, 116, 401, 432, 218, 47, 218, 28, 116, 76, 401, 432, 116, 251, 15, 387, 145], [96, 79, 123, 330, 79, 268, 351, 88, 123, 310, 278, 345, 290, 267, 268, 310, 351, 96, 246, 290, 54, 88, 79, 268, 351, 92, 310, 290, 267, 268, 92, 310, 351, 246, 290, 88, 79, 268, 351, 173, 88, 123, 310, 278, 345, 290, 267, 268, 173, 123, 310, 394, 351, 96, 246, 96, 246, 79, 394, 79, 353, 290, 79, 54, 307, 394, 44, 307, 363, 348, 88, 145], [31, 448, 330, 5, 79, 448, 345, 290, 26, 348, 31, 26, 348, 448, 345, 26, 348, 31, 348, 55, 401, 79, 448, 345, 290, 382, 448, 26, 348, 31, 448, 26, 348, 448, 401, 145], [228, 79, 448, 330, 5, 161, 81, 310, 310, 290, 345, 79, 448, 228, 290, 207, 196, 161, 81, 310, 310, 290, 345, 448, 228, 290, 207, 196, 312, 161, 81, 310, 310, 290, 345, 448, 275, 425, 251, 448, 17, 358, 15, 228, 290, 173, 207, 39, 251, 196, 228, 251, 348, 448, 348, 358, 15, 348, 145], [79, 448, 330, 5, 79, 448, 173, 345, 290, 207, 161, 267, 79, 448, 290, 207, 267, 173, 79, 173, 345, 290, 207, 79, 448, 161, 267, 79, 448, 55, 267, 275, 425, 173, 145], [84, 330, 84, 67, 249, 246, 69, 116, 48, 84, 67, 25, 201, 246, 116, 272, 69, 84, 67, 201, 195, 67, 246, 67, 195, 69, 116, 67, 69, 419, 145], [15, 84, 67, 330, 131, 84, 192, 383, 179, 114, 84, 318, 246, 116, 76, 401, 432, 84, 84, 167, 179, 114, 84, 84, 192, 383, 114, 272, 84, 246, 401, 432, 84, 84, 167, 179, 114, 84, 84, 192, 383, 114, 179, 114, 84, 192, 251, 179, 114, 179, 114, 84, 192, 67, 318, 246, 445, 116, 76, 401, 432, 249, 84, 116, 251, 318, 251, 394, 318, 251, 84, 192, 179, 114, 179, 114, 84, 192, 179, 114, 318, 251, 383, 67, 67, 84, 167, 179, 114, 84, 145], [398, 84, 253, 330, 131, 84, 246, 432, 148, 345, 84, 246, 432, 345, 398, 246, 246, 246, 84, 432, 345, 84, 398, 145], [142, 330, 5, 142, 345, 290, 417, 12, 238, 257, 11, 76, 142, 290, 417, 12, 11, 12, 142, 307, 441, 44, 365, 441, 198, 441, 142, 458, 446, 458, 345, 290, 417, 12, 238, 11, 142, 251, 76, 12, 145], [140, 441, 175, 310, 375, 348, 395, 310, 161, 310, 441, 186, 304, 268, 360, 256, 176, 173, 401, 342, 267, 375, 348, 267, 441, 267, 357, 356, 268, 247, 441, 175, 310, 375, 348, 79, 395, 310, 441, 186, 304, 268, 360, 256, 176, 173, 401, 357, 267, 375, 348, 267, 247, 441, 357, 267, 357, 356, 268, 441, 175, 375, 348, 395, 249, 374, 374, 161, 310, 441, 251, 135, 135, 154, 186, 304, 268, 360, 256, 176, 173, 401, 342, 267, 348, 135, 294, 294, 338, 381, 154, 374, 175, 135, 140, 135, 154, 267, 140, 441, 140, 267, 47, 140, 357, 356, 268, 145], [415, 330, 51, 142, 246, 404, 432, 148, 186, 89, 345, 49, 401, 142, 246, 404, 432, 186, 89, 345, 370, 49, 401, 445, 142, 246, 404, 432, 89, 345, 445, 49, 370, 49, 401, 404, 145], [415, 330, 51, 142, 116, 401, 432, 148, 49, 142, 116, 401, 432, 370, 398, 142, 116, 401, 432, 148, 445, 370, 49, 249, 370, 55, 275, 193, 275, 275, 193, 370, 49, 193, 49, 275, 370, 49, 49, 193, 370, 275, 193, 401, 275, 193, 145], [308, 333, 330, 308, 267, 268, 71, 345, 290, 54, 285, 142, 373, 375, 348, 268, 170, 308, 267, 441, 268, 71, 345, 290, 54, 285, 142, 373, 375, 268, 333, 367, 267, 268, 333, 71, 345, 290, 54, 220, 308, 408, 285, 142, 375, 348, 268, 145], [79, 333, 330, 79, 333, 330, 170, 267, 345, 52, 333, 348, 268, 450, 267, 394, 142, 168, 142, 450, 450, 267, 79, 52, 168, 333, 433, 147, 375, 348, 394, 142, 385, 268, 145], [398, 457, 330, 372, 398, 20, 457, 398, 20, 457, 163, 315, 165, 419, 145], [398, 20, 457, 330, 372, 398, 20, 457, 398, 20, 457, 398, 457, 163, 315, 398, 165, 145], [79, 49, 330, 5, 79, 161, 142, 267, 14, 254, 76, 231, 348, 63, 278, 170, 79, 161, 142, 267, 14, 254, 370, 63, 92, 278, 277, 370, 79, 399, 161, 142, 118, 79, 399, 161, 399, 142, 49, 118, 79, 267, 79, 399, 55, 14, 14, 294, 79, 399, 200, 14, 294, 370, 254, 79, 399, 254, 79, 79, 14, 294, 400, 14, 294, 370, 358, 162, 63, 63, 14, 370, 370, 162, 14, 15, 162, 231, 145], [391, 400, 49, 330, 5, 267, 391, 345, 186, 290, 63, 14, 63, 277, 76, 278, 170, 267, 391, 345, 290, 63, 92, 14, 63, 370, 391, 345, 290, 267, 267, 391, 63, 63, 63, 391, 391, 391, 399, 391, 14, 200, 312, 391, 14, 391, 14, 358, 63, 278, 63, 391, 400, 14, 15, 391, 370, 145], [62, 330, 132, 80, 268, 278, 207, 121, 345, 290, 290, 157, 267, 173, 278, 186, 121, 208, 345, 406, 214, 207, 268, 435, 208, 345, 441, 401, 257, 65, 435, 345, 417, 190, 60, 268, 278, 92, 214, 207, 345, 290, 290, 157, 267, 92, 278, 186, 121, 345, 214, 207, 268, 435, 208, 345, 441, 401, 65, 435, 417, 190, 60, 406, 348, 173, 278, 121, 406, 207, 441, 441, 348, 116, 353, 345, 80, 268, 278, 207, 80, 251, 268, 268, 435, 441, 401, 345, 401, 268, 268, 268, 65, 251, 284, 367, 267, 62, 345, 406, 348, 60, 62, 60, 417, 370, 268, 65, 145], [32, 197, 330, 372, 172, 27, 345, 186, 121, 173, 32, 207, 436, 267, 441, 27, 345, 186, 92, 32, 214, 207, 436, 267, 441, 80, 8, 425, 425, 27, 368, 27, 163, 315, 345, 121, 173, 32, 315, 251, 163, 315, 341, 309, 207, 267, 315, 340, 251, 85, 434, 145], [197, 71, 372, 80, 267, 214, 186, 121, 345, 71, 80, 267, 92, 186, 121, 345, 71, 80, 267, 214, 207, 173, 121, 345, 163, 315, 39, 71, 197, 145], [79, 210, 330, 132, 345, 290, 161, 142, 118, 267, 334, 76, 231, 348, 210, 262, 254, 76, 76, 23, 79, 161, 142, 118, 267, 334, 262, 79, 353, 345, 290, 161, 142, 118, 142, 79, 161, 142, 118, 267, 367, 79, 334, 76, 142, 231, 348, 47, 262, 461, 334, 251, 254, 76, 211, 348, 254, 76, 162, 15, 210, 23, 145], [337, 132, 30, 268, 417, 270, 23, 345, 290, 334, 76, 254, 76, 76, 278, 142, 358, 170, 30, 268, 417, 92, 23, 345, 334, 254, 277, 278, 142, 358, 30, 268, 417, 173, 270, 23, 353, 36, 345, 36, 290, 348, 334, 254, 277, 278, 142, 353, 345, 290, 358, 426, 417, 30, 251, 358, 36, 211, 251, 30, 251, 337, 30, 76, 277, 251, 145], [223, 3, 330, 225, 116, 76, 75, 76, 116, 142, 345, 267, 76, 75, 76, 116, 142, 345, 267, 276, 225, 130, 136, 441, 312, 76, 76, 116, 142, 345, 225, 267, 251, 387, 76, 276, 145], [223, 330, 308, 71, 268, 72, 310, 268, 310, 345, 456, 225, 308, 71, 268, 72, 310, 268, 310, 345, 456, 308, 71, 268, 433, 307, 271, 441, 308, 408, 433, 441, 72, 433, 310, 268, 310, 345, 441, 456, 348, 225, 394, 387, 276, 145], [76, 223, 330, 225, 246, 142, 63, 75, 76, 254, 76, 76, 316, 76, 76, 345, 144, 246, 142, 63, 254, 76, 316, 76, 76, 345, 312, 307, 225, 441, 345, 136, 307, 63, 225, 249, 353, 345, 290, 75, 76, 254, 316, 178, 76, 249, 142, 246, 249, 75, 76, 271, 145], [226, 16, 1, 239, 401, 142, 401, 95, 310, 205, 163, 99, 84, 116, 76, 1, 239, 401, 142, 446, 401, 310, 205, 163, 84, 116, 76, 312, 239, 401, 249, 250, 142, 401, 142, 446, 28, 286, 164, 401, 1, 239, 142, 446, 25, 95, 310, 205, 163, 99, 25, 299, 13, 298, 275, 358, 15, 84, 116, 76, 84, 154, 299, 13, 298, 154, 224, 359, 226, 226, 145], [391, 259, 330, 5, 391, 345, 290, 81, 310, 310, 121, 267, 421, 290, 290, 268, 203, 76, 75, 76, 187, 116, 290, 310, 310, 267, 421, 290, 290, 268, 203, 75, 187, 116, 312, 391, 345, 290, 81, 310, 310, 121, 267, 391, 391, 367, 421, 391, 290, 268, 203, 75, 15, 145], [257, 79, 330, 5, 369, 79, 180, 257, 267, 345, 290, 268, 75, 76, 203, 76, 180, 257, 257, 267, 78, 417, 11, 116, 180, 257, 267, 345, 290, 268, 75, 203, 180, 257, 257, 267, 78, 417, 11, 116, 358, 15, 180, 257, 345, 290, 267, 367, 79, 268, 268, 79, 367, 211, 79, 257, 79, 268, 75, 203, 76, 417, 11, 116, 145], [268, 335, 330, 345, 310, 268, 161, 142, 274, 50, 267, 441, 345, 310, 268, 142, 118, 267, 345, 310, 268, 370, 211, 335, 275, 426, 268, 335, 426, 166, 142, 49, 118, 335, 166, 161, 142, 118, 142, 118, 267, 268, 335, 275, 425, 267, 387, 345, 116, 145], [42, 273, 51, 20, 266, 142, 432, 148, 310, 89, 42, 20, 266, 432, 148, 310, 89, 42, 42, 25, 367, 42, 273, 58, 273, 159, 145], [300, 330, 132, 441, 401, 456, 161, 436, 267, 345, 161, 417, 345, 417, 348, 254, 76, 23, 436, 267, 254, 76, 23, 270, 417, 348, 254, 76, 370, 441, 401, 161, 267, 345, 417, 417, 348, 254, 76, 294, 23, 436, 267, 254, 76, 294, 23, 417, 348, 254, 76, 294, 370, 456, 401, 456, 441, 402, 161, 345, 267, 251, 135, 161, 135, 452, 402, 135, 135, 338, 267, 135, 267, 135, 452, 402, 338, 135, 332, 348, 271, 271, 300, 396, 358, 402, 271, 417, 348, 345, 417, 163, 315, 135, 402, 135, 338, 358, 119, 348, 300, 300, 396, 15, 119, 76, 55, 145], [347, 343, 152, 357, 356, 267, 357, 356, 348, 176, 152, 357, 356, 267, 402, 357, 356, 267, 155, 343, 152, 357, 356, 402, 155, 152, 152, 156, 357, 356, 267, 393, 152, 152, 156, 306, 152, 155, 152, 152, 124, 152, 348, 402, 38, 343, 370, 155, 306, 343, 348, 176, 343, 347, 370, 348], [428, 294, 294, 176, 375, 348, 348, 133, 348, 375, 348, 375, 348, 403, 375, 348, 348, 127, 358, 358, 370, 393, 441, 127, 176, 101, 101, 176, 428, 428, 41, 370, 180, 180, 41, 428, 399, 428, 41, 428, 156, 127, 212, 212, 268, 120, 176, 375, 348, 156, 428, 133, 348, 127, 375, 348, 347], [268, 92, 267, 435, 375, 348, 257, 290, 391, 268, 267, 435, 375, 348, 345, 290, 391, 13, 309, 230, 391, 185, 185, 183, 267, 230, 391, 185], [330, 132, 364, 391, 208, 345, 186, 290, 290, 348, 267, 364, 391, 391, 391, 208, 345, 186, 290, 267, 353, 391, 345, 290, 391, 13, 290, 267, 367, 391, 391, 294, 145], [290, 391, 330, 132, 399, 391, 208, 345, 290, 267, 63, 116, 257, 399, 391, 391, 290, 267, 63, 116, 257, 353, 391, 345, 290, 402, 240, 240, 267, 63, 367, 391, 327, 391, 391, 367, 116, 257, 63, 348, 93, 391, 294, 116, 348, 145], [92, 176, 268, 375, 348, 267, 240, 401, 391, 173, 176, 268, 40, 348, 267, 451, 348, 240, 401, 230, 13, 391, 230, 185, 185, 391, 185], [52, 357, 267, 267, 352, 345, 409, 348, 358, 333, 79, 348, 403, 333, 267, 267, 294, 79, 52, 370, 294, 337, 294, 295, 352, 345, 295, 345, 79, 52, 79, 240, 79, 52, 366, 279, 207, 279, 79, 240, 79, 409, 348, 240, 79, 52, 409, 409, 79, 337, 337, 403, 337, 19, 236, 459, 13, 333, 236, 358, 263, 236, 151, 431, 441, 333, 435, 333, 403, 333, 402, 358, 333, 358, 333, 236, 294, 409, 79, 52, 358, 120, 79, 337, 36, 409, 348, 120, 79, 337, 403, 337, 19, 236, 154, 337, 314, 19, 236, 314, 236, 337, 337, 36, 337, 333, 11, 11, 370, 370, 11, 124, 11, 11, 36, 370, 11, 337, 403, 337, 11, 370, 370, 403, 337, 337, 358, 337, 19, 236, 13, 314, 337, 236, 91, 333, 38, 337, 294, 294, 337, 124, 125, 91, 403, 403, 176, 409, 348, 403, 294, 357, 356, 337, 409, 79, 403, 337], [333, 441, 294, 333, 435, 401, 345, 238, 290, 267, 345, 290, 358, 403, 333, 357, 356, 428, 385, 268, 333, 441, 441, 136, 333, 349, 441, 251, 44, 441, 333, 403, 333, 435, 401, 345, 349, 394, 401, 251, 333, 333, 251, 333, 425, 445, 238, 267, 353, 345, 290, 403, 236, 402, 358, 263, 236, 358, 236, 155, 333, 263, 211, 46, 333, 357, 356, 263, 333, 263, 211, 120, 379, 238, 251, 120, 428, 387, 385, 268, 337, 403, 236, 294, 423, 337], [379, 428, 337, 330, 132, 403, 333, 333, 435, 403, 263, 428, 238, 389, 268, 267, 333, 333, 435, 358, 263, 238, 389, 268, 357, 267, 353, 403, 333, 333, 435, 358, 263, 151, 431, 428, 154, 191, 333, 263, 333, 263, 333, 9, 339, 47, 428, 154, 236, 236, 154, 376, 424, 337, 337, 337, 275, 425, 337, 305, 145], [11, 263, 353, 333, 333, 330, 336, 176, 439, 375, 348, 11, 356, 356, 356, 279, 11, 356, 158, 11, 158, 11, 370, 370, 356, 356, 279, 11, 356, 356, 11, 356, 11, 294, 347, 343, 370, 399, 343, 337, 337, 294, 337, 19, 236, 358, 263, 236, 333, 263, 358, 36, 439, 358, 155, 155, 358, 399, 358, 370, 11, 343, 337, 337, 403, 330, 337, 337, 211, 402, 337, 337, 439, 337, 337, 333, 38, 124, 337, 403, 403, 125, 19, 236, 370, 337, 403, 36, 176, 439, 375, 348, 145], [27, 308, 405, 330, 222, 308, 267, 345, 186, 290, 110, 421, 27, 88, 308, 308, 267, 345, 186, 290, 110, 421, 27, 292, 330, 308, 267, 345, 290, 421, 367, 308, 27, 88, 145], [79, 405, 330, 421, 63, 79, 344, 116, 76, 170, 421, 63, 79, 116, 63, 421, 421, 421, 405, 405, 79, 79, 344, 145], [142, 458, 401, 234, 209, 98, 310, 458, 401, 432, 234, 251, 209, 98, 47, 383, 234, 251, 209, 234, 387, 95], [398, 102, 401, 234, 116, 219, 209, 310, 398, 34, 34, 398, 102, 76, 116, 401, 219, 28, 250, 378, 209, 146, 98, 383, 251, 209, 211, 398, 209, 146, 251, 398, 209, 116, 401, 28, 250, 378, 116, 211, 209, 95, 116, 398, 309, 398], [300, 438, 300, 135, 231, 348, 254, 76, 300, 135, 300, 254, 370, 300, 135, 294, 294, 294, 135, 438, 202, 231, 254, 370, 294, 135, 294, 135, 55, 145], [438, 330, 249, 142, 267, 246, 142, 267, 246, 76, 249, 142, 267, 246, 394, 25, 438, 202, 438, 202, 438, 55, 370, 76, 438, 76, 145]]\n"
     ]
    }
   ],
   "source": [
    "text_data_test = text_to_numbers(preprocessed_texts_test, word_dictionary)\n",
    "text_data_train = text_to_numbers(preprocessed_texts_train, word_dictionary)\n",
    "print(np.shape(text_data_test))\n",
    "print(np.shape(text_data_train))\n",
    "text_data = []\n",
    "text_data.extend(text_data_train)\n",
    "text_data.extend(text_data_test)\n",
    "print(np.shape(text_data))\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[428, 333, 403, 337, 347, 391, 185, 348, 116, 77, 300, 210, 190]\n"
     ]
    }
   ],
   "source": [
    "valid_words = ['tuna', 'rice', 'sushi', 'roll', 'sashimi','steak','grill', 'sauce', 'cream',\n",
    "               'cheesecake', 'pizza', 'lasagna', 'hamburger']\n",
    "\n",
    "valid_examples = [word_dictionary[x] for x in valid_words if x in word_dictionary.keys()]\n",
    "print(valid_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['soy' '102']\n",
      " ['soy' '102']\n",
      " ['soy' '102']\n",
      " ['soy' '102']\n",
      " ['soy' '102']\n",
      " ['soy' '102']\n",
      " ['soy' '102']\n",
      " ['soy' '102']]\n",
      "['egg' 'rice' 'egg' 'salt' 'rice' 'soy' 'sauce' 'salt']\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "batch_data, label_data = create_batch_data(preprocessed_texts_all)\n",
    "print(batch_data)\n",
    "print(label_data)\n",
    "print(np.shape(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "embeddings = tf.get_variable(\"embeddings\", shape=[vocabulary_size, embedding_size], trainable=False)\n",
    "doc_embeddings = tf.get_variable(\"doc_embeddings\", shape=[len(preprocessed_texts_train), doc_embedding_size])\n",
    "decoder_weights = tf.get_variable(\"decoder_weights\", shape=[vocabulary_size, concatenated_size], trainable=False)\n",
    "decoder_biases = tf.get_variable(\"decoder_biases\", shape=[vocabulary_size], trainable=False)\n",
    "print(embeddings.trainable)\n",
    "restorer = tf.train.Saver(name=\"restoring\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /notebooks/school/text_feature_extraction/models/train/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Model restored.\n",
      "embeddings : [[-0.6765208  -0.02028374  0.6749721  ...  0.73815125  0.56695515\n",
      "   0.03896838]\n",
      " [ 0.00754435 -0.12381801 -0.03212944 ...  0.36472398  0.85472256\n",
      "  -0.8338148 ]\n",
      " [-0.00252698  0.47475803 -0.46337193 ... -0.7189209   0.7055618\n",
      "   0.18893655]\n",
      " ...\n",
      " [ 0.4988878  -0.00134857 -0.6899472  ... -0.14175357  0.5502593\n",
      "  -0.71772   ]\n",
      " [ 0.00183253  0.3980746  -0.37587786 ... -0.5823257   0.99532753\n",
      "   0.33312145]\n",
      " [ 0.30322266 -0.00235443  0.07464378 ...  0.85145634 -0.83651346\n",
      "   0.24367917]]\n",
      "doc_embeddings : [[-0.24955428 -0.9826422   1.2733805  ...  0.2514961   0.42668948\n",
      "  -1.8259071 ]\n",
      " [-0.35951674 -1.9170965   0.14980748 ...  1.2531807   0.7351144\n",
      "  -0.81338286]\n",
      " [-1.8282828  -1.1303542   0.2403738  ...  0.18064134 -0.37028372\n",
      "  -0.57124025]\n",
      " ...\n",
      " [ 0.06142383 -0.4224579  -0.71129364 ...  0.6132714  -0.39184216\n",
      "   1.6800978 ]\n",
      " [-1.6466497  -0.4515682   0.23712814 ...  0.16389151  0.13004278\n",
      "   0.09575307]\n",
      " [-0.31929702  0.08910733  0.45950502 ...  0.64740914 -0.7659907\n",
      "  -0.21637559]]\n",
      "decoder_weights : [[ 0.4404888   0.35270783  0.5414476  ...  0.3927131   0.9113707\n",
      "  -0.2678713 ]\n",
      " [ 0.31174874  0.31436738  0.3261784  ... -0.17310591  0.74409926\n",
      "  -0.5663443 ]\n",
      " [ 0.74258524  0.66754     0.4875609  ...  0.30448666  0.07006788\n",
      "  -0.60686123]\n",
      " ...\n",
      " [ 0.51984495  0.43862885  0.5815212  ... -0.19006453 -0.07146364\n",
      "  -0.00439298]\n",
      " [ 0.659483    0.512451    0.4466865  ...  0.00301441  1.3554926\n",
      "   0.2789093 ]\n",
      " [ 0.7937902   0.6917156   0.9892521  ...  0.4749484   0.03264845\n",
      "   0.0605864 ]]\n",
      "decoder_biases : [-1.5411026e+00 -5.0227892e-01 -1.4502889e+00 -1.2695315e+00\n",
      " -1.3550551e+00 -1.1592547e-01 -4.5815989e-01 -1.1242820e+00\n",
      " -1.6408395e+00 -1.7603871e+00 -1.0597020e+00 -1.8340950e-01\n",
      " -1.2039973e+00 -2.4376498e-01 -1.1498604e+00  5.2468807e-01\n",
      " -7.1577466e-01 -4.8303908e-01 -7.3760813e-01 -8.2097888e-01\n",
      " -1.2400152e-01 -1.6376958e+00 -1.2858709e+00 -6.8941802e-01\n",
      " -1.2627616e+00  1.0663351e-01 -8.0047596e-01 -8.1850064e-01\n",
      " -4.8799816e-01 -1.2243888e+00 -7.2024423e-01 -1.2875834e+00\n",
      " -1.5179114e+00 -1.0307832e+00 -1.2838905e+00 -1.7585590e+00\n",
      " -1.0829617e+00 -1.6417009e+00 -1.1768509e+00 -8.8305533e-01\n",
      " -1.3362029e+00 -1.4588698e+00 -1.1969390e+00 -1.6380873e+00\n",
      " -3.1685847e-01 -1.4122856e+00 -2.3228664e+00  1.5605085e-01\n",
      " -1.0195348e+00 -3.8065448e-01 -1.3400854e+00 -6.6588914e-01\n",
      " -1.0301884e+00 -1.3464800e+00 -1.2886844e+00  1.7022997e-01\n",
      " -1.0519398e+00 -1.4500397e+00 -1.1796902e+00 -1.3331289e+00\n",
      " -1.0136118e+00 -1.4480509e+00 -1.0537870e+00  1.3180796e+00\n",
      " -1.5579472e+00 -1.2000434e+00 -1.1693772e+00 -4.7547576e-01\n",
      " -1.3946500e+00 -7.7612329e-01 -1.6143056e+00 -2.8776807e-01\n",
      " -1.5835369e+00 -1.0510223e+00 -1.9414761e+00 -8.0206913e-01\n",
      "  9.2746311e-01 -9.0410113e-01 -1.2324567e+00  2.4379876e-01\n",
      " -8.1691176e-01 -7.4687648e-01 -1.3126016e+00 -1.0044798e+00\n",
      "  1.5202418e-01 -1.5639033e+00 -1.7860729e+00 -2.0560472e+00\n",
      " -5.8049268e-01 -7.7864349e-02 -1.4771477e+00 -8.3485246e-01\n",
      "  3.3867526e-01 -1.3735348e+00 -1.0336508e+00 -4.7195795e-01\n",
      " -1.0529640e+00 -1.5905097e+00 -9.8718661e-01 -1.4125348e+00\n",
      " -1.5262603e+00 -2.4136651e+00 -1.1911125e+00 -1.2454939e+00\n",
      " -1.1553994e+00 -1.0996606e+00 -8.3260101e-01 -1.2847810e+00\n",
      " -1.2512680e+00 -8.3575118e-01 -8.6100644e-01 -1.2990009e+00\n",
      " -6.7939174e-01 -1.1453515e+00 -1.1652378e+00 -1.1080703e+00\n",
      "  1.1738799e+00 -1.1214219e+00 -8.2049692e-01 -3.9233029e-01\n",
      " -8.3260882e-01 -4.5874768e-01 -1.3602351e+00 -1.1953696e+00\n",
      " -5.0739199e-01 -2.1957572e+00 -1.5032058e+00 -1.4714677e+00\n",
      " -1.2584997e+00 -1.7097423e+00 -1.4050353e+00  3.4246558e-01\n",
      "  4.1343573e-01 -1.5701140e+00 -1.5261997e+00 -4.8834100e-01\n",
      " -9.9495840e-01 -1.3218421e+00 -1.6094005e+00 -1.1796620e+00\n",
      " -8.8217896e-01 -1.5131007e+00  9.2900717e-01 -1.6068214e+00\n",
      " -1.2497613e+00  2.8835402e+00 -1.4486091e+00 -1.2044706e+00\n",
      " -4.8381072e-01 -1.3036944e+00 -1.1932794e+00 -1.5566109e+00\n",
      " -9.6957380e-01 -1.3453676e+00  1.1617144e-01 -1.6144405e-01\n",
      " -9.9328268e-01 -1.2341466e+00 -2.0410540e+00 -8.1696129e-01\n",
      " -1.1234988e+00  4.2640167e-01 -7.2764289e-01  4.6544439e-01\n",
      " -8.9104521e-01 -4.6055979e-01 -1.0397615e+00 -1.7545688e+00\n",
      " -1.5971732e+00 -1.2340947e+00 -3.1271994e-02 -1.6152556e+00\n",
      " -1.1570305e+00  4.5466512e-01 -1.6322062e+00 -9.2804015e-01\n",
      " -4.4022992e-01 -1.7262243e+00 -1.5873623e+00 -1.3434502e+00\n",
      " -1.1521796e+00 -9.1967267e-01 -1.2441928e+00 -1.1356554e+00\n",
      " -1.5704163e+00 -8.3602989e-01  4.7165585e-01 -1.3506413e+00\n",
      " -1.6281488e+00 -1.5981268e+00 -1.0330455e+00 -1.1602362e+00\n",
      " -1.4489957e+00 -1.4500401e+00 -1.4506235e+00 -1.0892104e+00\n",
      " -5.3615320e-01 -7.6654768e-01 -4.6653000e-01 -1.5533218e+00\n",
      " -2.0197117e+00 -1.7070512e+00 -8.9596778e-01 -1.4263525e+00\n",
      " -1.3691819e+00 -1.9482399e+00 -1.6476629e+00  6.5252614e-01\n",
      " -1.4124683e+00 -1.0877454e+00 -1.0086960e+00  3.5309456e-02\n",
      " -1.3413016e+00 -1.7886580e+00 -4.5290661e-01 -1.5287807e+00\n",
      " -1.0576142e+00 -8.8656467e-01 -1.9720048e+00 -9.2286408e-01\n",
      " -1.2181735e+00 -1.1033304e+00 -9.9457127e-01 -9.3914664e-01\n",
      " -1.2702416e+00 -7.2868288e-01 -9.6648383e-01 -1.6573974e+00\n",
      " -9.7348207e-01 -1.5918494e+00 -1.0940768e+00 -8.6198354e-01\n",
      " -1.4043535e+00 -1.5882466e+00 -1.1712925e+00 -1.2924092e+00\n",
      " -4.0967238e-01 -1.2634237e+00 -7.4436516e-01 -1.6176221e+00\n",
      " -8.1996793e-01 -1.5835363e+00 -1.5828350e+00 -1.5762533e+00\n",
      " -1.5139911e+00 -7.7040684e-01  4.7608083e-01 -1.3876625e+00\n",
      " -1.1845459e+00  1.0329711e+00 -6.6068488e-01  1.8670927e+00\n",
      " -1.0521277e+00 -1.1894795e+00 -5.2014671e-02 -1.8082123e+00\n",
      " -1.0551434e+00 -8.9841443e-01 -2.0816472e+00 -1.3573608e+00\n",
      " -1.4399097e+00 -1.1431657e+00 -7.8443694e-01 -6.6553557e-01\n",
      " -1.5187879e+00 -1.3238769e+00 -1.2230481e+00  2.0728328e+00\n",
      "  6.6044915e-01 -1.3854324e+00 -8.1259215e-01  1.6407692e-01\n",
      " -1.8157876e+00 -7.6215917e-01 -1.7140306e+00  5.4231322e-01\n",
      " -9.6171021e-01 -9.9218315e-01 -3.8974512e-01 -1.6997056e+00\n",
      " -9.1253167e-01 -9.0982556e-01 -1.0481905e+00 -9.6263623e-01\n",
      " -1.1251429e+00 -1.0710554e+00 -7.5240839e-01 -1.0842105e+00\n",
      " -1.5051695e+00 -7.8375489e-01  1.4897275e+00 -1.3076310e+00\n",
      " -1.5136335e+00 -5.0023675e-01  8.1295979e-01 -1.0895638e+00\n",
      " -1.3776307e+00 -1.2082032e+00 -1.4621681e+00 -8.3308250e-01\n",
      " -7.7968889e-01 -1.8453127e+00 -1.4227974e+00 -1.1061554e+00\n",
      " -1.1058302e+00 -1.7345092e+00 -7.9422426e-01 -3.4077629e-01\n",
      " -7.3286015e-01 -1.6388239e-01  1.4126155e+00 -1.2587436e+00\n",
      "  3.7510264e-01 -8.4328407e-01 -1.0920306e+00  5.6554917e-02\n",
      " -2.0553458e+00 -1.7036483e+00 -1.7674150e+00 -1.4046205e+00\n",
      " -1.5788482e+00 -1.2422752e+00 -1.7548518e+00 -7.1026719e-01\n",
      " -1.1249588e+00 -1.8242869e+00 -1.8574082e+00 -1.1268408e+00\n",
      " -1.1967831e+00 -1.4871068e+00  2.6171885e+00 -1.5973825e+00\n",
      " -9.6296835e-01  2.3974301e-01 -7.1990937e-01 -8.5695124e-01\n",
      " -1.0372622e+00  8.4237653e-01 -7.6608133e-01 -1.1684607e+00\n",
      " -1.3048234e+00 -1.4297891e+00 -1.4725624e+00 -6.9657439e-01\n",
      " -1.4719901e+00  2.6711309e+00 -1.3002517e+00 -1.0568024e+00\n",
      "  1.8934416e+00 -8.2753021e-01 -1.4025495e+00 -1.1780634e+00\n",
      " -1.6377600e+00 -2.3475385e-01 -1.4614984e+00 -1.2603718e+00\n",
      " -2.6684454e-01 -3.2467857e-01  1.3918986e+00 -1.1334713e+00\n",
      " -1.2763736e+00 -1.0316265e+00 -1.4052279e+00 -1.3510953e+00\n",
      " -1.0637037e+00 -1.2312812e+00 -1.0191013e+00  5.6337383e-02\n",
      " -6.2990719e-01 -1.0792578e+00  1.2867041e+00 -9.6387327e-01\n",
      " -4.4573170e-01 -1.7997290e+00 -8.8061213e-01  4.2210074e-04\n",
      " -1.2097507e+00 -1.6126933e+00 -8.8095516e-01 -1.2831091e+00\n",
      " -9.2549402e-01 -1.3002414e+00 -1.0515720e+00 -1.1053743e+00\n",
      " -1.4049386e+00 -1.1729938e+00 -1.4409261e+00  8.1022298e-03\n",
      " -1.3039829e+00 -1.4096981e+00 -1.1895249e+00 -1.0508716e-01\n",
      " -1.4255781e+00 -5.9020323e-01  4.7668609e-01 -1.9910498e+00\n",
      " -1.5088469e+00 -1.5496564e+00 -7.6553035e-01 -1.8914467e-01\n",
      " -1.5054212e+00  1.2741288e+00 -3.5269225e-01 -9.3660817e-02\n",
      " -7.4329400e-01 -9.3170905e-01 -1.0705982e+00 -1.3207213e+00\n",
      " -8.5817689e-01 -1.2419415e+00 -1.1652838e+00 -1.7502092e+00\n",
      " -1.7101477e+00 -1.4576050e+00 -1.2513512e+00 -9.4750994e-01\n",
      " -1.5803792e+00 -3.4219658e-01 -1.5193861e+00 -3.1045261e-01\n",
      " -1.9066633e+00 -5.8562458e-01 -1.1941829e+00 -2.9150493e+00\n",
      " -1.0165790e+00  6.6898718e-02 -9.3467611e-01 -1.8792868e+00\n",
      " -7.2530270e-01 -1.5469506e+00 -1.7603722e+00 -1.4084233e+00\n",
      "  2.5770196e-01 -8.0010772e-01 -4.0734428e-01 -3.6643851e-01\n",
      " -9.6945727e-01 -1.4750218e+00 -7.6412106e-01 -9.5302999e-01\n",
      " -1.5345688e+00  1.2610930e+00 -1.7526866e+00 -1.6313169e+00\n",
      " -1.4154147e+00 -2.3028493e-01 -6.9745463e-01 -1.1315187e+00\n",
      " -4.3662596e-01 -1.4413925e+00 -1.2435992e+00 -1.5488018e+00\n",
      " -1.6228935e+00 -1.6278965e+00 -1.5287591e+00 -9.7919458e-01\n",
      " -1.2621584e+00 -7.4981883e-02 -7.9046756e-01 -1.0828428e+00\n",
      " -1.3006221e+00 -1.6062531e+00]\n"
     ]
    }
   ],
   "source": [
    "restorer.restore(sess, os.path.join(models_folder_name_train,\"doc2vec_recipes_checkpoint.ckpt\"))\n",
    "print(\"Model restored.\")\n",
    "# Check the values of the variables\n",
    "print(\"embeddings : %s\" % embeddings.eval())\n",
    "print(\"doc_embeddings : %s\" % doc_embeddings.eval())\n",
    "print(\"decoder_weights : %s\" % decoder_weights.eval())\n",
    "print(\"decoder_biases : %s\" % decoder_biases.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "(264, 27)\n",
      "(316, 27)\n",
      "WARNING:tensorflow:From <ipython-input-13-a5e7fa9e2995>:35: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Starting Training\n",
      "Loss at step 50 : 3.0290775299072266\n",
      "Loss at step 100 : 3.360969066619873\n",
      "Loss at step 150 : 8.807123184204102\n",
      "Loss at step 200 : 2.406243324279785\n",
      "Loss at step 250 : 3.0291762351989746\n",
      "Loss at step 300 : 5.334019660949707\n",
      "Loss at step 350 : 7.515295028686523\n",
      "Loss at step 400 : 3.528195381164551\n",
      "Loss at step 450 : 2.8757855892181396\n",
      "Loss at step 500 : 6.138593673706055\n",
      "Loss at step 550 : 3.0525684356689453\n",
      "Loss at step 600 : 7.907103061676025\n",
      "Loss at step 650 : 3.4754695892333984\n",
      "Loss at step 700 : 7.502150535583496\n",
      "Loss at step 750 : 6.156990051269531\n",
      "Loss at step 800 : 3.054830312728882\n",
      "Loss at step 850 : 3.2054712772369385\n",
      "Loss at step 900 : 3.0558252334594727\n",
      "Loss at step 950 : 2.8244338035583496\n",
      "Loss at step 1000 : 3.4065909385681152\n",
      "Loss at step 1050 : 7.052783489227295\n",
      "Loss at step 1100 : 3.301682233810425\n",
      "Loss at step 1150 : 7.137914180755615\n",
      "Loss at step 1200 : 6.640952110290527\n",
      "Loss at step 1250 : 2.911311149597168\n",
      "Loss at step 1300 : 2.805367946624756\n",
      "Loss at step 1350 : 2.3779401779174805\n",
      "Loss at step 1400 : 5.317473411560059\n",
      "Loss at step 1450 : 2.8464348316192627\n",
      "Loss at step 1500 : 2.7417140007019043\n",
      "Loss at step 1550 : 2.2752697467803955\n",
      "Loss at step 1600 : 3.880216598510742\n",
      "Loss at step 1650 : 2.762253761291504\n",
      "Loss at step 1700 : 6.452180862426758\n",
      "Loss at step 1750 : 3.650818347930908\n",
      "Loss at step 1800 : 2.4959182739257812\n",
      "Loss at step 1850 : 2.9779629707336426\n",
      "Loss at step 1900 : 2.6342520713806152\n",
      "Loss at step 1950 : 3.0342092514038086\n",
      "Loss at step 2000 : 4.670450210571289\n",
      "Loss at step 2050 : 2.947680950164795\n",
      "Loss at step 2100 : 7.065168380737305\n",
      "Loss at step 2150 : 2.1949305534362793\n",
      "Loss at step 2200 : 2.274745225906372\n",
      "Loss at step 2250 : 3.6271278858184814\n",
      "Loss at step 2300 : 2.9756011962890625\n",
      "Loss at step 2350 : 3.642854928970337\n",
      "Loss at step 2400 : 2.830475330352783\n",
      "Loss at step 2450 : 2.7169179916381836\n",
      "Loss at step 2500 : 2.8654754161834717\n",
      "Loss at step 2550 : 2.296896457672119\n",
      "Loss at step 2600 : 2.7748894691467285\n",
      "Loss at step 2650 : 2.6416454315185547\n",
      "Loss at step 2700 : 2.506333589553833\n",
      "Loss at step 2750 : 5.788934707641602\n",
      "Loss at step 2800 : 2.5087320804595947\n",
      "Loss at step 2850 : 3.5658349990844727\n",
      "Loss at step 2900 : 2.618000030517578\n",
      "Loss at step 2950 : 3.037156581878662\n",
      "Loss at step 3000 : 2.955921173095703\n",
      "Loss at step 3050 : 2.466473340988159\n",
      "Loss at step 3100 : 3.3489561080932617\n",
      "Loss at step 3150 : 2.6584982872009277\n",
      "Loss at step 3200 : 2.0471880435943604\n",
      "Loss at step 3250 : 3.1330487728118896\n",
      "Loss at step 3300 : 2.8108510971069336\n",
      "Loss at step 3350 : 2.6586198806762695\n",
      "Loss at step 3400 : 2.0848770141601562\n",
      "Loss at step 3450 : 3.2897729873657227\n",
      "Loss at step 3500 : 2.9656691551208496\n",
      "Loss at step 3550 : 2.8128037452697754\n",
      "Loss at step 3600 : 2.298668384552002\n",
      "Loss at step 3650 : 5.23097038269043\n",
      "Loss at step 3700 : 3.0597963333129883\n",
      "Loss at step 3750 : 3.4317121505737305\n",
      "Loss at step 3800 : 2.7596497535705566\n",
      "Loss at step 3850 : 3.7117154598236084\n",
      "Loss at step 3900 : 2.6223580837249756\n",
      "Loss at step 3950 : 2.906590223312378\n",
      "Loss at step 4000 : 5.438821792602539\n",
      "Loss at step 4050 : 2.429452419281006\n",
      "Loss at step 4100 : 6.506877899169922\n",
      "Loss at step 4150 : 2.964066982269287\n",
      "Loss at step 4200 : 2.419651508331299\n",
      "Loss at step 4250 : 3.0839624404907227\n",
      "Loss at step 4300 : 3.737105369567871\n",
      "Loss at step 4350 : 3.482893943786621\n",
      "Loss at step 4400 : 2.781383991241455\n",
      "Loss at step 4450 : 2.7970187664031982\n",
      "Loss at step 4500 : 2.779207706451416\n",
      "Loss at step 4550 : 2.909280300140381\n",
      "Loss at step 4600 : 2.8937461376190186\n",
      "Loss at step 4650 : 6.572622299194336\n",
      "Loss at step 4700 : 2.813380241394043\n",
      "Loss at step 4750 : 2.7218642234802246\n",
      "Loss at step 4800 : 2.449580192565918\n",
      "Loss at step 4850 : 3.4719343185424805\n",
      "Loss at step 4900 : 6.217432498931885\n",
      "Loss at step 4950 : 2.7011613845825195\n",
      "Loss at step 5000 : 3.232386350631714\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 5050 : 5.662592887878418\n",
      "Loss at step 5100 : 3.235523223876953\n",
      "Loss at step 5150 : 7.413663864135742\n",
      "Loss at step 5200 : 2.5562610626220703\n",
      "Loss at step 5250 : 4.9857892990112305\n",
      "Loss at step 5300 : 3.0386035442352295\n",
      "Loss at step 5350 : 3.702974319458008\n",
      "Loss at step 5400 : 3.887101650238037\n",
      "Loss at step 5450 : 3.482625722885132\n",
      "Loss at step 5500 : 2.6897945404052734\n",
      "Loss at step 5550 : 3.442774534225464\n",
      "Loss at step 5600 : 5.792842388153076\n",
      "Loss at step 5650 : 2.649383306503296\n",
      "Loss at step 5700 : 2.6746129989624023\n",
      "Loss at step 5750 : 5.390208721160889\n",
      "Loss at step 5800 : 2.6905131340026855\n",
      "Loss at step 5850 : 3.194838047027588\n",
      "Loss at step 5900 : 3.376420259475708\n",
      "Loss at step 5950 : 6.324343204498291\n",
      "Loss at step 6000 : 2.631700038909912\n",
      "Loss at step 6050 : 2.797945022583008\n",
      "Loss at step 6100 : 2.4701476097106934\n",
      "Loss at step 6150 : 5.662519454956055\n",
      "Loss at step 6200 : 2.420506477355957\n",
      "Loss at step 6250 : 2.27347993850708\n",
      "Loss at step 6300 : 2.546266555786133\n",
      "Loss at step 6350 : 2.7988884449005127\n",
      "Loss at step 6400 : 3.23195219039917\n",
      "Loss at step 6450 : 2.1401305198669434\n",
      "Loss at step 6500 : 6.873147964477539\n",
      "Loss at step 6550 : 2.4201760292053223\n",
      "Loss at step 6600 : 3.8565890789031982\n",
      "Loss at step 6650 : 3.224435329437256\n",
      "Loss at step 6700 : 3.314199209213257\n",
      "Loss at step 6750 : 2.2629237174987793\n",
      "Loss at step 6800 : 6.666079521179199\n",
      "Loss at step 6850 : 5.402643203735352\n",
      "Loss at step 6900 : 3.3326334953308105\n",
      "Loss at step 6950 : 2.2275495529174805\n",
      "Loss at step 7000 : 3.2448408603668213\n",
      "Loss at step 7050 : 2.478912830352783\n",
      "Loss at step 7100 : 3.3957979679107666\n",
      "Loss at step 7150 : 3.191756248474121\n",
      "Loss at step 7200 : 2.94966459274292\n",
      "Loss at step 7250 : 2.9360098838806152\n",
      "Loss at step 7300 : 4.017692565917969\n",
      "Loss at step 7350 : 1.6055431365966797\n",
      "Loss at step 7400 : 5.850614070892334\n",
      "Loss at step 7450 : 2.867781162261963\n",
      "Loss at step 7500 : 3.0817461013793945\n",
      "Loss at step 7550 : 3.7237136363983154\n",
      "Loss at step 7600 : 2.927335262298584\n",
      "Loss at step 7650 : 2.9249298572540283\n",
      "Loss at step 7700 : 3.2868733406066895\n",
      "Loss at step 7750 : 6.435757637023926\n",
      "Loss at step 7800 : 5.24824333190918\n",
      "Loss at step 7850 : 3.1167280673980713\n",
      "Loss at step 7900 : 3.410069465637207\n",
      "Loss at step 7950 : 3.0446982383728027\n",
      "Loss at step 8000 : 1.8819096088409424\n",
      "Loss at step 8050 : 2.1601438522338867\n",
      "Loss at step 8100 : 1.920890212059021\n",
      "Loss at step 8150 : 3.3336267471313477\n",
      "Loss at step 8200 : 2.484389305114746\n",
      "Loss at step 8250 : 3.1220791339874268\n",
      "Loss at step 8300 : 6.599692344665527\n",
      "Loss at step 8350 : 3.0435609817504883\n",
      "Loss at step 8400 : 3.0587854385375977\n",
      "Loss at step 8450 : 2.1103432178497314\n",
      "Loss at step 8500 : 2.56315016746521\n",
      "Loss at step 8550 : 3.1805734634399414\n",
      "Loss at step 8600 : 3.1580848693847656\n",
      "Loss at step 8650 : 3.066478729248047\n",
      "Loss at step 8700 : 3.4467825889587402\n",
      "Loss at step 8750 : 2.6417417526245117\n",
      "Loss at step 8800 : 6.769665718078613\n",
      "Loss at step 8850 : 2.6045236587524414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 8900 : 2.5960659980773926\n",
      "Loss at step 8950 : 3.4089841842651367\n",
      "Loss at step 9000 : 2.7690718173980713\n",
      "Loss at step 9050 : 5.533332347869873\n",
      "Loss at step 9100 : 2.7925872802734375\n",
      "Loss at step 9150 : 2.6319339275360107\n",
      "Loss at step 9200 : 3.0769405364990234\n",
      "Loss at step 9250 : 3.003282070159912\n",
      "Loss at step 9300 : 2.483160972595215\n",
      "Loss at step 9350 : 2.9699389934539795\n",
      "Loss at step 9400 : 3.6949410438537598\n",
      "Loss at step 9450 : 2.8764729499816895\n",
      "Loss at step 9500 : 4.223791122436523\n",
      "Loss at step 9550 : 2.651318311691284\n",
      "Loss at step 9600 : 2.777953624725342\n",
      "Loss at step 9650 : 2.885868549346924\n",
      "Loss at step 9700 : 2.435645580291748\n",
      "Loss at step 9750 : 3.0214080810546875\n",
      "Loss at step 9800 : 2.98506236076355\n",
      "Loss at step 9850 : 7.900766372680664\n",
      "Loss at step 9900 : 6.4893035888671875\n",
      "Loss at step 9950 : 2.382174253463745\n",
      "Loss at step 10000 : 3.262024402618408\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 10050 : 2.184149980545044\n",
      "Loss at step 10100 : 2.9503982067108154\n",
      "Loss at step 10150 : 3.447509765625\n",
      "Loss at step 10200 : 4.2956976890563965\n",
      "Loss at step 10250 : 3.056948184967041\n",
      "Loss at step 10300 : 3.6020383834838867\n",
      "Loss at step 10350 : 2.3336126804351807\n",
      "Loss at step 10400 : 3.007023572921753\n",
      "Loss at step 10450 : 3.406560182571411\n",
      "Loss at step 10500 : 2.9797089099884033\n",
      "Loss at step 10550 : 3.056030511856079\n",
      "Loss at step 10600 : 4.398345947265625\n",
      "Loss at step 10650 : 6.312676906585693\n",
      "Loss at step 10700 : 2.3129730224609375\n",
      "Loss at step 10750 : 2.65206241607666\n",
      "Loss at step 10800 : 2.4898898601531982\n",
      "Loss at step 10850 : 3.4896652698516846\n",
      "Loss at step 10900 : 3.523574113845825\n",
      "Loss at step 10950 : 2.863924264907837\n",
      "Loss at step 11000 : 3.1031458377838135\n",
      "Loss at step 11050 : 3.184739351272583\n",
      "Loss at step 11100 : 4.001026630401611\n",
      "Loss at step 11150 : 3.313438653945923\n",
      "Loss at step 11200 : 2.8285372257232666\n",
      "Loss at step 11250 : 2.9427387714385986\n",
      "Loss at step 11300 : 1.9765340089797974\n",
      "Loss at step 11350 : 2.607466459274292\n",
      "Loss at step 11400 : 2.675565242767334\n",
      "Loss at step 11450 : 2.5668702125549316\n",
      "Loss at step 11500 : 2.695988893508911\n",
      "Loss at step 11550 : 3.155215263366699\n",
      "Loss at step 11600 : 5.202144145965576\n",
      "Loss at step 11650 : 5.17059326171875\n",
      "Loss at step 11700 : 3.0922093391418457\n",
      "Loss at step 11750 : 2.669337272644043\n",
      "Loss at step 11800 : 2.061936855316162\n",
      "Loss at step 11850 : 2.679335117340088\n",
      "Loss at step 11900 : 2.8821732997894287\n",
      "Loss at step 11950 : 3.36328125\n",
      "Loss at step 12000 : 4.746010780334473\n",
      "Loss at step 12050 : 2.9995031356811523\n",
      "Loss at step 12100 : 3.108579635620117\n",
      "Loss at step 12150 : 5.491075038909912\n",
      "Loss at step 12200 : 2.245112895965576\n",
      "Loss at step 12250 : 3.3571786880493164\n",
      "Loss at step 12300 : 3.400580883026123\n",
      "Loss at step 12350 : 2.1477932929992676\n",
      "Loss at step 12400 : 2.9018547534942627\n",
      "Loss at step 12450 : 2.9854207038879395\n",
      "Loss at step 12500 : 2.479391098022461\n",
      "Loss at step 12550 : 3.621293067932129\n",
      "Loss at step 12600 : 3.0636167526245117\n",
      "Loss at step 12650 : 4.377457141876221\n",
      "Loss at step 12700 : 3.5798494815826416\n",
      "Loss at step 12750 : 2.796398639678955\n",
      "Loss at step 12800 : 3.573765754699707\n",
      "Loss at step 12850 : 5.098941802978516\n",
      "Loss at step 12900 : 2.7770721912384033\n",
      "Loss at step 12950 : 3.152242660522461\n",
      "Loss at step 13000 : 3.75386643409729\n",
      "Loss at step 13050 : 7.468255996704102\n",
      "Loss at step 13100 : 2.9826536178588867\n",
      "Loss at step 13150 : 3.3819313049316406\n",
      "Loss at step 13200 : 2.923975944519043\n",
      "Loss at step 13250 : 2.2065205574035645\n",
      "Loss at step 13300 : 1.8039095401763916\n",
      "Loss at step 13350 : 2.397124767303467\n",
      "Loss at step 13400 : 2.8757970333099365\n",
      "Loss at step 13450 : 3.6046884059906006\n",
      "Loss at step 13500 : 2.632920026779175\n",
      "Loss at step 13550 : 2.242628574371338\n",
      "Loss at step 13600 : 4.1338090896606445\n",
      "Loss at step 13650 : 2.3890223503112793\n",
      "Loss at step 13700 : 2.9531712532043457\n",
      "Loss at step 13750 : 2.2305376529693604\n",
      "Loss at step 13800 : 3.129828929901123\n",
      "Loss at step 13850 : 2.3464131355285645\n",
      "Loss at step 13900 : 3.311602830886841\n",
      "Loss at step 13950 : 3.482180595397949\n",
      "Loss at step 14000 : 2.3405003547668457\n",
      "Loss at step 14050 : 2.1678543090820312\n",
      "Loss at step 14100 : 5.098263263702393\n",
      "Loss at step 14150 : 4.769306659698486\n",
      "Loss at step 14200 : 2.6747570037841797\n",
      "Loss at step 14250 : 4.321866035461426\n",
      "Loss at step 14300 : 2.1130928993225098\n",
      "Loss at step 14350 : 2.866825580596924\n",
      "Loss at step 14400 : 2.290489673614502\n",
      "Loss at step 14450 : 2.6704962253570557\n",
      "Loss at step 14500 : 2.807803153991699\n",
      "Loss at step 14550 : 3.0775387287139893\n",
      "Loss at step 14600 : 2.458758592605591\n",
      "Loss at step 14650 : 2.7380733489990234\n",
      "Loss at step 14700 : 3.02152681350708\n",
      "Loss at step 14750 : 2.6304690837860107\n",
      "Loss at step 14800 : 2.7375683784484863\n",
      "Loss at step 14850 : 2.4128894805908203\n",
      "Loss at step 14900 : 2.2441627979278564\n",
      "Loss at step 14950 : 2.6443753242492676\n",
      "Loss at step 15000 : 1.8082491159439087\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 15050 : 2.665809154510498\n",
      "Loss at step 15100 : 4.837299346923828\n",
      "Loss at step 15150 : 2.8674638271331787\n",
      "Loss at step 15200 : 3.0054945945739746\n",
      "Loss at step 15250 : 2.192695140838623\n",
      "Loss at step 15300 : 3.2598466873168945\n",
      "Loss at step 15350 : 5.481322288513184\n",
      "Loss at step 15400 : 3.396529197692871\n",
      "Loss at step 15450 : 3.974654197692871\n",
      "Loss at step 15500 : 3.802018642425537\n",
      "Loss at step 15550 : 5.272917747497559\n",
      "Loss at step 15600 : 5.906169891357422\n",
      "Loss at step 15650 : 4.844942092895508\n",
      "Loss at step 15700 : 2.8939714431762695\n",
      "Loss at step 15750 : 2.563699960708618\n",
      "Loss at step 15800 : 2.8857421875\n",
      "Loss at step 15850 : 2.803464651107788\n",
      "Loss at step 15900 : 2.296060085296631\n",
      "Loss at step 15950 : 2.8444128036499023\n",
      "Loss at step 16000 : 3.1072146892547607\n",
      "Loss at step 16050 : 2.249657154083252\n",
      "Loss at step 16100 : 3.423431634902954\n",
      "Loss at step 16150 : 2.510167360305786\n",
      "Loss at step 16200 : 2.674229145050049\n",
      "Loss at step 16250 : 3.045377016067505\n",
      "Loss at step 16300 : 3.33062744140625\n",
      "Loss at step 16350 : 3.2880115509033203\n",
      "Loss at step 16400 : 4.747472286224365\n",
      "Loss at step 16450 : 2.802616596221924\n",
      "Loss at step 16500 : 3.0374948978424072\n",
      "Loss at step 16550 : 2.6327176094055176\n",
      "Loss at step 16600 : 4.312736988067627\n",
      "Loss at step 16650 : 5.198507308959961\n",
      "Loss at step 16700 : 2.612867832183838\n",
      "Loss at step 16750 : 2.917940139770508\n",
      "Loss at step 16800 : 3.2190518379211426\n",
      "Loss at step 16850 : 3.2913222312927246\n",
      "Loss at step 16900 : 5.327393054962158\n",
      "Loss at step 16950 : 1.7522982358932495\n",
      "Loss at step 17000 : 1.9938716888427734\n",
      "Loss at step 17050 : 3.6471474170684814\n",
      "Loss at step 17100 : 3.1479530334472656\n",
      "Loss at step 17150 : 2.550666570663452\n",
      "Loss at step 17200 : 5.17035436630249\n",
      "Loss at step 17250 : 3.347573757171631\n",
      "Loss at step 17300 : 3.5431101322174072\n",
      "Loss at step 17350 : 3.5796051025390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 17400 : 3.326770782470703\n",
      "Loss at step 17450 : 3.112553596496582\n",
      "Loss at step 17500 : 2.4997687339782715\n",
      "Loss at step 17550 : 3.1086995601654053\n",
      "Loss at step 17600 : 3.01926326751709\n",
      "Loss at step 17650 : 2.7939977645874023\n",
      "Loss at step 17700 : 2.3297345638275146\n",
      "Loss at step 17750 : 3.2448205947875977\n",
      "Loss at step 17800 : 3.6008403301239014\n",
      "Loss at step 17850 : 1.8896619081497192\n",
      "Loss at step 17900 : 4.087776184082031\n",
      "Loss at step 17950 : 2.192676305770874\n",
      "Loss at step 18000 : 2.0449037551879883\n",
      "Loss at step 18050 : 3.2113659381866455\n",
      "Loss at step 18100 : 5.101892471313477\n",
      "Loss at step 18150 : 3.6930034160614014\n",
      "Loss at step 18200 : 2.5765442848205566\n",
      "Loss at step 18250 : 2.412639617919922\n",
      "Loss at step 18300 : 5.222782135009766\n",
      "Loss at step 18350 : 4.448805809020996\n",
      "Loss at step 18400 : 4.027799606323242\n",
      "Loss at step 18450 : 2.8745079040527344\n",
      "Loss at step 18500 : 2.7000396251678467\n",
      "Loss at step 18550 : 2.1796514987945557\n",
      "Loss at step 18600 : 2.349194288253784\n",
      "Loss at step 18650 : 4.618988037109375\n",
      "Loss at step 18700 : 2.530698537826538\n",
      "Loss at step 18750 : 3.2191829681396484\n",
      "Loss at step 18800 : 3.383856773376465\n",
      "Loss at step 18850 : 6.086969375610352\n",
      "Loss at step 18900 : 3.254091262817383\n",
      "Loss at step 18950 : 2.6779685020446777\n",
      "Loss at step 19000 : 3.4474008083343506\n",
      "Loss at step 19050 : 2.6618027687072754\n",
      "Loss at step 19100 : 2.8028430938720703\n",
      "Loss at step 19150 : 2.8425707817077637\n",
      "Loss at step 19200 : 2.966836929321289\n",
      "Loss at step 19250 : 3.472182273864746\n",
      "Loss at step 19300 : 2.980844497680664\n",
      "Loss at step 19350 : 2.7524256706237793\n",
      "Loss at step 19400 : 3.3198132514953613\n",
      "Loss at step 19450 : 5.199302673339844\n",
      "Loss at step 19500 : 4.9513773918151855\n",
      "Loss at step 19550 : 1.9881477355957031\n",
      "Loss at step 19600 : 2.9154162406921387\n",
      "Loss at step 19650 : 2.3028969764709473\n",
      "Loss at step 19700 : 2.656083583831787\n",
      "Loss at step 19750 : 2.6923115253448486\n",
      "Loss at step 19800 : 2.769235372543335\n",
      "Loss at step 19850 : 2.2028512954711914\n",
      "Loss at step 19900 : 3.2800803184509277\n",
      "Loss at step 19950 : 2.8674299716949463\n",
      "Loss at step 20000 : 2.880584478378296\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 20050 : 3.363572120666504\n",
      "Loss at step 20100 : 4.4366302490234375\n",
      "Loss at step 20150 : 3.445011615753174\n",
      "Loss at step 20200 : 3.6052446365356445\n",
      "Loss at step 20250 : 2.3259999752044678\n",
      "Loss at step 20300 : 2.8283872604370117\n",
      "Loss at step 20350 : 3.0050301551818848\n",
      "Loss at step 20400 : 2.888521194458008\n",
      "Loss at step 20450 : 3.1595354080200195\n",
      "Loss at step 20500 : 2.4368910789489746\n",
      "Loss at step 20550 : 2.574240207672119\n",
      "Loss at step 20600 : 2.914400577545166\n",
      "Loss at step 20650 : 3.34192156791687\n",
      "Loss at step 20700 : 2.562624216079712\n",
      "Loss at step 20750 : 2.5393919944763184\n",
      "Loss at step 20800 : 2.8105978965759277\n",
      "Loss at step 20850 : 3.544732093811035\n",
      "Loss at step 20900 : 2.1203131675720215\n",
      "Loss at step 20950 : 3.0565953254699707\n",
      "Loss at step 21000 : 2.8525943756103516\n",
      "Loss at step 21050 : 2.7791996002197266\n",
      "Loss at step 21100 : 3.0190317630767822\n",
      "Loss at step 21150 : 3.091127872467041\n",
      "Loss at step 21200 : 2.9674108028411865\n",
      "Loss at step 21250 : 2.8246121406555176\n",
      "Loss at step 21300 : 2.166159152984619\n",
      "Loss at step 21350 : 2.501919746398926\n",
      "Loss at step 21400 : 2.610947370529175\n",
      "Loss at step 21450 : 5.970333099365234\n",
      "Loss at step 21500 : 4.879297256469727\n",
      "Loss at step 21550 : 2.721499443054199\n",
      "Loss at step 21600 : 3.066269874572754\n",
      "Loss at step 21650 : 4.518310070037842\n",
      "Loss at step 21700 : 3.287336587905884\n",
      "Loss at step 21750 : 2.587942600250244\n",
      "Loss at step 21800 : 3.1432385444641113\n",
      "Loss at step 21850 : 2.7494349479675293\n",
      "Loss at step 21900 : 2.94416880607605\n",
      "Loss at step 21950 : 3.765011787414551\n",
      "Loss at step 22000 : 3.2227420806884766\n",
      "Loss at step 22050 : 3.4514477252960205\n",
      "Loss at step 22100 : 2.6562578678131104\n",
      "Loss at step 22150 : 3.298349380493164\n",
      "Loss at step 22200 : 2.7598819732666016\n",
      "Loss at step 22250 : 2.7194724082946777\n",
      "Loss at step 22300 : 2.776811122894287\n",
      "Loss at step 22350 : 2.910034656524658\n",
      "Loss at step 22400 : 3.5459301471710205\n",
      "Loss at step 22450 : 2.967470169067383\n",
      "Loss at step 22500 : 3.4337384700775146\n",
      "Loss at step 22550 : 3.2930212020874023\n",
      "Loss at step 22600 : 3.7645058631896973\n",
      "Loss at step 22650 : 3.124441385269165\n",
      "Loss at step 22700 : 2.5629241466522217\n",
      "Loss at step 22750 : 3.935238838195801\n",
      "Loss at step 22800 : 5.500816345214844\n",
      "Loss at step 22850 : 1.630678415298462\n",
      "Loss at step 22900 : 3.0761499404907227\n",
      "Loss at step 22950 : 3.2335119247436523\n",
      "Loss at step 23000 : 2.4123239517211914\n",
      "Loss at step 23050 : 2.743635654449463\n",
      "Loss at step 23100 : 3.6406962871551514\n",
      "Loss at step 23150 : 3.036247730255127\n",
      "Loss at step 23200 : 2.9607181549072266\n",
      "Loss at step 23250 : 2.6764965057373047\n",
      "Loss at step 23300 : 2.517949342727661\n",
      "Loss at step 23350 : 1.7780503034591675\n",
      "Loss at step 23400 : 2.4937877655029297\n",
      "Loss at step 23450 : 1.8471763134002686\n",
      "Loss at step 23500 : 3.5346627235412598\n",
      "Loss at step 23550 : 3.2720813751220703\n",
      "Loss at step 23600 : 3.199943780899048\n",
      "Loss at step 23650 : 2.912877082824707\n",
      "Loss at step 23700 : 3.1456761360168457\n",
      "Loss at step 23750 : 2.65012264251709\n",
      "Loss at step 23800 : 2.7508368492126465\n",
      "Loss at step 23850 : 2.783337116241455\n",
      "Loss at step 23900 : 3.2220263481140137\n",
      "Loss at step 23950 : 4.309730529785156\n",
      "Loss at step 24000 : 4.501435279846191\n",
      "Loss at step 24050 : 3.1324896812438965\n",
      "Loss at step 24100 : 1.9178307056427002\n",
      "Loss at step 24150 : 3.0024447441101074\n",
      "Loss at step 24200 : 3.2064690589904785\n",
      "Loss at step 24250 : 2.0577659606933594\n",
      "Loss at step 24300 : 2.427363872528076\n",
      "Loss at step 24350 : 2.1903629302978516\n",
      "Loss at step 24400 : 2.783123016357422\n",
      "Loss at step 24450 : 2.2197093963623047\n",
      "Loss at step 24500 : 2.726860761642456\n",
      "Loss at step 24550 : 3.3846192359924316\n",
      "Loss at step 24600 : 2.8952770233154297\n",
      "Loss at step 24650 : 3.4688894748687744\n",
      "Loss at step 24700 : 4.750079154968262\n",
      "Loss at step 24750 : 2.9494662284851074\n",
      "Loss at step 24800 : 3.381558895111084\n",
      "Loss at step 24850 : 3.536802291870117\n",
      "Loss at step 24900 : 2.3441104888916016\n",
      "Loss at step 24950 : 2.9148716926574707\n",
      "Loss at step 25000 : 3.742014169692993\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 25050 : 3.2183141708374023\n",
      "Loss at step 25100 : 2.6504180431365967\n",
      "Loss at step 25150 : 2.5733280181884766\n",
      "Loss at step 25200 : 3.573805332183838\n",
      "Loss at step 25250 : 3.0254483222961426\n",
      "Loss at step 25300 : 3.3192572593688965\n",
      "Loss at step 25350 : 5.679582595825195\n",
      "Loss at step 25400 : 4.377811431884766\n",
      "Loss at step 25450 : 2.9618496894836426\n",
      "Loss at step 25500 : 3.732741355895996\n",
      "Loss at step 25550 : 3.8894577026367188\n",
      "Loss at step 25600 : 2.8710920810699463\n",
      "Loss at step 25650 : 2.9093563556671143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 25700 : 3.039734363555908\n",
      "Loss at step 25750 : 2.6663432121276855\n",
      "Loss at step 25800 : 2.7856807708740234\n",
      "Loss at step 25850 : 2.611057996749878\n",
      "Loss at step 25900 : 2.8066418170928955\n",
      "Loss at step 25950 : 2.537956714630127\n",
      "Loss at step 26000 : 2.3649845123291016\n",
      "Loss at step 26050 : 3.2953107357025146\n",
      "Loss at step 26100 : 3.114234685897827\n",
      "Loss at step 26150 : 2.3381738662719727\n",
      "Loss at step 26200 : 4.456948280334473\n",
      "Loss at step 26250 : 2.3253114223480225\n",
      "Loss at step 26300 : 4.396428108215332\n",
      "Loss at step 26350 : 3.253453493118286\n",
      "Loss at step 26400 : 3.1802384853363037\n",
      "Loss at step 26450 : 3.8754372596740723\n",
      "Loss at step 26500 : 3.005016803741455\n",
      "Loss at step 26550 : 3.6911137104034424\n",
      "Loss at step 26600 : 3.646693706512451\n",
      "Loss at step 26650 : 3.5391111373901367\n",
      "Loss at step 26700 : 2.095207929611206\n",
      "Loss at step 26750 : 3.050137996673584\n",
      "Loss at step 26800 : 3.7426345348358154\n",
      "Loss at step 26850 : 2.600449323654175\n",
      "Loss at step 26900 : 2.1537070274353027\n",
      "Loss at step 26950 : 3.1347832679748535\n",
      "Loss at step 27000 : 3.3890013694763184\n",
      "Loss at step 27050 : 4.829033374786377\n",
      "Loss at step 27100 : 2.3359899520874023\n",
      "Loss at step 27150 : 2.647967576980591\n",
      "Loss at step 27200 : 2.731581211090088\n",
      "Loss at step 27250 : 3.198723316192627\n",
      "Loss at step 27300 : 3.7947967052459717\n",
      "Loss at step 27350 : 2.9515366554260254\n",
      "Loss at step 27400 : 3.393734931945801\n",
      "Loss at step 27450 : 1.537982702255249\n",
      "Loss at step 27500 : 2.2950103282928467\n",
      "Loss at step 27550 : 2.7430834770202637\n",
      "Loss at step 27600 : 2.7160089015960693\n",
      "Loss at step 27650 : 2.6767783164978027\n",
      "Loss at step 27700 : 3.2548155784606934\n",
      "Loss at step 27750 : 2.5697240829467773\n",
      "Loss at step 27800 : 3.4393792152404785\n",
      "Loss at step 27850 : 2.713440179824829\n",
      "Loss at step 27900 : 3.933915615081787\n",
      "Loss at step 27950 : 2.805128574371338\n",
      "Loss at step 28000 : 2.313965320587158\n",
      "Loss at step 28050 : 2.1733086109161377\n",
      "Loss at step 28100 : 1.9349687099456787\n",
      "Loss at step 28150 : 3.3056282997131348\n",
      "Loss at step 28200 : 3.1590793132781982\n",
      "Loss at step 28250 : 2.749412775039673\n",
      "Loss at step 28300 : 1.8548595905303955\n",
      "Loss at step 28350 : 3.3774003982543945\n",
      "Loss at step 28400 : 3.8643815517425537\n",
      "Loss at step 28450 : 2.2775068283081055\n",
      "Loss at step 28500 : 1.790701985359192\n",
      "Loss at step 28550 : 2.105278253555298\n",
      "Loss at step 28600 : 3.148993492126465\n",
      "Loss at step 28650 : 2.211183786392212\n",
      "Loss at step 28700 : 3.1976077556610107\n",
      "Loss at step 28750 : 1.9301292896270752\n",
      "Loss at step 28800 : 3.908352851867676\n",
      "Loss at step 28850 : 3.3219380378723145\n",
      "Loss at step 28900 : 1.9436155557632446\n",
      "Loss at step 28950 : 5.157295227050781\n",
      "Loss at step 29000 : 2.6675257682800293\n",
      "Loss at step 29050 : 3.3568472862243652\n",
      "Loss at step 29100 : 2.4757072925567627\n",
      "Loss at step 29150 : 2.725168228149414\n",
      "Loss at step 29200 : 3.893397331237793\n",
      "Loss at step 29250 : 5.204906463623047\n",
      "Loss at step 29300 : 2.658339500427246\n",
      "Loss at step 29350 : 4.049866676330566\n",
      "Loss at step 29400 : 3.169503688812256\n",
      "Loss at step 29450 : 3.2593088150024414\n",
      "Loss at step 29500 : 2.9500794410705566\n",
      "Loss at step 29550 : 1.9885330200195312\n",
      "Loss at step 29600 : 2.7818498611450195\n",
      "Loss at step 29650 : 2.7530293464660645\n",
      "Loss at step 29700 : 2.3209123611450195\n",
      "Loss at step 29750 : 3.6774964332580566\n",
      "Loss at step 29800 : 3.5072903633117676\n",
      "Loss at step 29850 : 3.0799834728240967\n",
      "Loss at step 29900 : 2.359710216522217\n",
      "Loss at step 29950 : 3.3311166763305664\n",
      "Loss at step 30000 : 3.2417173385620117\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 30050 : 2.055633068084717\n",
      "Loss at step 30100 : 3.2443549633026123\n",
      "Loss at step 30150 : 3.264145612716675\n",
      "Loss at step 30200 : 3.2220990657806396\n",
      "Loss at step 30250 : 3.095369338989258\n",
      "Loss at step 30300 : 2.38480806350708\n",
      "Loss at step 30350 : 4.121930122375488\n",
      "Loss at step 30400 : 2.5607476234436035\n",
      "Loss at step 30450 : 2.611420154571533\n",
      "Loss at step 30500 : 3.6851320266723633\n",
      "Loss at step 30550 : 2.4322071075439453\n",
      "Loss at step 30600 : 3.0537896156311035\n",
      "Loss at step 30650 : 2.799302101135254\n",
      "Loss at step 30700 : 1.4835950136184692\n",
      "Loss at step 30750 : 2.8761229515075684\n",
      "Loss at step 30800 : 3.818169593811035\n",
      "Loss at step 30850 : 3.289205551147461\n",
      "Loss at step 30900 : 2.4881410598754883\n",
      "Loss at step 30950 : 3.4121651649475098\n",
      "Loss at step 31000 : 3.323838949203491\n",
      "Loss at step 31050 : 2.7511816024780273\n",
      "Loss at step 31100 : 3.0296263694763184\n",
      "Loss at step 31150 : 5.599432945251465\n",
      "Loss at step 31200 : 3.5286970138549805\n",
      "Loss at step 31250 : 3.2410378456115723\n",
      "Loss at step 31300 : 3.0600125789642334\n",
      "Loss at step 31350 : 2.7593398094177246\n",
      "Loss at step 31400 : 2.978248119354248\n",
      "Loss at step 31450 : 2.6890957355499268\n",
      "Loss at step 31500 : 4.587681293487549\n",
      "Loss at step 31550 : 3.0143752098083496\n",
      "Loss at step 31600 : 2.839118242263794\n",
      "Loss at step 31650 : 4.272332191467285\n",
      "Loss at step 31700 : 2.689589738845825\n",
      "Loss at step 31750 : 3.377042293548584\n",
      "Loss at step 31800 : 3.3059237003326416\n",
      "Loss at step 31850 : 2.7711076736450195\n",
      "Loss at step 31900 : 3.226533889770508\n",
      "Loss at step 31950 : 2.7141342163085938\n",
      "Loss at step 32000 : 2.7695322036743164\n",
      "Loss at step 32050 : 3.398977756500244\n",
      "Loss at step 32100 : 5.199274063110352\n",
      "Loss at step 32150 : 3.355506420135498\n",
      "Loss at step 32200 : 3.028038501739502\n",
      "Loss at step 32250 : 3.049180507659912\n",
      "Loss at step 32300 : 2.8294005393981934\n",
      "Loss at step 32350 : 2.817986011505127\n",
      "Loss at step 32400 : 2.3408355712890625\n",
      "Loss at step 32450 : 5.070413589477539\n",
      "Loss at step 32500 : 2.3348283767700195\n",
      "Loss at step 32550 : 3.720248222351074\n",
      "Loss at step 32600 : 3.0321755409240723\n",
      "Loss at step 32650 : 3.1447606086730957\n",
      "Loss at step 32700 : 2.256840944290161\n",
      "Loss at step 32750 : 4.42735481262207\n",
      "Loss at step 32800 : 1.5480424165725708\n",
      "Loss at step 32850 : 3.7573001384735107\n",
      "Loss at step 32900 : 3.480998992919922\n",
      "Loss at step 32950 : 3.059826374053955\n",
      "Loss at step 33000 : 2.233120918273926\n",
      "Loss at step 33050 : 1.7222347259521484\n",
      "Loss at step 33100 : 2.492957353591919\n",
      "Loss at step 33150 : 2.2654757499694824\n",
      "Loss at step 33200 : 2.436213731765747\n",
      "Loss at step 33250 : 3.181669235229492\n",
      "Loss at step 33300 : 2.4177803993225098\n",
      "Loss at step 33350 : 2.107618808746338\n",
      "Loss at step 33400 : 3.2764220237731934\n",
      "Loss at step 33450 : 3.233607292175293\n",
      "Loss at step 33500 : 2.542412757873535\n",
      "Loss at step 33550 : 3.5288784503936768\n",
      "Loss at step 33600 : 2.9255905151367188\n",
      "Loss at step 33650 : 3.2237937450408936\n",
      "Loss at step 33700 : 2.5716116428375244\n",
      "Loss at step 33750 : 3.037407398223877\n",
      "Loss at step 33800 : 1.8725807666778564\n",
      "Loss at step 33850 : 2.8818418979644775\n",
      "Loss at step 33900 : 2.8793396949768066\n",
      "Loss at step 33950 : 3.1638875007629395\n",
      "Loss at step 34000 : 3.476355791091919\n",
      "Loss at step 34050 : 2.6332592964172363\n",
      "Loss at step 34100 : 2.250548839569092\n",
      "Loss at step 34150 : 3.083062171936035\n",
      "Loss at step 34200 : 2.0528688430786133\n",
      "Loss at step 34250 : 2.956943988800049\n",
      "Loss at step 34300 : 2.8265867233276367\n",
      "Loss at step 34350 : 3.1807265281677246\n",
      "Loss at step 34400 : 3.1318235397338867\n",
      "Loss at step 34450 : 2.526010513305664\n",
      "Loss at step 34500 : 2.02077317237854\n",
      "Loss at step 34550 : 2.3889074325561523\n",
      "Loss at step 34600 : 1.9389700889587402\n",
      "Loss at step 34650 : 2.523308515548706\n",
      "Loss at step 34700 : 2.4624600410461426\n",
      "Loss at step 34750 : 2.4887709617614746\n",
      "Loss at step 34800 : 3.9169411659240723\n",
      "Loss at step 34850 : 3.232215404510498\n",
      "Loss at step 34900 : 4.630804538726807\n",
      "Loss at step 34950 : 3.3136768341064453\n",
      "Loss at step 35000 : 3.441073417663574\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 35050 : 3.3349685668945312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 35100 : 3.294724941253662\n",
      "Loss at step 35150 : 3.949187755584717\n",
      "Loss at step 35200 : 2.3460521697998047\n",
      "Loss at step 35250 : 2.9438014030456543\n",
      "Loss at step 35300 : 3.053992748260498\n",
      "Loss at step 35350 : 2.3246192932128906\n",
      "Loss at step 35400 : 3.795971393585205\n",
      "Loss at step 35450 : 2.927128791809082\n",
      "Loss at step 35500 : 3.286423921585083\n",
      "Loss at step 35550 : 3.9401941299438477\n",
      "Loss at step 35600 : 2.8243870735168457\n",
      "Loss at step 35650 : 5.784549236297607\n",
      "Loss at step 35700 : 3.1891446113586426\n",
      "Loss at step 35750 : 2.960413694381714\n",
      "Loss at step 35800 : 3.191460132598877\n",
      "Loss at step 35850 : 3.498077869415283\n",
      "Loss at step 35900 : 2.4937398433685303\n",
      "Loss at step 35950 : 3.155271053314209\n",
      "Loss at step 36000 : 4.75688362121582\n",
      "Loss at step 36050 : 1.8074861764907837\n",
      "Loss at step 36100 : 2.6644864082336426\n",
      "Loss at step 36150 : 2.779261827468872\n",
      "Loss at step 36200 : 2.887299060821533\n",
      "Loss at step 36250 : 2.036921501159668\n",
      "Loss at step 36300 : 3.0575056076049805\n",
      "Loss at step 36350 : 3.2654309272766113\n",
      "Loss at step 36400 : 3.347579002380371\n",
      "Loss at step 36450 : 1.9542332887649536\n",
      "Loss at step 36500 : 2.777653932571411\n",
      "Loss at step 36550 : 2.661935806274414\n",
      "Loss at step 36600 : 2.9822230339050293\n",
      "Loss at step 36650 : 2.7734386920928955\n",
      "Loss at step 36700 : 2.831735610961914\n",
      "Loss at step 36750 : 2.5529603958129883\n",
      "Loss at step 36800 : 3.1633358001708984\n",
      "Loss at step 36850 : 3.198448657989502\n",
      "Loss at step 36900 : 2.6757376194000244\n",
      "Loss at step 36950 : 3.3840813636779785\n",
      "Loss at step 37000 : 3.3038768768310547\n",
      "Loss at step 37050 : 3.8809449672698975\n",
      "Loss at step 37100 : 2.682396650314331\n",
      "Loss at step 37150 : 3.0499677658081055\n",
      "Loss at step 37200 : 3.807281732559204\n",
      "Loss at step 37250 : 3.6073527336120605\n",
      "Loss at step 37300 : 3.6755268573760986\n",
      "Loss at step 37350 : 3.057649612426758\n",
      "Loss at step 37400 : 2.3980813026428223\n",
      "Loss at step 37450 : 2.949746608734131\n",
      "Loss at step 37500 : 3.9267024993896484\n",
      "Loss at step 37550 : 2.814817428588867\n",
      "Loss at step 37600 : 2.8007335662841797\n",
      "Loss at step 37650 : 2.975903034210205\n",
      "Loss at step 37700 : 2.6067659854888916\n",
      "Loss at step 37750 : 3.195850372314453\n",
      "Loss at step 37800 : 3.6601462364196777\n",
      "Loss at step 37850 : 2.9467177391052246\n",
      "Loss at step 37900 : 2.792445182800293\n",
      "Loss at step 37950 : 2.930234432220459\n",
      "Loss at step 38000 : 3.875413656234741\n",
      "Loss at step 38050 : 4.450946807861328\n",
      "Loss at step 38100 : 3.4418766498565674\n",
      "Loss at step 38150 : 1.9655877351760864\n",
      "Loss at step 38200 : 3.27024507522583\n",
      "Loss at step 38250 : 2.677791118621826\n",
      "Loss at step 38300 : 3.1433401107788086\n",
      "Loss at step 38350 : 2.4704525470733643\n",
      "Loss at step 38400 : 2.969338893890381\n",
      "Loss at step 38450 : 3.833836078643799\n",
      "Loss at step 38500 : 3.1030166149139404\n",
      "Loss at step 38550 : 2.5553388595581055\n",
      "Loss at step 38600 : 2.957798957824707\n",
      "Loss at step 38650 : 1.967526912689209\n",
      "Loss at step 38700 : 1.7309471368789673\n",
      "Loss at step 38750 : 3.098067283630371\n",
      "Loss at step 38800 : 3.360353946685791\n",
      "Loss at step 38850 : 3.5326526165008545\n",
      "Loss at step 38900 : 3.459339141845703\n",
      "Loss at step 38950 : 2.7520265579223633\n",
      "Loss at step 39000 : 3.8233673572540283\n",
      "Loss at step 39050 : 3.328007221221924\n",
      "Loss at step 39100 : 2.8225302696228027\n",
      "Loss at step 39150 : 3.0003862380981445\n",
      "Loss at step 39200 : 4.521456241607666\n",
      "Loss at step 39250 : 2.747284173965454\n",
      "Loss at step 39300 : 3.1426589488983154\n",
      "Loss at step 39350 : 3.559751033782959\n",
      "Loss at step 39400 : 2.9304232597351074\n",
      "Loss at step 39450 : 3.0022289752960205\n",
      "Loss at step 39500 : 2.450888156890869\n",
      "Loss at step 39550 : 2.992175817489624\n",
      "Loss at step 39600 : 2.8379054069519043\n",
      "Loss at step 39650 : 3.076749086380005\n",
      "Loss at step 39700 : 2.9626283645629883\n",
      "Loss at step 39750 : 2.231738328933716\n",
      "Loss at step 39800 : 4.520650863647461\n",
      "Loss at step 39850 : 2.8001086711883545\n",
      "Loss at step 39900 : 2.308617353439331\n",
      "Loss at step 39950 : 2.5825843811035156\n",
      "Loss at step 40000 : 3.309021472930908\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 40050 : 3.507599115371704\n",
      "Loss at step 40100 : 2.5481419563293457\n",
      "Loss at step 40150 : 2.4590907096862793\n",
      "Loss at step 40200 : 3.9005722999572754\n",
      "Loss at step 40250 : 2.7149834632873535\n",
      "Loss at step 40300 : 4.0794172286987305\n",
      "Loss at step 40350 : 2.18986177444458\n",
      "Loss at step 40400 : 4.050053596496582\n",
      "Loss at step 40450 : 2.9549686908721924\n",
      "Loss at step 40500 : 2.885921001434326\n",
      "Loss at step 40550 : 3.4538631439208984\n",
      "Loss at step 40600 : 2.9110324382781982\n",
      "Loss at step 40650 : 2.8392391204833984\n",
      "Loss at step 40700 : 3.767544984817505\n",
      "Loss at step 40750 : 2.836674451828003\n",
      "Loss at step 40800 : 3.074028968811035\n",
      "Loss at step 40850 : 4.540505886077881\n",
      "Loss at step 40900 : 3.066221237182617\n",
      "Loss at step 40950 : 2.624619722366333\n",
      "Loss at step 41000 : 3.0671350955963135\n",
      "Loss at step 41050 : 2.906585216522217\n",
      "Loss at step 41100 : 5.250059127807617\n",
      "Loss at step 41150 : 2.78043270111084\n",
      "Loss at step 41200 : 3.2505736351013184\n",
      "Loss at step 41250 : 2.621110439300537\n",
      "Loss at step 41300 : 2.821894645690918\n",
      "Loss at step 41350 : 2.473736047744751\n",
      "Loss at step 41400 : 3.2200894355773926\n",
      "Loss at step 41450 : 2.4767699241638184\n",
      "Loss at step 41500 : 2.9638919830322266\n",
      "Loss at step 41550 : 3.3860011100769043\n",
      "Loss at step 41600 : 3.2026920318603516\n",
      "Loss at step 41650 : 3.5845861434936523\n",
      "Loss at step 41700 : 3.582068920135498\n",
      "Loss at step 41750 : 3.297589063644409\n",
      "Loss at step 41800 : 3.387392997741699\n",
      "Loss at step 41850 : 2.146510601043701\n",
      "Loss at step 41900 : 3.6137821674346924\n",
      "Loss at step 41950 : 4.58204460144043\n",
      "Loss at step 42000 : 3.155709981918335\n",
      "Loss at step 42050 : 2.162055015563965\n",
      "Loss at step 42100 : 3.7377707958221436\n",
      "Loss at step 42150 : 2.419790506362915\n",
      "Loss at step 42200 : 2.347724437713623\n",
      "Loss at step 42250 : 3.4015004634857178\n",
      "Loss at step 42300 : 2.17071533203125\n",
      "Loss at step 42350 : 2.907702684402466\n",
      "Loss at step 42400 : 3.2593941688537598\n",
      "Loss at step 42450 : 3.760845422744751\n",
      "Loss at step 42500 : 4.066725730895996\n",
      "Loss at step 42550 : 2.6238279342651367\n",
      "Loss at step 42600 : 2.266927719116211\n",
      "Loss at step 42650 : 2.423922538757324\n",
      "Loss at step 42700 : 2.088059902191162\n",
      "Loss at step 42750 : 2.9546189308166504\n",
      "Loss at step 42800 : 3.541511058807373\n",
      "Loss at step 42850 : 2.961871385574341\n",
      "Loss at step 42900 : 3.359989881515503\n",
      "Loss at step 42950 : 3.5864973068237305\n",
      "Loss at step 43000 : 3.2946150302886963\n",
      "Loss at step 43050 : 3.0855255126953125\n",
      "Loss at step 43100 : 3.2656407356262207\n",
      "Loss at step 43150 : 2.4337046146392822\n",
      "Loss at step 43200 : 2.5529747009277344\n",
      "Loss at step 43250 : 4.237280368804932\n",
      "Loss at step 43300 : 3.0521492958068848\n",
      "Loss at step 43350 : 4.40003776550293\n",
      "Loss at step 43400 : 2.8371617794036865\n",
      "Loss at step 43450 : 4.615881443023682\n",
      "Loss at step 43500 : 3.404188632965088\n",
      "Loss at step 43550 : 2.569676637649536\n",
      "Loss at step 43600 : 2.8280582427978516\n",
      "Loss at step 43650 : 3.6305341720581055\n",
      "Loss at step 43700 : 3.279026985168457\n",
      "Loss at step 43750 : 5.277103900909424\n",
      "Loss at step 43800 : 3.064094305038452\n",
      "Loss at step 43850 : 2.9171881675720215\n",
      "Loss at step 43900 : 2.3037917613983154\n",
      "Loss at step 43950 : 2.2837133407592773\n",
      "Loss at step 44000 : 3.6585240364074707\n",
      "Loss at step 44050 : 3.07340145111084\n",
      "Loss at step 44100 : 2.695089817047119\n",
      "Loss at step 44150 : 2.165461540222168\n",
      "Loss at step 44200 : 2.5380444526672363\n",
      "Loss at step 44250 : 4.544525146484375\n",
      "Loss at step 44300 : 3.5389881134033203\n",
      "Loss at step 44350 : 2.8128061294555664\n",
      "Loss at step 44400 : 3.261033535003662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 44450 : 3.5000133514404297\n",
      "Loss at step 44500 : 3.4926908016204834\n",
      "Loss at step 44550 : 2.585010051727295\n",
      "Loss at step 44600 : 2.8051161766052246\n",
      "Loss at step 44650 : 2.978146553039551\n",
      "Loss at step 44700 : 4.398280143737793\n",
      "Loss at step 44750 : 3.153144359588623\n",
      "Loss at step 44800 : 2.7498412132263184\n",
      "Loss at step 44850 : 2.6469779014587402\n",
      "Loss at step 44900 : 2.542144298553467\n",
      "Loss at step 44950 : 3.028059482574463\n",
      "Loss at step 45000 : 2.624687910079956\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 45050 : 3.0614566802978516\n",
      "Loss at step 45100 : 3.3880791664123535\n",
      "Loss at step 45150 : 2.901670455932617\n",
      "Loss at step 45200 : 3.3696608543395996\n",
      "Loss at step 45250 : 2.5169529914855957\n",
      "Loss at step 45300 : 4.691902160644531\n",
      "Loss at step 45350 : 3.541411876678467\n",
      "Loss at step 45400 : 3.7419395446777344\n",
      "Loss at step 45450 : 3.677110433578491\n",
      "Loss at step 45500 : 3.618999481201172\n",
      "Loss at step 45550 : 2.4178361892700195\n",
      "Loss at step 45600 : 2.983458995819092\n",
      "Loss at step 45650 : 2.5614871978759766\n",
      "Loss at step 45700 : 3.159292459487915\n",
      "Loss at step 45750 : 2.3260366916656494\n",
      "Loss at step 45800 : 2.784304618835449\n",
      "Loss at step 45850 : 3.347198963165283\n",
      "Loss at step 45900 : 2.741802215576172\n",
      "Loss at step 45950 : 3.060720920562744\n",
      "Loss at step 46000 : 3.0516157150268555\n",
      "Loss at step 46050 : 3.5824058055877686\n",
      "Loss at step 46100 : 3.759793281555176\n",
      "Loss at step 46150 : 3.1648919582366943\n",
      "Loss at step 46200 : 1.9500430822372437\n",
      "Loss at step 46250 : 2.531877040863037\n",
      "Loss at step 46300 : 2.55554461479187\n",
      "Loss at step 46350 : 4.4055585861206055\n",
      "Loss at step 46400 : 2.2574284076690674\n",
      "Loss at step 46450 : 2.6581954956054688\n",
      "Loss at step 46500 : 4.899503707885742\n",
      "Loss at step 46550 : 3.7173893451690674\n",
      "Loss at step 46600 : 3.22792387008667\n",
      "Loss at step 46650 : 4.546759128570557\n",
      "Loss at step 46700 : 2.8262534141540527\n",
      "Loss at step 46750 : 2.077688694000244\n",
      "Loss at step 46800 : 2.5026063919067383\n",
      "Loss at step 46850 : 4.49552059173584\n",
      "Loss at step 46900 : 2.1994452476501465\n",
      "Loss at step 46950 : 1.9346158504486084\n",
      "Loss at step 47000 : 2.6126341819763184\n",
      "Loss at step 47050 : 3.260953426361084\n",
      "Loss at step 47100 : 2.6967198848724365\n",
      "Loss at step 47150 : 4.000591278076172\n",
      "Loss at step 47200 : 2.409475326538086\n",
      "Loss at step 47250 : 2.6905949115753174\n",
      "Loss at step 47300 : 4.473104476928711\n",
      "Loss at step 47350 : 3.3446450233459473\n",
      "Loss at step 47400 : 2.8253276348114014\n",
      "Loss at step 47450 : 4.0687761306762695\n",
      "Loss at step 47500 : 2.714191436767578\n",
      "Loss at step 47550 : 2.2160873413085938\n",
      "Loss at step 47600 : 2.3457677364349365\n",
      "Loss at step 47650 : 2.4231369495391846\n",
      "Loss at step 47700 : 2.513340711593628\n",
      "Loss at step 47750 : 3.3272862434387207\n",
      "Loss at step 47800 : 4.846667766571045\n",
      "Loss at step 47850 : 2.102264881134033\n",
      "Loss at step 47900 : 2.4000942707061768\n",
      "Loss at step 47950 : 4.017676830291748\n",
      "Loss at step 48000 : 2.4858970642089844\n",
      "Loss at step 48050 : 3.31404709815979\n",
      "Loss at step 48100 : 3.2103404998779297\n",
      "Loss at step 48150 : 2.845695734024048\n",
      "Loss at step 48200 : 2.974736452102661\n",
      "Loss at step 48250 : 3.0114526748657227\n",
      "Loss at step 48300 : 2.805673599243164\n",
      "Loss at step 48350 : 3.844968795776367\n",
      "Loss at step 48400 : 2.765974998474121\n",
      "Loss at step 48450 : 4.51992130279541\n",
      "Loss at step 48500 : 3.0494284629821777\n",
      "Loss at step 48550 : 3.819672107696533\n",
      "Loss at step 48600 : 3.7753467559814453\n",
      "Loss at step 48650 : 2.137660026550293\n",
      "Loss at step 48700 : 3.038750648498535\n",
      "Loss at step 48750 : 3.1701955795288086\n",
      "Loss at step 48800 : 2.2464451789855957\n",
      "Loss at step 48850 : 2.370839834213257\n",
      "Loss at step 48900 : 2.2490625381469727\n",
      "Loss at step 48950 : 3.01343035697937\n",
      "Loss at step 49000 : 2.3219687938690186\n",
      "Loss at step 49050 : 2.400146007537842\n",
      "Loss at step 49100 : 2.1010994911193848\n",
      "Loss at step 49150 : 3.542335033416748\n",
      "Loss at step 49200 : 3.4224233627319336\n",
      "Loss at step 49250 : 2.711937427520752\n",
      "Loss at step 49300 : 3.173619508743286\n",
      "Loss at step 49350 : 3.0493502616882324\n",
      "Loss at step 49400 : 2.9273176193237305\n",
      "Loss at step 49450 : 2.690624713897705\n",
      "Loss at step 49500 : 2.7087059020996094\n",
      "Loss at step 49550 : 2.6734445095062256\n",
      "Loss at step 49600 : 2.6542441844940186\n",
      "Loss at step 49650 : 2.227365255355835\n",
      "Loss at step 49700 : 2.8627572059631348\n",
      "Loss at step 49750 : 4.047210216522217\n",
      "Loss at step 49800 : 2.3951797485351562\n",
      "Loss at step 49850 : 2.5521254539489746\n",
      "Loss at step 49900 : 2.2098140716552734\n",
      "Loss at step 49950 : 2.6715707778930664\n",
      "Loss at step 50000 : 2.718907117843628\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 50050 : 2.9783496856689453\n",
      "Loss at step 50100 : 3.1760449409484863\n",
      "Loss at step 50150 : 2.388530969619751\n",
      "Loss at step 50200 : 3.021479368209839\n",
      "Loss at step 50250 : 2.4938063621520996\n",
      "Loss at step 50300 : 2.3233113288879395\n",
      "Loss at step 50350 : 2.7562851905822754\n",
      "Loss at step 50400 : 3.2430171966552734\n",
      "Loss at step 50450 : 2.865321159362793\n",
      "Loss at step 50500 : 3.03067684173584\n",
      "Loss at step 50550 : 3.246877431869507\n",
      "Loss at step 50600 : 3.438636302947998\n",
      "Loss at step 50650 : 2.9761312007904053\n",
      "Loss at step 50700 : 3.500246524810791\n",
      "Loss at step 50750 : 3.18527889251709\n",
      "Loss at step 50800 : 3.1143229007720947\n",
      "Loss at step 50850 : 2.6158971786499023\n",
      "Loss at step 50900 : 3.1568546295166016\n",
      "Loss at step 50950 : 1.7989977598190308\n",
      "Loss at step 51000 : 2.682814359664917\n",
      "Loss at step 51050 : 3.0953121185302734\n",
      "Loss at step 51100 : 3.537353992462158\n",
      "Loss at step 51150 : 3.454507827758789\n",
      "Loss at step 51200 : 3.0130133628845215\n",
      "Loss at step 51250 : 2.241123676300049\n",
      "Loss at step 51300 : 2.5997979640960693\n",
      "Loss at step 51350 : 2.5731635093688965\n",
      "Loss at step 51400 : 2.724147081375122\n",
      "Loss at step 51450 : 3.7565231323242188\n",
      "Loss at step 51500 : 2.362196922302246\n",
      "Loss at step 51550 : 3.3672685623168945\n",
      "Loss at step 51600 : 3.329949140548706\n",
      "Loss at step 51650 : 4.5267229080200195\n",
      "Loss at step 51700 : 3.5806970596313477\n",
      "Loss at step 51750 : 2.96113920211792\n",
      "Loss at step 51800 : 1.64217209815979\n",
      "Loss at step 51850 : 3.507920026779175\n",
      "Loss at step 51900 : 3.1139533519744873\n",
      "Loss at step 51950 : 2.559889793395996\n",
      "Loss at step 52000 : 1.6037551164627075\n",
      "Loss at step 52050 : 2.829784631729126\n",
      "Loss at step 52100 : 2.4430882930755615\n",
      "Loss at step 52150 : 3.5347647666931152\n",
      "Loss at step 52200 : 3.708573579788208\n",
      "Loss at step 52250 : 3.4354004859924316\n",
      "Loss at step 52300 : 2.9458138942718506\n",
      "Loss at step 52350 : 2.6747148036956787\n",
      "Loss at step 52400 : 3.4136738777160645\n",
      "Loss at step 52450 : 2.4355263710021973\n",
      "Loss at step 52500 : 2.8717122077941895\n",
      "Loss at step 52550 : 3.1137428283691406\n",
      "Loss at step 52600 : 3.315382957458496\n",
      "Loss at step 52650 : 2.4661645889282227\n",
      "Loss at step 52700 : 2.5265345573425293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 52750 : 3.4049429893493652\n",
      "Loss at step 52800 : 4.032680034637451\n",
      "Loss at step 52850 : 2.506974697113037\n",
      "Loss at step 52900 : 2.8726511001586914\n",
      "Loss at step 52950 : 2.9346210956573486\n",
      "Loss at step 53000 : 3.245558738708496\n",
      "Loss at step 53050 : 3.2135956287384033\n",
      "Loss at step 53100 : 3.1614885330200195\n",
      "Loss at step 53150 : 3.234816551208496\n",
      "Loss at step 53200 : 2.6116161346435547\n",
      "Loss at step 53250 : 3.0362579822540283\n",
      "Loss at step 53300 : 3.030684232711792\n",
      "Loss at step 53350 : 2.791177272796631\n",
      "Loss at step 53400 : 3.6416001319885254\n",
      "Loss at step 53450 : 2.7366034984588623\n",
      "Loss at step 53500 : 3.728024959564209\n",
      "Loss at step 53550 : 3.6204521656036377\n",
      "Loss at step 53600 : 4.350249290466309\n",
      "Loss at step 53650 : 2.9259676933288574\n",
      "Loss at step 53700 : 2.7715654373168945\n",
      "Loss at step 53750 : 2.6058363914489746\n",
      "Loss at step 53800 : 3.335623264312744\n",
      "Loss at step 53850 : 2.881819725036621\n",
      "Loss at step 53900 : 3.7391552925109863\n",
      "Loss at step 53950 : 2.43271541595459\n",
      "Loss at step 54000 : 3.7410707473754883\n",
      "Loss at step 54050 : 3.6871705055236816\n",
      "Loss at step 54100 : 3.5817832946777344\n",
      "Loss at step 54150 : 2.266503095626831\n",
      "Loss at step 54200 : 3.0237457752227783\n",
      "Loss at step 54250 : 2.744415521621704\n",
      "Loss at step 54300 : 2.834106683731079\n",
      "Loss at step 54350 : 2.6734540462493896\n",
      "Loss at step 54400 : 3.1750752925872803\n",
      "Loss at step 54450 : 3.6287715435028076\n",
      "Loss at step 54500 : 3.162277936935425\n",
      "Loss at step 54550 : 3.113499879837036\n",
      "Loss at step 54600 : 3.232283115386963\n",
      "Loss at step 54650 : 2.319303035736084\n",
      "Loss at step 54700 : 3.0051348209381104\n",
      "Loss at step 54750 : 3.083329677581787\n",
      "Loss at step 54800 : 3.0564613342285156\n",
      "Loss at step 54850 : 2.982895851135254\n",
      "Loss at step 54900 : 3.0536327362060547\n",
      "Loss at step 54950 : 3.4103591442108154\n",
      "Loss at step 55000 : 2.258181571960449\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 55050 : 2.7067315578460693\n",
      "Loss at step 55100 : 4.21042537689209\n",
      "Loss at step 55150 : 2.4492340087890625\n",
      "Loss at step 55200 : 2.6516921520233154\n",
      "Loss at step 55250 : 2.949276924133301\n",
      "Loss at step 55300 : 2.5819337368011475\n",
      "Loss at step 55350 : 2.6684882640838623\n",
      "Loss at step 55400 : 3.350524425506592\n",
      "Loss at step 55450 : 2.0539495944976807\n",
      "Loss at step 55500 : 3.1321470737457275\n",
      "Loss at step 55550 : 3.69278621673584\n",
      "Loss at step 55600 : 2.7709474563598633\n",
      "Loss at step 55650 : 4.014175891876221\n",
      "Loss at step 55700 : 3.529177188873291\n",
      "Loss at step 55750 : 3.074334144592285\n",
      "Loss at step 55800 : 3.427395820617676\n",
      "Loss at step 55850 : 2.3313939571380615\n",
      "Loss at step 55900 : 2.744845151901245\n",
      "Loss at step 55950 : 3.2337281703948975\n",
      "Loss at step 56000 : 3.0400893688201904\n",
      "Loss at step 56050 : 2.4158060550689697\n",
      "Loss at step 56100 : 3.164487838745117\n",
      "Loss at step 56150 : 3.156034231185913\n",
      "Loss at step 56200 : 3.0784285068511963\n",
      "Loss at step 56250 : 2.8629369735717773\n",
      "Loss at step 56300 : 2.7748284339904785\n",
      "Loss at step 56350 : 3.3353464603424072\n",
      "Loss at step 56400 : 2.829317569732666\n",
      "Loss at step 56450 : 3.4711170196533203\n",
      "Loss at step 56500 : 3.328355312347412\n",
      "Loss at step 56550 : 2.1637330055236816\n",
      "Loss at step 56600 : 2.9706287384033203\n",
      "Loss at step 56650 : 3.128103256225586\n",
      "Loss at step 56700 : 2.496493339538574\n",
      "Loss at step 56750 : 5.051811218261719\n",
      "Loss at step 56800 : 2.395585536956787\n",
      "Loss at step 56850 : 2.701833724975586\n",
      "Loss at step 56900 : 2.6421103477478027\n",
      "Loss at step 56950 : 3.351377487182617\n",
      "Loss at step 57000 : 3.276745319366455\n",
      "Loss at step 57050 : 2.6794352531433105\n",
      "Loss at step 57100 : 3.0528295040130615\n",
      "Loss at step 57150 : 4.209886074066162\n",
      "Loss at step 57200 : 3.0780067443847656\n",
      "Loss at step 57250 : 2.5551693439483643\n",
      "Loss at step 57300 : 1.940534234046936\n",
      "Loss at step 57350 : 2.6622724533081055\n",
      "Loss at step 57400 : 2.2218446731567383\n",
      "Loss at step 57450 : 2.5984630584716797\n",
      "Loss at step 57500 : 5.386226654052734\n",
      "Loss at step 57550 : 3.292681932449341\n",
      "Loss at step 57600 : 3.1407206058502197\n",
      "Loss at step 57650 : 1.977440595626831\n",
      "Loss at step 57700 : 2.4920341968536377\n",
      "Loss at step 57750 : 2.709625720977783\n",
      "Loss at step 57800 : 2.7952868938446045\n",
      "Loss at step 57850 : 3.1793482303619385\n",
      "Loss at step 57900 : 2.500525951385498\n",
      "Loss at step 57950 : 3.3336524963378906\n",
      "Loss at step 58000 : 2.9829225540161133\n",
      "Loss at step 58050 : 3.384617328643799\n",
      "Loss at step 58100 : 2.106513738632202\n",
      "Loss at step 58150 : 3.1003382205963135\n",
      "Loss at step 58200 : 2.783710241317749\n",
      "Loss at step 58250 : 3.254149913787842\n",
      "Loss at step 58300 : 2.2616162300109863\n",
      "Loss at step 58350 : 4.961889743804932\n",
      "Loss at step 58400 : 3.4537253379821777\n",
      "Loss at step 58450 : 3.0130515098571777\n",
      "Loss at step 58500 : 2.474134922027588\n",
      "Loss at step 58550 : 2.6023714542388916\n",
      "Loss at step 58600 : 3.46325421333313\n",
      "Loss at step 58650 : 4.393925666809082\n",
      "Loss at step 58700 : 3.033689260482788\n",
      "Loss at step 58750 : 2.586735963821411\n",
      "Loss at step 58800 : 2.811312675476074\n",
      "Loss at step 58850 : 3.1037633419036865\n",
      "Loss at step 58900 : 3.361710786819458\n",
      "Loss at step 58950 : 2.7161941528320312\n",
      "Loss at step 59000 : 3.542205572128296\n",
      "Loss at step 59050 : 3.9870214462280273\n",
      "Loss at step 59100 : 3.403294563293457\n",
      "Loss at step 59150 : 2.1435916423797607\n",
      "Loss at step 59200 : 2.4459726810455322\n",
      "Loss at step 59250 : 2.84855318069458\n",
      "Loss at step 59300 : 2.595839023590088\n",
      "Loss at step 59350 : 2.847895622253418\n",
      "Loss at step 59400 : 2.753063917160034\n",
      "Loss at step 59450 : 2.925386667251587\n",
      "Loss at step 59500 : 4.113198757171631\n",
      "Loss at step 59550 : 2.765360116958618\n",
      "Loss at step 59600 : 2.627383232116699\n",
      "Loss at step 59650 : 1.8123893737792969\n",
      "Loss at step 59700 : 1.5961359739303589\n",
      "Loss at step 59750 : 2.898777484893799\n",
      "Loss at step 59800 : 3.4071881771087646\n",
      "Loss at step 59850 : 2.7010462284088135\n",
      "Loss at step 59900 : 3.146913528442383\n",
      "Loss at step 59950 : 2.859813928604126\n",
      "Loss at step 60000 : 3.1167049407958984\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 60050 : 3.092278003692627\n",
      "Loss at step 60100 : 2.1478939056396484\n",
      "Loss at step 60150 : 2.5006651878356934\n",
      "Loss at step 60200 : 2.728680372238159\n",
      "Loss at step 60250 : 2.657569408416748\n",
      "Loss at step 60300 : 5.2097907066345215\n",
      "Loss at step 60350 : 2.898113250732422\n",
      "Loss at step 60400 : 2.5247483253479004\n",
      "Loss at step 60450 : 3.1567797660827637\n",
      "Loss at step 60500 : 4.0973992347717285\n",
      "Loss at step 60550 : 3.016289234161377\n",
      "Loss at step 60600 : 4.67809534072876\n",
      "Loss at step 60650 : 2.2206788063049316\n",
      "Loss at step 60700 : 1.7552191019058228\n",
      "Loss at step 60750 : 2.975721836090088\n",
      "Loss at step 60800 : 2.036457061767578\n",
      "Loss at step 60850 : 2.0139336585998535\n",
      "Loss at step 60900 : 3.051708221435547\n",
      "Loss at step 60950 : 2.994410276412964\n",
      "Loss at step 61000 : 2.278503894805908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 61050 : 2.35147762298584\n",
      "Loss at step 61100 : 2.4697630405426025\n",
      "Loss at step 61150 : 3.531141519546509\n",
      "Loss at step 61200 : 3.259072780609131\n",
      "Loss at step 61250 : 2.1938300132751465\n",
      "Loss at step 61300 : 2.810722827911377\n",
      "Loss at step 61350 : 2.614015579223633\n",
      "Loss at step 61400 : 3.7856974601745605\n",
      "Loss at step 61450 : 3.321367025375366\n",
      "Loss at step 61500 : 2.5840706825256348\n",
      "Loss at step 61550 : 3.2620344161987305\n",
      "Loss at step 61600 : 3.0081048011779785\n",
      "Loss at step 61650 : 2.590158700942993\n",
      "Loss at step 61700 : 3.92183780670166\n",
      "Loss at step 61750 : 2.4279661178588867\n",
      "Loss at step 61800 : 2.8459084033966064\n",
      "Loss at step 61850 : 2.862119197845459\n",
      "Loss at step 61900 : 2.379037618637085\n",
      "Loss at step 61950 : 2.806828022003174\n",
      "Loss at step 62000 : 3.33433198928833\n",
      "Loss at step 62050 : 2.9618611335754395\n",
      "Loss at step 62100 : 2.570582389831543\n",
      "Loss at step 62150 : 2.872652530670166\n",
      "Loss at step 62200 : 2.899080991744995\n",
      "Loss at step 62250 : 3.517062187194824\n",
      "Loss at step 62300 : 3.3214216232299805\n",
      "Loss at step 62350 : 3.1250503063201904\n",
      "Loss at step 62400 : 3.1796109676361084\n",
      "Loss at step 62450 : 3.2728800773620605\n",
      "Loss at step 62500 : 4.619168281555176\n",
      "Loss at step 62550 : 3.1319572925567627\n",
      "Loss at step 62600 : 2.8323745727539062\n",
      "Loss at step 62650 : 4.116000175476074\n",
      "Loss at step 62700 : 2.544891834259033\n",
      "Loss at step 62750 : 3.1218254566192627\n",
      "Loss at step 62800 : 2.3739523887634277\n",
      "Loss at step 62850 : 3.2604427337646484\n",
      "Loss at step 62900 : 4.4693922996521\n",
      "Loss at step 62950 : 2.6468329429626465\n",
      "Loss at step 63000 : 1.6232631206512451\n",
      "Loss at step 63050 : 4.046207904815674\n",
      "Loss at step 63100 : 1.9827134609222412\n",
      "Loss at step 63150 : 4.1430792808532715\n",
      "Loss at step 63200 : 3.4908699989318848\n",
      "Loss at step 63250 : 3.6918413639068604\n",
      "Loss at step 63300 : 2.872230052947998\n",
      "Loss at step 63350 : 2.6998162269592285\n",
      "Loss at step 63400 : 2.7582688331604004\n",
      "Loss at step 63450 : 2.9941999912261963\n",
      "Loss at step 63500 : 3.5224313735961914\n",
      "Loss at step 63550 : 2.7145113945007324\n",
      "Loss at step 63600 : 3.0011777877807617\n",
      "Loss at step 63650 : 2.336254119873047\n",
      "Loss at step 63700 : 2.2305736541748047\n",
      "Loss at step 63750 : 3.338043689727783\n",
      "Loss at step 63800 : 3.2246713638305664\n",
      "Loss at step 63850 : 2.1802659034729004\n",
      "Loss at step 63900 : 3.461909294128418\n",
      "Loss at step 63950 : 3.0676631927490234\n",
      "Loss at step 64000 : 3.0445337295532227\n",
      "Loss at step 64050 : 2.6701064109802246\n",
      "Loss at step 64100 : 3.8518850803375244\n",
      "Loss at step 64150 : 4.191187381744385\n",
      "Loss at step 64200 : 2.5889625549316406\n",
      "Loss at step 64250 : 3.7244820594787598\n",
      "Loss at step 64300 : 2.4541828632354736\n",
      "Loss at step 64350 : 3.817707061767578\n",
      "Loss at step 64400 : 4.220569610595703\n",
      "Loss at step 64450 : 2.0824685096740723\n",
      "Loss at step 64500 : 2.2561264038085938\n",
      "Loss at step 64550 : 3.0267210006713867\n",
      "Loss at step 64600 : 3.6721174716949463\n",
      "Loss at step 64650 : 4.348870277404785\n",
      "Loss at step 64700 : 2.7593722343444824\n",
      "Loss at step 64750 : 3.230736017227173\n",
      "Loss at step 64800 : 3.4423418045043945\n",
      "Loss at step 64850 : 3.360342025756836\n",
      "Loss at step 64900 : 3.0386505126953125\n",
      "Loss at step 64950 : 2.393890619277954\n",
      "Loss at step 65000 : 4.234074592590332\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 65050 : 3.4204554557800293\n",
      "Loss at step 65100 : 4.677460670471191\n",
      "Loss at step 65150 : 3.0240583419799805\n",
      "Loss at step 65200 : 2.9574081897735596\n",
      "Loss at step 65250 : 3.250565528869629\n",
      "Loss at step 65300 : 3.3381505012512207\n",
      "Loss at step 65350 : 2.7117815017700195\n",
      "Loss at step 65400 : 2.8540515899658203\n",
      "Loss at step 65450 : 3.410447597503662\n",
      "Loss at step 65500 : 2.8285555839538574\n",
      "Loss at step 65550 : 5.0710039138793945\n",
      "Loss at step 65600 : 3.008486747741699\n",
      "Loss at step 65650 : 3.52207088470459\n",
      "Loss at step 65700 : 2.7360570430755615\n",
      "Loss at step 65750 : 2.809931755065918\n",
      "Loss at step 65800 : 2.210610866546631\n",
      "Loss at step 65850 : 3.5423789024353027\n",
      "Loss at step 65900 : 3.1756534576416016\n",
      "Loss at step 65950 : 3.045711040496826\n",
      "Loss at step 66000 : 3.0233569145202637\n",
      "Loss at step 66050 : 2.8676090240478516\n",
      "Loss at step 66100 : 2.6233272552490234\n",
      "Loss at step 66150 : 3.374166488647461\n",
      "Loss at step 66200 : 2.446422576904297\n",
      "Loss at step 66250 : 3.0382533073425293\n",
      "Loss at step 66300 : 3.7767696380615234\n",
      "Loss at step 66350 : 3.9723072052001953\n",
      "Loss at step 66400 : 2.6119470596313477\n",
      "Loss at step 66450 : 2.50427508354187\n",
      "Loss at step 66500 : 2.778202533721924\n",
      "Loss at step 66550 : 3.4176056385040283\n",
      "Loss at step 66600 : 2.327840805053711\n",
      "Loss at step 66650 : 3.0952770709991455\n",
      "Loss at step 66700 : 3.4001481533050537\n",
      "Loss at step 66750 : 2.467061996459961\n",
      "Loss at step 66800 : 2.6808764934539795\n",
      "Loss at step 66850 : 3.369762420654297\n",
      "Loss at step 66900 : 1.7068145275115967\n",
      "Loss at step 66950 : 2.2917869091033936\n",
      "Loss at step 67000 : 3.065349578857422\n",
      "Loss at step 67050 : 3.1823432445526123\n",
      "Loss at step 67100 : 3.2595481872558594\n",
      "Loss at step 67150 : 3.8418357372283936\n",
      "Loss at step 67200 : 3.0372934341430664\n",
      "Loss at step 67250 : 4.199737071990967\n",
      "Loss at step 67300 : 4.8525261878967285\n",
      "Loss at step 67350 : 2.8200531005859375\n",
      "Loss at step 67400 : 3.8093347549438477\n",
      "Loss at step 67450 : 3.308807849884033\n",
      "Loss at step 67500 : 2.943958282470703\n",
      "Loss at step 67550 : 2.748326539993286\n",
      "Loss at step 67600 : 2.8539040088653564\n",
      "Loss at step 67650 : 2.830348491668701\n",
      "Loss at step 67700 : 2.856898784637451\n",
      "Loss at step 67750 : 2.2744901180267334\n",
      "Loss at step 67800 : 3.137263298034668\n",
      "Loss at step 67850 : 2.928239107131958\n",
      "Loss at step 67900 : 3.992973566055298\n",
      "Loss at step 67950 : 2.433600425720215\n",
      "Loss at step 68000 : 2.9133129119873047\n",
      "Loss at step 68050 : 2.745643377304077\n",
      "Loss at step 68100 : 3.2788965702056885\n",
      "Loss at step 68150 : 3.685917615890503\n",
      "Loss at step 68200 : 3.69065260887146\n",
      "Loss at step 68250 : 2.1311633586883545\n",
      "Loss at step 68300 : 2.6892547607421875\n",
      "Loss at step 68350 : 2.9709882736206055\n",
      "Loss at step 68400 : 2.501622200012207\n",
      "Loss at step 68450 : 3.0181732177734375\n",
      "Loss at step 68500 : 3.088954448699951\n",
      "Loss at step 68550 : 2.676919460296631\n",
      "Loss at step 68600 : 4.530633926391602\n",
      "Loss at step 68650 : 3.4022083282470703\n",
      "Loss at step 68700 : 1.4137535095214844\n",
      "Loss at step 68750 : 2.0990498065948486\n",
      "Loss at step 68800 : 2.109182357788086\n",
      "Loss at step 68850 : 2.738142490386963\n",
      "Loss at step 68900 : 2.341947078704834\n",
      "Loss at step 68950 : 4.6464385986328125\n",
      "Loss at step 69000 : 2.864769458770752\n",
      "Loss at step 69050 : 2.1704251766204834\n",
      "Loss at step 69100 : 3.527888536453247\n",
      "Loss at step 69150 : 3.3176090717315674\n",
      "Loss at step 69200 : 3.4817469120025635\n",
      "Loss at step 69250 : 2.62783145904541\n",
      "Loss at step 69300 : 2.53456449508667\n",
      "Loss at step 69350 : 2.9913933277130127\n",
      "Loss at step 69400 : 2.4808101654052734\n",
      "Loss at step 69450 : 2.512402296066284\n",
      "Loss at step 69500 : 3.340665340423584\n",
      "Loss at step 69550 : 2.147758960723877\n",
      "Loss at step 69600 : 2.7950339317321777\n",
      "Loss at step 69650 : 3.003643035888672\n",
      "Loss at step 69700 : 3.198345184326172\n",
      "Loss at step 69750 : 2.3935956954956055\n",
      "Loss at step 69800 : 3.010073184967041\n",
      "Loss at step 69850 : 2.5706710815429688\n",
      "Loss at step 69900 : 3.5441060066223145\n",
      "Loss at step 69950 : 3.1719934940338135\n",
      "Loss at step 70000 : 3.178061008453369\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 70050 : 2.7643790245056152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 70100 : 3.455526113510132\n",
      "Loss at step 70150 : 3.0251290798187256\n",
      "Loss at step 70200 : 3.209392786026001\n",
      "Loss at step 70250 : 2.747838020324707\n",
      "Loss at step 70300 : 2.8251638412475586\n",
      "Loss at step 70350 : 2.612809658050537\n",
      "Loss at step 70400 : 2.62429141998291\n",
      "Loss at step 70450 : 1.949163556098938\n",
      "Loss at step 70500 : 2.6434109210968018\n",
      "Loss at step 70550 : 2.5803346633911133\n",
      "Loss at step 70600 : 2.3998231887817383\n",
      "Loss at step 70650 : 2.8829798698425293\n",
      "Loss at step 70700 : 3.8648643493652344\n",
      "Loss at step 70750 : 3.0215139389038086\n",
      "Loss at step 70800 : 3.0387253761291504\n",
      "Loss at step 70850 : 3.025979995727539\n",
      "Loss at step 70900 : 3.217860698699951\n",
      "Loss at step 70950 : 3.2877650260925293\n",
      "Loss at step 71000 : 2.6710219383239746\n",
      "Loss at step 71050 : 4.38529109954834\n",
      "Loss at step 71100 : 2.19644832611084\n",
      "Loss at step 71150 : 3.0381319522857666\n",
      "Loss at step 71200 : 3.273697853088379\n",
      "Loss at step 71250 : 6.378599166870117\n",
      "Loss at step 71300 : 2.8187286853790283\n",
      "Loss at step 71350 : 1.6848506927490234\n",
      "Loss at step 71400 : 2.585911273956299\n",
      "Loss at step 71450 : 3.3875575065612793\n",
      "Loss at step 71500 : 2.74107027053833\n",
      "Loss at step 71550 : 2.923785448074341\n",
      "Loss at step 71600 : 2.220470428466797\n",
      "Loss at step 71650 : 2.433284282684326\n",
      "Loss at step 71700 : 3.0796196460723877\n",
      "Loss at step 71750 : 4.72145414352417\n",
      "Loss at step 71800 : 4.575773239135742\n",
      "Loss at step 71850 : 3.1194069385528564\n",
      "Loss at step 71900 : 1.5473346710205078\n",
      "Loss at step 71950 : 3.653477430343628\n",
      "Loss at step 72000 : 2.8392813205718994\n",
      "Loss at step 72050 : 3.159641981124878\n",
      "Loss at step 72100 : 2.8762569427490234\n",
      "Loss at step 72150 : 3.2261204719543457\n",
      "Loss at step 72200 : 4.344995021820068\n",
      "Loss at step 72250 : 3.2778139114379883\n",
      "Loss at step 72300 : 2.896181106567383\n",
      "Loss at step 72350 : 2.850466728210449\n",
      "Loss at step 72400 : 3.012299060821533\n",
      "Loss at step 72450 : 3.0646181106567383\n",
      "Loss at step 72500 : 3.763373851776123\n",
      "Loss at step 72550 : 3.6416642665863037\n",
      "Loss at step 72600 : 3.3456201553344727\n",
      "Loss at step 72650 : 2.737982749938965\n",
      "Loss at step 72700 : 2.1476447582244873\n",
      "Loss at step 72750 : 2.686763048171997\n",
      "Loss at step 72800 : 4.098216533660889\n",
      "Loss at step 72850 : 2.518350839614868\n",
      "Loss at step 72900 : 3.103968620300293\n",
      "Loss at step 72950 : 2.0392987728118896\n",
      "Loss at step 73000 : 2.0915305614471436\n",
      "Loss at step 73050 : 2.784055709838867\n",
      "Loss at step 73100 : 2.870561122894287\n",
      "Loss at step 73150 : 3.781019687652588\n",
      "Loss at step 73200 : 3.574662208557129\n",
      "Loss at step 73250 : 3.249396800994873\n",
      "Loss at step 73300 : 3.1474556922912598\n",
      "Loss at step 73350 : 2.2358412742614746\n",
      "Loss at step 73400 : 1.74504816532135\n",
      "Loss at step 73450 : 2.0741047859191895\n",
      "Loss at step 73500 : 3.8909201622009277\n",
      "Loss at step 73550 : 3.380098819732666\n",
      "Loss at step 73600 : 2.257096290588379\n",
      "Loss at step 73650 : 3.228593111038208\n",
      "Loss at step 73700 : 2.437617063522339\n",
      "Loss at step 73750 : 3.37483286857605\n",
      "Loss at step 73800 : 2.5715830326080322\n",
      "Loss at step 73850 : 2.5740480422973633\n",
      "Loss at step 73900 : 2.3995251655578613\n",
      "Loss at step 73950 : 3.058710813522339\n",
      "Loss at step 74000 : 2.554090976715088\n",
      "Loss at step 74050 : 3.0528063774108887\n",
      "Loss at step 74100 : 2.879077434539795\n",
      "Loss at step 74150 : 3.028226852416992\n",
      "Loss at step 74200 : 2.4951558113098145\n",
      "Loss at step 74250 : 2.1995060443878174\n",
      "Loss at step 74300 : 2.8668575286865234\n",
      "Loss at step 74350 : 2.8727612495422363\n",
      "Loss at step 74400 : 3.470463752746582\n",
      "Loss at step 74450 : 3.111180067062378\n",
      "Loss at step 74500 : 3.64738392829895\n",
      "Loss at step 74550 : 2.989651679992676\n",
      "Loss at step 74600 : 2.2318434715270996\n",
      "Loss at step 74650 : 3.2219183444976807\n",
      "Loss at step 74700 : 2.4418575763702393\n",
      "Loss at step 74750 : 3.1564595699310303\n",
      "Loss at step 74800 : 3.6153345108032227\n",
      "Loss at step 74850 : 2.0125675201416016\n",
      "Loss at step 74900 : 2.640268325805664\n",
      "Loss at step 74950 : 2.67923641204834\n",
      "Loss at step 75000 : 1.5838041305541992\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 75050 : 2.811521530151367\n",
      "Loss at step 75100 : 2.5986857414245605\n",
      "Loss at step 75150 : 2.9303393363952637\n",
      "Loss at step 75200 : 3.1965012550354004\n",
      "Loss at step 75250 : 2.6703531742095947\n",
      "Loss at step 75300 : 2.797861099243164\n",
      "Loss at step 75350 : 2.7726659774780273\n",
      "Loss at step 75400 : 2.65427303314209\n",
      "Loss at step 75450 : 3.105726480484009\n",
      "Loss at step 75500 : 3.4094276428222656\n",
      "Loss at step 75550 : 3.0518958568573\n",
      "Loss at step 75600 : 2.6002726554870605\n",
      "Loss at step 75650 : 2.9351320266723633\n",
      "Loss at step 75700 : 2.5420000553131104\n",
      "Loss at step 75750 : 2.836927652359009\n",
      "Loss at step 75800 : 2.831648349761963\n",
      "Loss at step 75850 : 2.5488696098327637\n",
      "Loss at step 75900 : 3.3950319290161133\n",
      "Loss at step 75950 : 2.767455816268921\n",
      "Loss at step 76000 : 2.905626058578491\n",
      "Loss at step 76050 : 4.786494255065918\n",
      "Loss at step 76100 : 2.567601203918457\n",
      "Loss at step 76150 : 2.7625579833984375\n",
      "Loss at step 76200 : 4.060854434967041\n",
      "Loss at step 76250 : 3.2392683029174805\n",
      "Loss at step 76300 : 3.3434979915618896\n",
      "Loss at step 76350 : 2.9963631629943848\n",
      "Loss at step 76400 : 2.70611572265625\n",
      "Loss at step 76450 : 2.6779091358184814\n",
      "Loss at step 76500 : 3.282456159591675\n",
      "Loss at step 76550 : 2.6663546562194824\n",
      "Loss at step 76600 : 1.8919275999069214\n",
      "Loss at step 76650 : 4.772430896759033\n",
      "Loss at step 76700 : 3.5729308128356934\n",
      "Loss at step 76750 : 2.086667537689209\n",
      "Loss at step 76800 : 2.7692854404449463\n",
      "Loss at step 76850 : 2.164386749267578\n",
      "Loss at step 76900 : 2.676278591156006\n",
      "Loss at step 76950 : 2.789614200592041\n",
      "Loss at step 77000 : 2.576911449432373\n",
      "Loss at step 77050 : 2.8111939430236816\n",
      "Loss at step 77100 : 2.648833990097046\n",
      "Loss at step 77150 : 2.5601353645324707\n",
      "Loss at step 77200 : 3.06492280960083\n",
      "Loss at step 77250 : 2.21649169921875\n",
      "Loss at step 77300 : 2.6599621772766113\n",
      "Loss at step 77350 : 3.692890167236328\n",
      "Loss at step 77400 : 2.9260501861572266\n",
      "Loss at step 77450 : 2.8939175605773926\n",
      "Loss at step 77500 : 3.3442628383636475\n",
      "Loss at step 77550 : 3.2530534267425537\n",
      "Loss at step 77600 : 2.8692355155944824\n",
      "Loss at step 77650 : 2.64719295501709\n",
      "Loss at step 77700 : 3.540900230407715\n",
      "Loss at step 77750 : 3.3825645446777344\n",
      "Loss at step 77800 : 4.053548812866211\n",
      "Loss at step 77850 : 2.521850109100342\n",
      "Loss at step 77900 : 2.7311763763427734\n",
      "Loss at step 77950 : 2.646890640258789\n",
      "Loss at step 78000 : 2.5142159461975098\n",
      "Loss at step 78050 : 4.397477149963379\n",
      "Loss at step 78100 : 3.709223747253418\n",
      "Loss at step 78150 : 3.267087936401367\n",
      "Loss at step 78200 : 4.362853527069092\n",
      "Loss at step 78250 : 2.9272146224975586\n",
      "Loss at step 78300 : 2.6461164951324463\n",
      "Loss at step 78350 : 2.446653127670288\n",
      "Loss at step 78400 : 2.5298690795898438\n",
      "Loss at step 78450 : 2.1456491947174072\n",
      "Loss at step 78500 : 2.521667718887329\n",
      "Loss at step 78550 : 2.737940549850464\n",
      "Loss at step 78600 : 2.681941509246826\n",
      "Loss at step 78650 : 2.9380884170532227\n",
      "Loss at step 78700 : 3.306816577911377\n",
      "Loss at step 78750 : 4.122007369995117\n",
      "Loss at step 78800 : 3.5042426586151123\n",
      "Loss at step 78850 : 3.1654021739959717\n",
      "Loss at step 78900 : 2.293745994567871\n",
      "Loss at step 78950 : 3.0069174766540527\n",
      "Loss at step 79000 : 3.1974430084228516\n",
      "Loss at step 79050 : 3.6002612113952637\n",
      "Loss at step 79100 : 2.523195266723633\n",
      "Loss at step 79150 : 3.3728132247924805\n",
      "Loss at step 79200 : 3.2161307334899902\n",
      "Loss at step 79250 : 2.637885808944702\n",
      "Loss at step 79300 : 2.5114080905914307\n",
      "Loss at step 79350 : 3.1313977241516113\n",
      "Loss at step 79400 : 2.5851752758026123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 79450 : 2.933208465576172\n",
      "Loss at step 79500 : 3.0817718505859375\n",
      "Loss at step 79550 : 3.194990634918213\n",
      "Loss at step 79600 : 3.171724319458008\n",
      "Loss at step 79650 : 2.569218635559082\n",
      "Loss at step 79700 : 2.679332971572876\n",
      "Loss at step 79750 : 3.537076950073242\n",
      "Loss at step 79800 : 2.2457528114318848\n",
      "Loss at step 79850 : 2.9160494804382324\n",
      "Loss at step 79900 : 3.4970896244049072\n",
      "Loss at step 79950 : 2.480205535888672\n",
      "Loss at step 80000 : 2.5226991176605225\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 80050 : 3.245539665222168\n",
      "Loss at step 80100 : 3.2080583572387695\n",
      "Loss at step 80150 : 2.6621179580688477\n",
      "Loss at step 80200 : 2.553144931793213\n",
      "Loss at step 80250 : 4.301834583282471\n",
      "Loss at step 80300 : 3.0145106315612793\n",
      "Loss at step 80350 : 2.1696066856384277\n",
      "Loss at step 80400 : 2.3339710235595703\n",
      "Loss at step 80450 : 2.9335741996765137\n",
      "Loss at step 80500 : 3.52640962600708\n",
      "Loss at step 80550 : 2.6010937690734863\n",
      "Loss at step 80600 : 3.9100561141967773\n",
      "Loss at step 80650 : 2.6764512062072754\n",
      "Loss at step 80700 : 3.295264959335327\n",
      "Loss at step 80750 : 2.132256507873535\n",
      "Loss at step 80800 : 3.1872637271881104\n",
      "Loss at step 80850 : 2.619812488555908\n",
      "Loss at step 80900 : 3.1526503562927246\n",
      "Loss at step 80950 : 3.952613353729248\n",
      "Loss at step 81000 : 2.7019786834716797\n",
      "Loss at step 81050 : 2.8375394344329834\n",
      "Loss at step 81100 : 2.896341323852539\n",
      "Loss at step 81150 : 3.652867555618286\n",
      "Loss at step 81200 : 3.228982448577881\n",
      "Loss at step 81250 : 2.458540439605713\n",
      "Loss at step 81300 : 4.689519882202148\n",
      "Loss at step 81350 : 2.7097883224487305\n",
      "Loss at step 81400 : 3.0156967639923096\n",
      "Loss at step 81450 : 2.822849750518799\n",
      "Loss at step 81500 : 3.0248217582702637\n",
      "Loss at step 81550 : 4.01661491394043\n",
      "Loss at step 81600 : 2.2688703536987305\n",
      "Loss at step 81650 : 2.552461624145508\n",
      "Loss at step 81700 : 2.64874267578125\n",
      "Loss at step 81750 : 2.492837905883789\n",
      "Loss at step 81800 : 1.8445137739181519\n",
      "Loss at step 81850 : 3.102234363555908\n",
      "Loss at step 81900 : 3.3750672340393066\n",
      "Loss at step 81950 : 2.746887445449829\n",
      "Loss at step 82000 : 2.67427921295166\n",
      "Loss at step 82050 : 2.7343764305114746\n",
      "Loss at step 82100 : 2.6597890853881836\n",
      "Loss at step 82150 : 2.6724627017974854\n",
      "Loss at step 82200 : 3.782360792160034\n",
      "Loss at step 82250 : 3.632012367248535\n",
      "Loss at step 82300 : 5.049878120422363\n",
      "Loss at step 82350 : 2.5983333587646484\n",
      "Loss at step 82400 : 2.264073610305786\n",
      "Loss at step 82450 : 2.5839385986328125\n",
      "Loss at step 82500 : 3.2667317390441895\n",
      "Loss at step 82550 : 3.4583749771118164\n",
      "Loss at step 82600 : 2.7698702812194824\n",
      "Loss at step 82650 : 2.8497419357299805\n",
      "Loss at step 82700 : 1.9889037609100342\n",
      "Loss at step 82750 : 2.7886433601379395\n",
      "Loss at step 82800 : 3.1068484783172607\n",
      "Loss at step 82850 : 2.8534154891967773\n",
      "Loss at step 82900 : 3.1273012161254883\n",
      "Loss at step 82950 : 3.1194047927856445\n",
      "Loss at step 83000 : 3.212397336959839\n",
      "Loss at step 83050 : 2.716604232788086\n",
      "Loss at step 83100 : 3.0266668796539307\n",
      "Loss at step 83150 : 3.167073965072632\n",
      "Loss at step 83200 : 2.7434616088867188\n",
      "Loss at step 83250 : 2.841458320617676\n",
      "Loss at step 83300 : 3.3662517070770264\n",
      "Loss at step 83350 : 3.09163761138916\n",
      "Loss at step 83400 : 2.92018461227417\n",
      "Loss at step 83450 : 3.2057580947875977\n",
      "Loss at step 83500 : 2.8104982376098633\n",
      "Loss at step 83550 : 3.9245381355285645\n",
      "Loss at step 83600 : 3.394205093383789\n",
      "Loss at step 83650 : 2.213059425354004\n",
      "Loss at step 83700 : 3.4720211029052734\n",
      "Loss at step 83750 : 3.441713333129883\n",
      "Loss at step 83800 : 2.883474826812744\n",
      "Loss at step 83850 : 4.13055944442749\n",
      "Loss at step 83900 : 2.9657516479492188\n",
      "Loss at step 83950 : 3.2180469036102295\n",
      "Loss at step 84000 : 2.866593360900879\n",
      "Loss at step 84050 : 3.181971549987793\n",
      "Loss at step 84100 : 3.067765712738037\n",
      "Loss at step 84150 : 2.904151439666748\n",
      "Loss at step 84200 : 2.218553304672241\n",
      "Loss at step 84250 : 1.8560787439346313\n",
      "Loss at step 84300 : 3.1314101219177246\n",
      "Loss at step 84350 : 2.915127992630005\n",
      "Loss at step 84400 : 3.296234607696533\n",
      "Loss at step 84450 : 3.3109688758850098\n",
      "Loss at step 84500 : 4.256147384643555\n",
      "Loss at step 84550 : 2.8964626789093018\n",
      "Loss at step 84600 : 2.0921387672424316\n",
      "Loss at step 84650 : 3.4675052165985107\n",
      "Loss at step 84700 : 2.8011465072631836\n",
      "Loss at step 84750 : 2.461644172668457\n",
      "Loss at step 84800 : 2.743131637573242\n",
      "Loss at step 84850 : 4.026745796203613\n",
      "Loss at step 84900 : 3.0345969200134277\n",
      "Loss at step 84950 : 2.468881607055664\n",
      "Loss at step 85000 : 1.892122745513916\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 85050 : 3.252808094024658\n",
      "Loss at step 85100 : 2.6890509128570557\n",
      "Loss at step 85150 : 2.0192718505859375\n",
      "Loss at step 85200 : 2.2534656524658203\n",
      "Loss at step 85250 : 3.062356472015381\n",
      "Loss at step 85300 : 3.3924593925476074\n",
      "Loss at step 85350 : 2.51733136177063\n",
      "Loss at step 85400 : 3.5809531211853027\n",
      "Loss at step 85450 : 3.5403242111206055\n",
      "Loss at step 85500 : 2.9938392639160156\n",
      "Loss at step 85550 : 3.532444953918457\n",
      "Loss at step 85600 : 3.9515116214752197\n",
      "Loss at step 85650 : 3.0308008193969727\n",
      "Loss at step 85700 : 2.410623073577881\n",
      "Loss at step 85750 : 3.0183329582214355\n",
      "Loss at step 85800 : 3.077256679534912\n",
      "Loss at step 85850 : 2.170820951461792\n",
      "Loss at step 85900 : 2.462836503982544\n",
      "Loss at step 85950 : 2.403258800506592\n",
      "Loss at step 86000 : 3.7268528938293457\n",
      "Loss at step 86050 : 2.736659526824951\n",
      "Loss at step 86100 : 5.43744421005249\n",
      "Loss at step 86150 : 3.453132390975952\n",
      "Loss at step 86200 : 2.936098575592041\n",
      "Loss at step 86250 : 2.6950879096984863\n",
      "Loss at step 86300 : 3.421682596206665\n",
      "Loss at step 86350 : 2.6844193935394287\n",
      "Loss at step 86400 : 3.2593650817871094\n",
      "Loss at step 86450 : 2.6418731212615967\n",
      "Loss at step 86500 : 2.9215660095214844\n",
      "Loss at step 86550 : 3.2437267303466797\n",
      "Loss at step 86600 : 3.383129119873047\n",
      "Loss at step 86650 : 2.14880633354187\n",
      "Loss at step 86700 : 2.810311794281006\n",
      "Loss at step 86750 : 2.7707343101501465\n",
      "Loss at step 86800 : 2.829014301300049\n",
      "Loss at step 86850 : 3.846271276473999\n",
      "Loss at step 86900 : 2.7601711750030518\n",
      "Loss at step 86950 : 3.2233235836029053\n",
      "Loss at step 87000 : 2.7938554286956787\n",
      "Loss at step 87050 : 3.067643165588379\n",
      "Loss at step 87100 : 2.413843870162964\n",
      "Loss at step 87150 : 3.2631287574768066\n",
      "Loss at step 87200 : 3.4037015438079834\n",
      "Loss at step 87250 : 3.4134714603424072\n",
      "Loss at step 87300 : 2.8190817832946777\n",
      "Loss at step 87350 : 3.6648664474487305\n",
      "Loss at step 87400 : 2.5571765899658203\n",
      "Loss at step 87450 : 3.254906177520752\n",
      "Loss at step 87500 : 3.801584243774414\n",
      "Loss at step 87550 : 3.0539743900299072\n",
      "Loss at step 87600 : 1.9333586692810059\n",
      "Loss at step 87650 : 2.9340810775756836\n",
      "Loss at step 87700 : 2.920989751815796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 87750 : 4.112751007080078\n",
      "Loss at step 87800 : 2.8107504844665527\n",
      "Loss at step 87850 : 2.5344772338867188\n",
      "Loss at step 87900 : 1.9901286363601685\n",
      "Loss at step 87950 : 2.456120014190674\n",
      "Loss at step 88000 : 2.8223958015441895\n",
      "Loss at step 88050 : 3.4180033206939697\n",
      "Loss at step 88100 : 2.8692855834960938\n",
      "Loss at step 88150 : 2.1320643424987793\n",
      "Loss at step 88200 : 3.657623767852783\n",
      "Loss at step 88250 : 2.5709657669067383\n",
      "Loss at step 88300 : 2.976458787918091\n",
      "Loss at step 88350 : 2.9660325050354004\n",
      "Loss at step 88400 : 1.965456485748291\n",
      "Loss at step 88450 : 2.6392407417297363\n",
      "Loss at step 88500 : 2.255122661590576\n",
      "Loss at step 88550 : 3.5336127281188965\n",
      "Loss at step 88600 : 3.164696216583252\n",
      "Loss at step 88650 : 2.7253079414367676\n",
      "Loss at step 88700 : 2.8501718044281006\n",
      "Loss at step 88750 : 3.1472620964050293\n",
      "Loss at step 88800 : 2.434634208679199\n",
      "Loss at step 88850 : 3.138693332672119\n",
      "Loss at step 88900 : 2.739577293395996\n",
      "Loss at step 88950 : 3.22391414642334\n",
      "Loss at step 89000 : 2.7682137489318848\n",
      "Loss at step 89050 : 3.555011749267578\n",
      "Loss at step 89100 : 2.6275453567504883\n",
      "Loss at step 89150 : 2.670543670654297\n",
      "Loss at step 89200 : 2.715203285217285\n",
      "Loss at step 89250 : 3.377802848815918\n",
      "Loss at step 89300 : 2.8633527755737305\n",
      "Loss at step 89350 : 3.165782928466797\n",
      "Loss at step 89400 : 3.4081358909606934\n",
      "Loss at step 89450 : 2.6836109161376953\n",
      "Loss at step 89500 : 2.984731674194336\n",
      "Loss at step 89550 : 3.714779853820801\n",
      "Loss at step 89600 : 2.748727798461914\n",
      "Loss at step 89650 : 3.853595733642578\n",
      "Loss at step 89700 : 3.3213353157043457\n",
      "Loss at step 89750 : 3.2832579612731934\n",
      "Loss at step 89800 : 4.2181315422058105\n",
      "Loss at step 89850 : 2.710750102996826\n",
      "Loss at step 89900 : 2.2494678497314453\n",
      "Loss at step 89950 : 2.600311279296875\n",
      "Loss at step 90000 : 2.885976791381836\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 90050 : 2.1989269256591797\n",
      "Loss at step 90100 : 3.0035624504089355\n",
      "Loss at step 90150 : 2.7198405265808105\n",
      "Loss at step 90200 : 2.8882927894592285\n",
      "Loss at step 90250 : 2.586087226867676\n",
      "Loss at step 90300 : 3.7564949989318848\n",
      "Loss at step 90350 : 3.74149227142334\n",
      "Loss at step 90400 : 1.98855721950531\n",
      "Loss at step 90450 : 2.7294626235961914\n",
      "Loss at step 90500 : 3.5339832305908203\n",
      "Loss at step 90550 : 3.371234178543091\n",
      "Loss at step 90600 : 2.175036668777466\n",
      "Loss at step 90650 : 3.18275785446167\n",
      "Loss at step 90700 : 4.680720806121826\n",
      "Loss at step 90750 : 3.174849033355713\n",
      "Loss at step 90800 : 3.117722988128662\n",
      "Loss at step 90850 : 4.518608093261719\n",
      "Loss at step 90900 : 2.779390811920166\n",
      "Loss at step 90950 : 2.8684425354003906\n",
      "Loss at step 91000 : 2.4811904430389404\n",
      "Loss at step 91050 : 4.840391159057617\n",
      "Loss at step 91100 : 3.319868803024292\n",
      "Loss at step 91150 : 3.4795496463775635\n",
      "Loss at step 91200 : 4.116644382476807\n",
      "Loss at step 91250 : 2.5184340476989746\n",
      "Loss at step 91300 : 3.1750099658966064\n",
      "Loss at step 91350 : 2.5689220428466797\n",
      "Loss at step 91400 : 2.5718860626220703\n",
      "Loss at step 91450 : 3.1183700561523438\n",
      "Loss at step 91500 : 3.7744858264923096\n",
      "Loss at step 91550 : 3.2290072441101074\n",
      "Loss at step 91600 : 3.194129228591919\n",
      "Loss at step 91650 : 2.5642127990722656\n",
      "Loss at step 91700 : 2.4551897048950195\n",
      "Loss at step 91750 : 2.2295680046081543\n",
      "Loss at step 91800 : 3.436201572418213\n",
      "Loss at step 91850 : 3.0110018253326416\n",
      "Loss at step 91900 : 3.4060492515563965\n",
      "Loss at step 91950 : 2.410027027130127\n",
      "Loss at step 92000 : 1.8719305992126465\n",
      "Loss at step 92050 : 3.1807594299316406\n",
      "Loss at step 92100 : 1.97087562084198\n",
      "Loss at step 92150 : 3.607877731323242\n",
      "Loss at step 92200 : 2.1676125526428223\n",
      "Loss at step 92250 : 4.320487976074219\n",
      "Loss at step 92300 : 3.287144660949707\n",
      "Loss at step 92350 : 3.2013401985168457\n",
      "Loss at step 92400 : 3.2598140239715576\n",
      "Loss at step 92450 : 3.3458192348480225\n",
      "Loss at step 92500 : 3.3605899810791016\n",
      "Loss at step 92550 : 2.2817745208740234\n",
      "Loss at step 92600 : 2.6717216968536377\n",
      "Loss at step 92650 : 2.816338539123535\n",
      "Loss at step 92700 : 1.9265073537826538\n",
      "Loss at step 92750 : 4.036812782287598\n",
      "Loss at step 92800 : 2.413558006286621\n",
      "Loss at step 92850 : 2.6658928394317627\n",
      "Loss at step 92900 : 2.7462353706359863\n",
      "Loss at step 92950 : 4.279146194458008\n",
      "Loss at step 93000 : 2.1888339519500732\n",
      "Loss at step 93050 : 2.9162304401397705\n",
      "Loss at step 93100 : 2.538384199142456\n",
      "Loss at step 93150 : 3.0384912490844727\n",
      "Loss at step 93200 : 2.719459056854248\n",
      "Loss at step 93250 : 3.02030086517334\n",
      "Loss at step 93300 : 3.308742046356201\n",
      "Loss at step 93350 : 3.2706565856933594\n",
      "Loss at step 93400 : 2.3775815963745117\n",
      "Loss at step 93450 : 3.1390247344970703\n",
      "Loss at step 93500 : 2.9129340648651123\n",
      "Loss at step 93550 : 2.830430507659912\n",
      "Loss at step 93600 : 3.255126476287842\n",
      "Loss at step 93650 : 3.0266969203948975\n",
      "Loss at step 93700 : 3.5947248935699463\n",
      "Loss at step 93750 : 3.222038745880127\n",
      "Loss at step 93800 : 3.9746577739715576\n",
      "Loss at step 93850 : 3.3124845027923584\n",
      "Loss at step 93900 : 3.2261972427368164\n",
      "Loss at step 93950 : 3.903388261795044\n",
      "Loss at step 94000 : 2.2045669555664062\n",
      "Loss at step 94050 : 2.5453810691833496\n",
      "Loss at step 94100 : 2.56693172454834\n",
      "Loss at step 94150 : 2.3830671310424805\n",
      "Loss at step 94200 : 3.4462621212005615\n",
      "Loss at step 94250 : 2.9641571044921875\n",
      "Loss at step 94300 : 3.3768725395202637\n",
      "Loss at step 94350 : 2.453244924545288\n",
      "Loss at step 94400 : 3.770328998565674\n",
      "Loss at step 94450 : 3.058574676513672\n",
      "Loss at step 94500 : 2.2572178840637207\n",
      "Loss at step 94550 : 3.2338767051696777\n",
      "Loss at step 94600 : 2.745995044708252\n",
      "Loss at step 94650 : 2.6865901947021484\n",
      "Loss at step 94700 : 3.3177173137664795\n",
      "Loss at step 94750 : 2.9436404705047607\n",
      "Loss at step 94800 : 3.4965598583221436\n",
      "Loss at step 94850 : 2.481635093688965\n",
      "Loss at step 94900 : 2.8839480876922607\n",
      "Loss at step 94950 : 3.4189062118530273\n",
      "Loss at step 95000 : 3.7068638801574707\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 95050 : 2.2806529998779297\n",
      "Loss at step 95100 : 2.719215154647827\n",
      "Loss at step 95150 : 2.6499252319335938\n",
      "Loss at step 95200 : 3.330692768096924\n",
      "Loss at step 95250 : 3.225259780883789\n",
      "Loss at step 95300 : 2.7098238468170166\n",
      "Loss at step 95350 : 4.458924770355225\n",
      "Loss at step 95400 : 2.6828508377075195\n",
      "Loss at step 95450 : 2.8756096363067627\n",
      "Loss at step 95500 : 2.5728821754455566\n",
      "Loss at step 95550 : 2.9319207668304443\n",
      "Loss at step 95600 : 2.328498601913452\n",
      "Loss at step 95650 : 2.0695741176605225\n",
      "Loss at step 95700 : 1.8771382570266724\n",
      "Loss at step 95750 : 3.3770737648010254\n",
      "Loss at step 95800 : 3.421566963195801\n",
      "Loss at step 95850 : 3.726388931274414\n",
      "Loss at step 95900 : 2.629080295562744\n",
      "Loss at step 95950 : 2.413003921508789\n",
      "Loss at step 96000 : 3.0279932022094727\n",
      "Loss at step 96050 : 2.719893217086792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 96100 : 2.91772198677063\n",
      "Loss at step 96150 : 3.5511584281921387\n",
      "Loss at step 96200 : 3.1501173973083496\n",
      "Loss at step 96250 : 3.1703107357025146\n",
      "Loss at step 96300 : 2.2520477771759033\n",
      "Loss at step 96350 : 3.1045126914978027\n",
      "Loss at step 96400 : 2.5672388076782227\n",
      "Loss at step 96450 : 2.832186460494995\n",
      "Loss at step 96500 : 2.9874162673950195\n",
      "Loss at step 96550 : 2.4934306144714355\n",
      "Loss at step 96600 : 2.796048402786255\n",
      "Loss at step 96650 : 3.1495492458343506\n",
      "Loss at step 96700 : 2.9061131477355957\n",
      "Loss at step 96750 : 3.1796188354492188\n",
      "Loss at step 96800 : 3.183058977127075\n",
      "Loss at step 96850 : 3.65494704246521\n",
      "Loss at step 96900 : 2.0662708282470703\n",
      "Loss at step 96950 : 3.2090110778808594\n",
      "Loss at step 97000 : 5.134377956390381\n",
      "Loss at step 97050 : 2.585972309112549\n",
      "Loss at step 97100 : 2.3378734588623047\n",
      "Loss at step 97150 : 2.813228130340576\n",
      "Loss at step 97200 : 2.8289942741394043\n",
      "Loss at step 97250 : 2.7872915267944336\n",
      "Loss at step 97300 : 3.0469512939453125\n",
      "Loss at step 97350 : 2.8821070194244385\n",
      "Loss at step 97400 : 2.555187940597534\n",
      "Loss at step 97450 : 2.728943109512329\n",
      "Loss at step 97500 : 2.226461887359619\n",
      "Loss at step 97550 : 3.0440783500671387\n",
      "Loss at step 97600 : 3.427253246307373\n",
      "Loss at step 97650 : 4.056805610656738\n",
      "Loss at step 97700 : 3.1361241340637207\n",
      "Loss at step 97750 : 3.3311662673950195\n",
      "Loss at step 97800 : 3.19633150100708\n",
      "Loss at step 97850 : 2.7686896324157715\n",
      "Loss at step 97900 : 3.8053512573242188\n",
      "Loss at step 97950 : 3.58524227142334\n",
      "Loss at step 98000 : 3.107461452484131\n",
      "Loss at step 98050 : 2.3216981887817383\n",
      "Loss at step 98100 : 3.903564453125\n",
      "Loss at step 98150 : 2.5960817337036133\n",
      "Loss at step 98200 : 2.9357404708862305\n",
      "Loss at step 98250 : 3.2888951301574707\n",
      "Loss at step 98300 : 4.094468116760254\n",
      "Loss at step 98350 : 2.9116570949554443\n",
      "Loss at step 98400 : 3.1395092010498047\n",
      "Loss at step 98450 : 2.985323429107666\n",
      "Loss at step 98500 : 2.598564624786377\n",
      "Loss at step 98550 : 3.3416895866394043\n",
      "Loss at step 98600 : 3.2929000854492188\n",
      "Loss at step 98650 : 3.006148099899292\n",
      "Loss at step 98700 : 3.2611985206604004\n",
      "Loss at step 98750 : 3.7387335300445557\n",
      "Loss at step 98800 : 4.362363338470459\n",
      "Loss at step 98850 : 3.3874897956848145\n",
      "Loss at step 98900 : 2.0135140419006348\n",
      "Loss at step 98950 : 2.4137682914733887\n",
      "Loss at step 99000 : 2.56710147857666\n",
      "Loss at step 99050 : 3.9658141136169434\n",
      "Loss at step 99100 : 3.177842855453491\n",
      "Loss at step 99150 : 2.98537278175354\n",
      "Loss at step 99200 : 3.2555551528930664\n",
      "Loss at step 99250 : 2.0373597145080566\n",
      "Loss at step 99300 : 1.995531439781189\n",
      "Loss at step 99350 : 2.5337634086608887\n",
      "Loss at step 99400 : 2.3525569438934326\n",
      "Loss at step 99450 : 4.27281379699707\n",
      "Loss at step 99500 : 3.020354986190796\n",
      "Loss at step 99550 : 2.9734630584716797\n",
      "Loss at step 99600 : 2.3156676292419434\n",
      "Loss at step 99650 : 3.1376051902770996\n",
      "Loss at step 99700 : 2.8346972465515137\n",
      "Loss at step 99750 : 3.38260817527771\n",
      "Loss at step 99800 : 2.642033100128174\n",
      "Loss at step 99850 : 2.8921139240264893\n",
      "Loss at step 99900 : 2.477290153503418\n",
      "Loss at step 99950 : 3.09805965423584\n",
      "Loss at step 100000 : 3.348081588745117\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 100050 : 1.6522871255874634\n",
      "Loss at step 100100 : 2.738966941833496\n",
      "Loss at step 100150 : 3.1122865676879883\n",
      "Loss at step 100200 : 4.230269908905029\n",
      "Loss at step 100250 : 3.231828451156616\n",
      "Loss at step 100300 : 3.3011960983276367\n",
      "Loss at step 100350 : 2.3304648399353027\n",
      "Loss at step 100400 : 2.939258098602295\n",
      "Loss at step 100450 : 3.244694709777832\n",
      "Loss at step 100500 : 3.7996790409088135\n",
      "Loss at step 100550 : 2.563052177429199\n",
      "Loss at step 100600 : 2.9868273735046387\n",
      "Loss at step 100650 : 2.6509292125701904\n",
      "Loss at step 100700 : 2.9643545150756836\n",
      "Loss at step 100750 : 3.372011184692383\n",
      "Loss at step 100800 : 3.4737513065338135\n",
      "Loss at step 100850 : 3.617366313934326\n",
      "Loss at step 100900 : 1.9972600936889648\n",
      "Loss at step 100950 : 3.2779622077941895\n",
      "Loss at step 101000 : 2.8546037673950195\n",
      "Loss at step 101050 : 2.750241279602051\n",
      "Loss at step 101100 : 2.646684169769287\n",
      "Loss at step 101150 : 2.6123242378234863\n",
      "Loss at step 101200 : 2.2386105060577393\n",
      "Loss at step 101250 : 2.348386764526367\n",
      "Loss at step 101300 : 2.6433537006378174\n",
      "Loss at step 101350 : 3.3937854766845703\n",
      "Loss at step 101400 : 2.8642382621765137\n",
      "Loss at step 101450 : 2.9851293563842773\n",
      "Loss at step 101500 : 2.2478420734405518\n",
      "Loss at step 101550 : 2.719784736633301\n",
      "Loss at step 101600 : 1.727616786956787\n",
      "Loss at step 101650 : 2.6437149047851562\n",
      "Loss at step 101700 : 3.1861188411712646\n",
      "Loss at step 101750 : 2.6016345024108887\n",
      "Loss at step 101800 : 3.1829113960266113\n",
      "Loss at step 101850 : 3.4687352180480957\n",
      "Loss at step 101900 : 2.926403522491455\n",
      "Loss at step 101950 : 2.8309531211853027\n",
      "Loss at step 102000 : 3.5185980796813965\n",
      "Loss at step 102050 : 1.7773566246032715\n",
      "Loss at step 102100 : 2.17523193359375\n",
      "Loss at step 102150 : 2.8973288536071777\n",
      "Loss at step 102200 : 2.042142868041992\n",
      "Loss at step 102250 : 2.5055956840515137\n",
      "Loss at step 102300 : 2.9150102138519287\n",
      "Loss at step 102350 : 2.7421984672546387\n",
      "Loss at step 102400 : 2.432293653488159\n",
      "Loss at step 102450 : 3.259685516357422\n",
      "Loss at step 102500 : 5.1367950439453125\n",
      "Loss at step 102550 : 3.4591360092163086\n",
      "Loss at step 102600 : 2.983840227127075\n",
      "Loss at step 102650 : 4.087587356567383\n",
      "Loss at step 102700 : 3.4698591232299805\n",
      "Loss at step 102750 : 3.0328750610351562\n",
      "Loss at step 102800 : 2.6044414043426514\n",
      "Loss at step 102850 : 3.1616430282592773\n",
      "Loss at step 102900 : 3.327340841293335\n",
      "Loss at step 102950 : 2.4745259284973145\n",
      "Loss at step 103000 : 2.1718339920043945\n",
      "Loss at step 103050 : 2.74678373336792\n",
      "Loss at step 103100 : 3.217604398727417\n",
      "Loss at step 103150 : 3.4299135208129883\n",
      "Loss at step 103200 : 3.395414352416992\n",
      "Loss at step 103250 : 3.020543336868286\n",
      "Loss at step 103300 : 3.844383716583252\n",
      "Loss at step 103350 : 3.0330026149749756\n",
      "Loss at step 103400 : 3.782710075378418\n",
      "Loss at step 103450 : 3.229065418243408\n",
      "Loss at step 103500 : 2.232166290283203\n",
      "Loss at step 103550 : 1.5242831707000732\n",
      "Loss at step 103600 : 3.5315771102905273\n",
      "Loss at step 103650 : 3.1033387184143066\n",
      "Loss at step 103700 : 3.7988038063049316\n",
      "Loss at step 103750 : 2.9382543563842773\n",
      "Loss at step 103800 : 3.7072412967681885\n",
      "Loss at step 103850 : 3.2460789680480957\n",
      "Loss at step 103900 : 2.7599849700927734\n",
      "Loss at step 103950 : 3.4302430152893066\n",
      "Loss at step 104000 : 3.3737659454345703\n",
      "Loss at step 104050 : 2.6843972206115723\n",
      "Loss at step 104100 : 2.3845860958099365\n",
      "Loss at step 104150 : 4.011353492736816\n",
      "Loss at step 104200 : 3.4254019260406494\n",
      "Loss at step 104250 : 2.453674077987671\n",
      "Loss at step 104300 : 3.5964155197143555\n",
      "Loss at step 104350 : 2.8190793991088867\n",
      "Loss at step 104400 : 3.0035738945007324\n",
      "Loss at step 104450 : 2.592494010925293\n",
      "Loss at step 104500 : 2.2296128273010254\n",
      "Loss at step 104550 : 2.517728328704834\n",
      "Loss at step 104600 : 2.8384366035461426\n",
      "Loss at step 104650 : 2.5640530586242676\n",
      "Loss at step 104700 : 2.9677047729492188\n",
      "Loss at step 104750 : 2.1877217292785645\n",
      "Loss at step 104800 : 2.970534086227417\n",
      "Loss at step 104850 : 2.873399257659912\n",
      "Loss at step 104900 : 3.5064051151275635\n",
      "Loss at step 104950 : 3.0956404209136963\n",
      "Loss at step 105000 : 4.167787551879883\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 105050 : 2.734377861022949\n",
      "Loss at step 105100 : 2.207775115966797\n",
      "Loss at step 105150 : 2.940833568572998\n",
      "Loss at step 105200 : 3.7385547161102295\n",
      "Loss at step 105250 : 1.955033779144287\n",
      "Loss at step 105300 : 3.1847801208496094\n",
      "Loss at step 105350 : 3.49318790435791\n",
      "Loss at step 105400 : 2.877545118331909\n",
      "Loss at step 105450 : 3.4372782707214355\n",
      "Loss at step 105500 : 3.788158893585205\n",
      "Loss at step 105550 : 2.408348798751831\n",
      "Loss at step 105600 : 3.411825180053711\n",
      "Loss at step 105650 : 3.0820062160491943\n",
      "Loss at step 105700 : 2.866783857345581\n",
      "Loss at step 105750 : 3.4197845458984375\n",
      "Loss at step 105800 : 3.428797721862793\n",
      "Loss at step 105850 : 2.8810789585113525\n",
      "Loss at step 105900 : 3.615532875061035\n",
      "Loss at step 105950 : 3.541433811187744\n",
      "Loss at step 106000 : 2.5033533573150635\n",
      "Loss at step 106050 : 2.477858066558838\n",
      "Loss at step 106100 : 2.8573131561279297\n",
      "Loss at step 106150 : 1.7678619623184204\n",
      "Loss at step 106200 : 3.165503978729248\n",
      "Loss at step 106250 : 3.153075695037842\n",
      "Loss at step 106300 : 2.313448429107666\n",
      "Loss at step 106350 : 3.0331366062164307\n",
      "Loss at step 106400 : 2.8608927726745605\n",
      "Loss at step 106450 : 3.6367015838623047\n",
      "Loss at step 106500 : 2.7167227268218994\n",
      "Loss at step 106550 : 3.0254709720611572\n",
      "Loss at step 106600 : 2.8155300617218018\n",
      "Loss at step 106650 : 3.810786247253418\n",
      "Loss at step 106700 : 3.1599016189575195\n",
      "Loss at step 106750 : 2.78938889503479\n",
      "Loss at step 106800 : 3.7898333072662354\n",
      "Loss at step 106850 : 2.0271506309509277\n",
      "Loss at step 106900 : 3.1413097381591797\n",
      "Loss at step 106950 : 2.9277639389038086\n",
      "Loss at step 107000 : 2.7750566005706787\n",
      "Loss at step 107050 : 2.500408411026001\n",
      "Loss at step 107100 : 3.30643367767334\n",
      "Loss at step 107150 : 3.4717557430267334\n",
      "Loss at step 107200 : 3.0007212162017822\n",
      "Loss at step 107250 : 3.246875286102295\n",
      "Loss at step 107300 : 2.6092100143432617\n",
      "Loss at step 107350 : 2.782925605773926\n",
      "Loss at step 107400 : 2.3951921463012695\n",
      "Loss at step 107450 : 3.071706771850586\n",
      "Loss at step 107500 : 3.4306137561798096\n",
      "Loss at step 107550 : 3.5410521030426025\n",
      "Loss at step 107600 : 3.933419704437256\n",
      "Loss at step 107650 : 2.607349395751953\n",
      "Loss at step 107700 : 2.17364501953125\n",
      "Loss at step 107750 : 4.268460750579834\n",
      "Loss at step 107800 : 3.39815092086792\n",
      "Loss at step 107850 : 3.438979387283325\n",
      "Loss at step 107900 : 2.917178153991699\n",
      "Loss at step 107950 : 2.4871091842651367\n",
      "Loss at step 108000 : 2.204660415649414\n",
      "Loss at step 108050 : 2.962559461593628\n",
      "Loss at step 108100 : 3.327711343765259\n",
      "Loss at step 108150 : 3.0324745178222656\n",
      "Loss at step 108200 : 2.8701462745666504\n",
      "Loss at step 108250 : 2.5797324180603027\n",
      "Loss at step 108300 : 3.3033947944641113\n",
      "Loss at step 108350 : 2.1741862297058105\n",
      "Loss at step 108400 : 3.809793472290039\n",
      "Loss at step 108450 : 2.9236698150634766\n",
      "Loss at step 108500 : 2.6295089721679688\n",
      "Loss at step 108550 : 3.0092928409576416\n",
      "Loss at step 108600 : 3.0882015228271484\n",
      "Loss at step 108650 : 3.0625829696655273\n",
      "Loss at step 108700 : 2.8142738342285156\n",
      "Loss at step 108750 : 3.477212905883789\n",
      "Loss at step 108800 : 1.9283149242401123\n",
      "Loss at step 108850 : 2.847579002380371\n",
      "Loss at step 108900 : 3.4863243103027344\n",
      "Loss at step 108950 : 3.339547634124756\n",
      "Loss at step 109000 : 2.7330870628356934\n",
      "Loss at step 109050 : 2.968137741088867\n",
      "Loss at step 109100 : 2.4182794094085693\n",
      "Loss at step 109150 : 3.786196231842041\n",
      "Loss at step 109200 : 2.813347339630127\n",
      "Loss at step 109250 : 2.903693914413452\n",
      "Loss at step 109300 : 2.3476688861846924\n",
      "Loss at step 109350 : 3.0860540866851807\n",
      "Loss at step 109400 : 2.6719985008239746\n",
      "Loss at step 109450 : 3.673269271850586\n",
      "Loss at step 109500 : 2.866044044494629\n",
      "Loss at step 109550 : 2.82654070854187\n",
      "Loss at step 109600 : 2.925222873687744\n",
      "Loss at step 109650 : 3.9343905448913574\n",
      "Loss at step 109700 : 2.46917724609375\n",
      "Loss at step 109750 : 3.907431125640869\n",
      "Loss at step 109800 : 2.8293275833129883\n",
      "Loss at step 109850 : 3.0055673122406006\n",
      "Loss at step 109900 : 3.304891586303711\n",
      "Loss at step 109950 : 3.7131617069244385\n",
      "Loss at step 110000 : 3.1143434047698975\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 110050 : 2.291790008544922\n",
      "Loss at step 110100 : 2.6897239685058594\n",
      "Loss at step 110150 : 2.50266170501709\n",
      "Loss at step 110200 : 2.272509813308716\n",
      "Loss at step 110250 : 2.4925942420959473\n",
      "Loss at step 110300 : 2.5718822479248047\n",
      "Loss at step 110350 : 2.2498021125793457\n",
      "Loss at step 110400 : 2.9981071949005127\n",
      "Loss at step 110450 : 2.4672513008117676\n",
      "Loss at step 110500 : 3.1397600173950195\n",
      "Loss at step 110550 : 2.93794584274292\n",
      "Loss at step 110600 : 2.836050033569336\n",
      "Loss at step 110650 : 2.4221067428588867\n",
      "Loss at step 110700 : 2.8794493675231934\n",
      "Loss at step 110750 : 3.0785088539123535\n",
      "Loss at step 110800 : 2.4781625270843506\n",
      "Loss at step 110850 : 2.679759979248047\n",
      "Loss at step 110900 : 3.257993698120117\n",
      "Loss at step 110950 : 2.3986570835113525\n",
      "Loss at step 111000 : 3.921971082687378\n",
      "Loss at step 111050 : 3.225069522857666\n",
      "Loss at step 111100 : 2.726933002471924\n",
      "Loss at step 111150 : 3.714261054992676\n",
      "Loss at step 111200 : 2.228022575378418\n",
      "Loss at step 111250 : 2.969360589981079\n",
      "Loss at step 111300 : 2.739759922027588\n",
      "Loss at step 111350 : 3.1869266033172607\n",
      "Loss at step 111400 : 3.488325357437134\n",
      "Loss at step 111450 : 3.5996346473693848\n",
      "Loss at step 111500 : 2.877068042755127\n",
      "Loss at step 111550 : 3.2952792644500732\n",
      "Loss at step 111600 : 3.879641532897949\n",
      "Loss at step 111650 : 2.297584056854248\n",
      "Loss at step 111700 : 2.3193578720092773\n",
      "Loss at step 111750 : 3.1736154556274414\n",
      "Loss at step 111800 : 2.5088796615600586\n",
      "Loss at step 111850 : 2.1638100147247314\n",
      "Loss at step 111900 : 2.160170555114746\n",
      "Loss at step 111950 : 3.6506614685058594\n",
      "Loss at step 112000 : 2.5833911895751953\n",
      "Loss at step 112050 : 2.318422317504883\n",
      "Loss at step 112100 : 2.890704870223999\n",
      "Loss at step 112150 : 3.0937118530273438\n",
      "Loss at step 112200 : 2.7009377479553223\n",
      "Loss at step 112250 : 3.27097225189209\n",
      "Loss at step 112300 : 3.247088670730591\n",
      "Loss at step 112350 : 3.5453920364379883\n",
      "Loss at step 112400 : 2.8084285259246826\n",
      "Loss at step 112450 : 2.6237173080444336\n",
      "Loss at step 112500 : 2.3191981315612793\n",
      "Loss at step 112550 : 4.042512893676758\n",
      "Loss at step 112600 : 2.988290786743164\n",
      "Loss at step 112650 : 3.0097594261169434\n",
      "Loss at step 112700 : 3.5422048568725586\n",
      "Loss at step 112750 : 3.782750368118286\n",
      "Loss at step 112800 : 2.2503175735473633\n",
      "Loss at step 112850 : 2.6329236030578613\n",
      "Loss at step 112900 : 2.2043323516845703\n",
      "Loss at step 112950 : 3.2556748390197754\n",
      "Loss at step 113000 : 2.887598752975464\n",
      "Loss at step 113050 : 2.9563703536987305\n",
      "Loss at step 113100 : 2.4335250854492188\n",
      "Loss at step 113150 : 3.178964376449585\n",
      "Loss at step 113200 : 2.8418540954589844\n",
      "Loss at step 113250 : 3.036226749420166\n",
      "Loss at step 113300 : 3.5170669555664062\n",
      "Loss at step 113350 : 2.7584588527679443\n",
      "Loss at step 113400 : 3.999164581298828\n",
      "Loss at step 113450 : 3.2836666107177734\n",
      "Loss at step 113500 : 2.2427268028259277\n",
      "Loss at step 113550 : 2.2583279609680176\n",
      "Loss at step 113600 : 5.431255340576172\n",
      "Loss at step 113650 : 3.1834793090820312\n",
      "Loss at step 113700 : 3.1580586433410645\n",
      "Loss at step 113750 : 2.708026647567749\n",
      "Loss at step 113800 : 3.0189809799194336\n",
      "Loss at step 113850 : 4.203367233276367\n",
      "Loss at step 113900 : 3.4498090744018555\n",
      "Loss at step 113950 : 3.607898712158203\n",
      "Loss at step 114000 : 3.509004592895508\n",
      "Loss at step 114050 : 3.5338709354400635\n",
      "Loss at step 114100 : 2.81119441986084\n",
      "Loss at step 114150 : 3.044837474822998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 114200 : 3.0091004371643066\n",
      "Loss at step 114250 : 3.8443989753723145\n",
      "Loss at step 114300 : 3.3534374237060547\n",
      "Loss at step 114350 : 2.8528523445129395\n",
      "Loss at step 114400 : 2.944143295288086\n",
      "Loss at step 114450 : 2.819162368774414\n",
      "Loss at step 114500 : 3.018343925476074\n",
      "Loss at step 114550 : 2.7543082237243652\n",
      "Loss at step 114600 : 4.730273246765137\n",
      "Loss at step 114650 : 2.6397922039031982\n",
      "Loss at step 114700 : 2.977363109588623\n",
      "Loss at step 114750 : 2.997973918914795\n",
      "Loss at step 114800 : 2.6457738876342773\n",
      "Loss at step 114850 : 2.8341991901397705\n",
      "Loss at step 114900 : 2.941622257232666\n",
      "Loss at step 114950 : 2.4258408546447754\n",
      "Loss at step 115000 : 1.9981346130371094\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 115050 : 3.4579572677612305\n",
      "Loss at step 115100 : 3.0090231895446777\n",
      "Loss at step 115150 : 2.501408576965332\n",
      "Loss at step 115200 : 3.0385711193084717\n",
      "Loss at step 115250 : 3.267341136932373\n",
      "Loss at step 115300 : 3.140598773956299\n",
      "Loss at step 115350 : 3.0117392539978027\n",
      "Loss at step 115400 : 3.0426230430603027\n",
      "Loss at step 115450 : 3.6142542362213135\n",
      "Loss at step 115500 : 3.5510430335998535\n",
      "Loss at step 115550 : 3.0123937129974365\n",
      "Loss at step 115600 : 2.711221218109131\n",
      "Loss at step 115650 : 1.5480537414550781\n",
      "Loss at step 115700 : 2.466737747192383\n",
      "Loss at step 115750 : 2.926593780517578\n",
      "Loss at step 115800 : 2.9230690002441406\n",
      "Loss at step 115850 : 3.111098527908325\n",
      "Loss at step 115900 : 3.3372435569763184\n",
      "Loss at step 115950 : 2.290409564971924\n",
      "Loss at step 116000 : 2.995443820953369\n",
      "Loss at step 116050 : 2.2708072662353516\n",
      "Loss at step 116100 : 3.0780649185180664\n",
      "Loss at step 116150 : 2.7437243461608887\n",
      "Loss at step 116200 : 3.0386886596679688\n",
      "Loss at step 116250 : 2.345014810562134\n",
      "Loss at step 116300 : 3.514408826828003\n",
      "Loss at step 116350 : 2.7940685749053955\n",
      "Loss at step 116400 : 3.811598300933838\n",
      "Loss at step 116450 : 3.5265820026397705\n",
      "Loss at step 116500 : 2.8772387504577637\n",
      "Loss at step 116550 : 3.326557159423828\n",
      "Loss at step 116600 : 2.696234703063965\n",
      "Loss at step 116650 : 3.414473295211792\n",
      "Loss at step 116700 : 3.129019260406494\n",
      "Loss at step 116750 : 2.040408134460449\n",
      "Loss at step 116800 : 2.2315475940704346\n",
      "Loss at step 116850 : 2.7636799812316895\n",
      "Loss at step 116900 : 3.6428651809692383\n",
      "Loss at step 116950 : 4.012214660644531\n",
      "Loss at step 117000 : 2.3283138275146484\n",
      "Loss at step 117050 : 2.4412989616394043\n",
      "Loss at step 117100 : 1.9081063270568848\n",
      "Loss at step 117150 : 3.200922727584839\n",
      "Loss at step 117200 : 2.0343523025512695\n",
      "Loss at step 117250 : 2.800260543823242\n",
      "Loss at step 117300 : 1.530971884727478\n",
      "Loss at step 117350 : 2.5970211029052734\n",
      "Loss at step 117400 : 2.665970802307129\n",
      "Loss at step 117450 : 4.034589767456055\n",
      "Loss at step 117500 : 3.452592372894287\n",
      "Loss at step 117550 : 2.267174243927002\n",
      "Loss at step 117600 : 3.960592746734619\n",
      "Loss at step 117650 : 4.342470645904541\n",
      "Loss at step 117700 : 3.2336156368255615\n",
      "Loss at step 117750 : 3.3298654556274414\n",
      "Loss at step 117800 : 3.3151021003723145\n",
      "Loss at step 117850 : 3.683384418487549\n",
      "Loss at step 117900 : 3.632579803466797\n",
      "Loss at step 117950 : 2.667328357696533\n",
      "Loss at step 118000 : 2.7629053592681885\n",
      "Loss at step 118050 : 3.050577163696289\n",
      "Loss at step 118100 : 2.5243639945983887\n",
      "Loss at step 118150 : 3.143336772918701\n",
      "Loss at step 118200 : 2.7314531803131104\n",
      "Loss at step 118250 : 2.987234115600586\n",
      "Loss at step 118300 : 3.226632595062256\n",
      "Loss at step 118350 : 3.252640962600708\n",
      "Loss at step 118400 : 3.1648290157318115\n",
      "Loss at step 118450 : 3.2068467140197754\n",
      "Loss at step 118500 : 3.5671286582946777\n",
      "Loss at step 118550 : 2.782158374786377\n",
      "Loss at step 118600 : 2.7398571968078613\n",
      "Loss at step 118650 : 3.482661485671997\n",
      "Loss at step 118700 : 3.257737159729004\n",
      "Loss at step 118750 : 2.989628791809082\n",
      "Loss at step 118800 : 2.8526015281677246\n",
      "Loss at step 118850 : 3.194823980331421\n",
      "Loss at step 118900 : 3.3418526649475098\n",
      "Loss at step 118950 : 2.311476945877075\n",
      "Loss at step 119000 : 3.572686195373535\n",
      "Loss at step 119050 : 4.2417802810668945\n",
      "Loss at step 119100 : 2.856173038482666\n",
      "Loss at step 119150 : 2.549576997756958\n",
      "Loss at step 119200 : 3.0810842514038086\n",
      "Loss at step 119250 : 3.0424933433532715\n",
      "Loss at step 119300 : 3.0985331535339355\n",
      "Loss at step 119350 : 2.888291835784912\n",
      "Loss at step 119400 : 4.026182174682617\n",
      "Loss at step 119450 : 2.3928589820861816\n",
      "Loss at step 119500 : 2.480833053588867\n",
      "Loss at step 119550 : 3.632565498352051\n",
      "Loss at step 119600 : 2.3871679306030273\n",
      "Loss at step 119650 : 2.247389078140259\n",
      "Loss at step 119700 : 2.0200533866882324\n",
      "Loss at step 119750 : 2.9167068004608154\n",
      "Loss at step 119800 : 3.1606030464172363\n",
      "Loss at step 119850 : 2.911419153213501\n",
      "Loss at step 119900 : 2.8296308517456055\n",
      "Loss at step 119950 : 2.9811606407165527\n",
      "Loss at step 120000 : 4.8952178955078125\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 120050 : 3.070380687713623\n",
      "Loss at step 120100 : 3.475529909133911\n",
      "Loss at step 120150 : 2.97446870803833\n",
      "Loss at step 120200 : 3.0632028579711914\n",
      "Loss at step 120250 : 2.9381985664367676\n",
      "Loss at step 120300 : 3.9375998973846436\n",
      "Loss at step 120350 : 3.086716890335083\n",
      "Loss at step 120400 : 3.3391733169555664\n",
      "Loss at step 120450 : 3.1731066703796387\n",
      "Loss at step 120500 : 3.321648597717285\n",
      "Loss at step 120550 : 3.0293564796447754\n",
      "Loss at step 120600 : 2.1249423027038574\n",
      "Loss at step 120650 : 2.9490761756896973\n",
      "Loss at step 120700 : 2.6334729194641113\n",
      "Loss at step 120750 : 3.615008592605591\n",
      "Loss at step 120800 : 2.944890260696411\n",
      "Loss at step 120850 : 2.774282932281494\n",
      "Loss at step 120900 : 4.667106628417969\n",
      "Loss at step 120950 : 4.455219268798828\n",
      "Loss at step 121000 : 2.066770553588867\n",
      "Loss at step 121050 : 3.286473274230957\n",
      "Loss at step 121100 : 3.0478367805480957\n",
      "Loss at step 121150 : 2.9469685554504395\n",
      "Loss at step 121200 : 3.690180540084839\n",
      "Loss at step 121250 : 2.8532049655914307\n",
      "Loss at step 121300 : 2.8256871700286865\n",
      "Loss at step 121350 : 2.9804773330688477\n",
      "Loss at step 121400 : 3.020714282989502\n",
      "Loss at step 121450 : 2.5864553451538086\n",
      "Loss at step 121500 : 3.047212600708008\n",
      "Loss at step 121550 : 3.1436691284179688\n",
      "Loss at step 121600 : 3.4231603145599365\n",
      "Loss at step 121650 : 3.4366867542266846\n",
      "Loss at step 121700 : 2.197354316711426\n",
      "Loss at step 121750 : 2.668436050415039\n",
      "Loss at step 121800 : 3.2606093883514404\n",
      "Loss at step 121850 : 4.192009925842285\n",
      "Loss at step 121900 : 2.9358456134796143\n",
      "Loss at step 121950 : 3.246807098388672\n",
      "Loss at step 122000 : 3.2509474754333496\n",
      "Loss at step 122050 : 3.4126524925231934\n",
      "Loss at step 122100 : 2.886294364929199\n",
      "Loss at step 122150 : 2.9848124980926514\n",
      "Loss at step 122200 : 2.890502691268921\n",
      "Loss at step 122250 : 3.1816470623016357\n",
      "Loss at step 122300 : 3.282712936401367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 122350 : 2.2602672576904297\n",
      "Loss at step 122400 : 2.9810047149658203\n",
      "Loss at step 122450 : 2.390986442565918\n",
      "Loss at step 122500 : 3.2548911571502686\n",
      "Loss at step 122550 : 2.5990421772003174\n",
      "Loss at step 122600 : 2.475238561630249\n",
      "Loss at step 122650 : 2.515568733215332\n",
      "Loss at step 122700 : 3.5528323650360107\n",
      "Loss at step 122750 : 4.291086196899414\n",
      "Loss at step 122800 : 3.6562719345092773\n",
      "Loss at step 122850 : 2.362699031829834\n",
      "Loss at step 122900 : 3.496732234954834\n",
      "Loss at step 122950 : 3.619757652282715\n",
      "Loss at step 123000 : 3.079115390777588\n",
      "Loss at step 123050 : 3.5403027534484863\n",
      "Loss at step 123100 : 2.737579822540283\n",
      "Loss at step 123150 : 2.2977347373962402\n",
      "Loss at step 123200 : 3.1714372634887695\n",
      "Loss at step 123250 : 3.212261915206909\n",
      "Loss at step 123300 : 4.037083625793457\n",
      "Loss at step 123350 : 2.4929990768432617\n",
      "Loss at step 123400 : 3.714766025543213\n",
      "Loss at step 123450 : 1.9148679971694946\n",
      "Loss at step 123500 : 3.4526538848876953\n",
      "Loss at step 123550 : 2.947417974472046\n",
      "Loss at step 123600 : 2.845449924468994\n",
      "Loss at step 123650 : 3.442340612411499\n",
      "Loss at step 123700 : 3.763451099395752\n",
      "Loss at step 123750 : 2.149418354034424\n",
      "Loss at step 123800 : 2.1247713565826416\n",
      "Loss at step 123850 : 2.2290875911712646\n",
      "Loss at step 123900 : 2.4539647102355957\n",
      "Loss at step 123950 : 3.0141453742980957\n",
      "Loss at step 124000 : 3.4713306427001953\n",
      "Loss at step 124050 : 2.259408950805664\n",
      "Loss at step 124100 : 3.089005470275879\n",
      "Loss at step 124150 : 2.192639112472534\n",
      "Loss at step 124200 : 3.048374652862549\n",
      "Loss at step 124250 : 2.2364864349365234\n",
      "Loss at step 124300 : 2.2940871715545654\n",
      "Loss at step 124350 : 3.242861032485962\n",
      "Loss at step 124400 : 4.03582763671875\n",
      "Loss at step 124450 : 3.4680466651916504\n",
      "Loss at step 124500 : 2.0879054069519043\n",
      "Loss at step 124550 : 2.3537447452545166\n",
      "Loss at step 124600 : 2.9137916564941406\n",
      "Loss at step 124650 : 3.3802742958068848\n",
      "Loss at step 124700 : 3.021507978439331\n",
      "Loss at step 124750 : 3.7316689491271973\n",
      "Loss at step 124800 : 2.3138723373413086\n",
      "Loss at step 124850 : 3.2068259716033936\n",
      "Loss at step 124900 : 2.5586323738098145\n",
      "Loss at step 124950 : 2.6858935356140137\n",
      "Loss at step 125000 : 2.0419249534606934\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 125050 : 2.179884672164917\n",
      "Loss at step 125100 : 2.842074394226074\n",
      "Loss at step 125150 : 2.1089353561401367\n",
      "Loss at step 125200 : 2.5577025413513184\n",
      "Loss at step 125250 : 3.388803005218506\n",
      "Loss at step 125300 : 3.077270269393921\n",
      "Loss at step 125350 : 2.7590203285217285\n",
      "Loss at step 125400 : 2.427136182785034\n",
      "Loss at step 125450 : 2.514292001724243\n",
      "Loss at step 125500 : 3.4182262420654297\n",
      "Loss at step 125550 : 3.3606157302856445\n",
      "Loss at step 125600 : 2.0578227043151855\n",
      "Loss at step 125650 : 3.1267387866973877\n",
      "Loss at step 125700 : 2.426276922225952\n",
      "Loss at step 125750 : 3.107156753540039\n",
      "Loss at step 125800 : 2.8847174644470215\n",
      "Loss at step 125850 : 3.3792929649353027\n",
      "Loss at step 125900 : 2.6439175605773926\n",
      "Loss at step 125950 : 3.7168002128601074\n",
      "Loss at step 126000 : 2.2504308223724365\n",
      "Loss at step 126050 : 2.6590816974639893\n",
      "Loss at step 126100 : 2.705962657928467\n",
      "Loss at step 126150 : 2.0998263359069824\n",
      "Loss at step 126200 : 3.4073472023010254\n",
      "Loss at step 126250 : 2.426332712173462\n",
      "Loss at step 126300 : 3.541207790374756\n",
      "Loss at step 126350 : 2.2932329177856445\n",
      "Loss at step 126400 : 2.9446916580200195\n",
      "Loss at step 126450 : 2.790743827819824\n",
      "Loss at step 126500 : 3.6457173824310303\n",
      "Loss at step 126550 : 2.6322922706604004\n",
      "Loss at step 126600 : 2.653630256652832\n",
      "Loss at step 126650 : 4.090233325958252\n",
      "Loss at step 126700 : 2.581516742706299\n",
      "Loss at step 126750 : 3.0837655067443848\n",
      "Loss at step 126800 : 3.0568346977233887\n",
      "Loss at step 126850 : 3.006439208984375\n",
      "Loss at step 126900 : 2.871969699859619\n",
      "Loss at step 126950 : 3.3637290000915527\n",
      "Loss at step 127000 : 1.8986759185791016\n",
      "Loss at step 127050 : 2.513460159301758\n",
      "Loss at step 127100 : 2.6183204650878906\n",
      "Loss at step 127150 : 2.3275444507598877\n",
      "Loss at step 127200 : 1.960488200187683\n",
      "Loss at step 127250 : 3.012892723083496\n",
      "Loss at step 127300 : 2.718761920928955\n",
      "Loss at step 127350 : 3.4834766387939453\n",
      "Loss at step 127400 : 3.7075417041778564\n",
      "Loss at step 127450 : 3.0185179710388184\n",
      "Loss at step 127500 : 3.620490550994873\n",
      "Loss at step 127550 : 3.140385627746582\n",
      "Loss at step 127600 : 2.408600330352783\n",
      "Loss at step 127650 : 2.8304848670959473\n",
      "Loss at step 127700 : 3.661741018295288\n",
      "Loss at step 127750 : 3.113818883895874\n",
      "Loss at step 127800 : 2.8720877170562744\n",
      "Loss at step 127850 : 2.659274101257324\n",
      "Loss at step 127900 : 2.736243963241577\n",
      "Loss at step 127950 : 2.870562791824341\n",
      "Loss at step 128000 : 2.2829341888427734\n",
      "Loss at step 128050 : 3.7445173263549805\n",
      "Loss at step 128100 : 3.225903272628784\n",
      "Loss at step 128150 : 3.5319790840148926\n",
      "Loss at step 128200 : 2.783346652984619\n",
      "Loss at step 128250 : 3.6205525398254395\n",
      "Loss at step 128300 : 3.2604146003723145\n",
      "Loss at step 128350 : 2.936856985092163\n",
      "Loss at step 128400 : 3.250425338745117\n",
      "Loss at step 128450 : 2.532033681869507\n",
      "Loss at step 128500 : 2.782139778137207\n",
      "Loss at step 128550 : 2.3719642162323\n",
      "Loss at step 128600 : 2.7102599143981934\n",
      "Loss at step 128650 : 2.778956413269043\n",
      "Loss at step 128700 : 3.6649270057678223\n",
      "Loss at step 128750 : 2.979400157928467\n",
      "Loss at step 128800 : 2.524341583251953\n",
      "Loss at step 128850 : 2.3924872875213623\n",
      "Loss at step 128900 : 3.0309605598449707\n",
      "Loss at step 128950 : 3.1153969764709473\n",
      "Loss at step 129000 : 3.118267059326172\n",
      "Loss at step 129050 : 2.665654182434082\n",
      "Loss at step 129100 : 2.357738971710205\n",
      "Loss at step 129150 : 2.6779537200927734\n",
      "Loss at step 129200 : 2.8531975746154785\n",
      "Loss at step 129250 : 2.567319869995117\n",
      "Loss at step 129300 : 3.4451990127563477\n",
      "Loss at step 129350 : 2.0972490310668945\n",
      "Loss at step 129400 : 3.1398630142211914\n",
      "Loss at step 129450 : 2.6039977073669434\n",
      "Loss at step 129500 : 4.122199058532715\n",
      "Loss at step 129550 : 3.8811473846435547\n",
      "Loss at step 129600 : 3.050041437149048\n",
      "Loss at step 129650 : 3.3799045085906982\n",
      "Loss at step 129700 : 4.402524471282959\n",
      "Loss at step 129750 : 2.7107670307159424\n",
      "Loss at step 129800 : 1.9554342031478882\n",
      "Loss at step 129850 : 3.2143771648406982\n",
      "Loss at step 129900 : 3.2021329402923584\n",
      "Loss at step 129950 : 2.8574016094207764\n",
      "Loss at step 130000 : 3.625445604324341\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 130050 : 2.541353225708008\n",
      "Loss at step 130100 : 2.7933740615844727\n",
      "Loss at step 130150 : 3.5706396102905273\n",
      "Loss at step 130200 : 2.828402519226074\n",
      "Loss at step 130250 : 2.266608238220215\n",
      "Loss at step 130300 : 1.5443230867385864\n",
      "Loss at step 130350 : 3.0192782878875732\n",
      "Loss at step 130400 : 1.680603265762329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 130450 : 2.737761974334717\n",
      "Loss at step 130500 : 3.0568675994873047\n",
      "Loss at step 130550 : 3.200409173965454\n",
      "Loss at step 130600 : 2.981660842895508\n",
      "Loss at step 130650 : 2.884124279022217\n",
      "Loss at step 130700 : 2.147979259490967\n",
      "Loss at step 130750 : 2.6112637519836426\n",
      "Loss at step 130800 : 3.5748767852783203\n",
      "Loss at step 130850 : 2.861572265625\n",
      "Loss at step 130900 : 3.669978141784668\n",
      "Loss at step 130950 : 2.232994794845581\n",
      "Loss at step 131000 : 3.6183042526245117\n",
      "Loss at step 131050 : 2.10115909576416\n",
      "Loss at step 131100 : 2.2399649620056152\n",
      "Loss at step 131150 : 2.411911725997925\n",
      "Loss at step 131200 : 2.3557004928588867\n",
      "Loss at step 131250 : 2.5082147121429443\n",
      "Loss at step 131300 : 3.2311110496520996\n",
      "Loss at step 131350 : 2.525890350341797\n",
      "Loss at step 131400 : 3.2896289825439453\n",
      "Loss at step 131450 : 3.827484607696533\n",
      "Loss at step 131500 : 1.5628089904785156\n",
      "Loss at step 131550 : 2.4968795776367188\n",
      "Loss at step 131600 : 2.5759406089782715\n",
      "Loss at step 131650 : 2.726235866546631\n",
      "Loss at step 131700 : 2.4359335899353027\n",
      "Loss at step 131750 : 3.620713710784912\n",
      "Loss at step 131800 : 3.1505002975463867\n",
      "Loss at step 131850 : 2.3934543132781982\n",
      "Loss at step 131900 : 2.2035765647888184\n",
      "Loss at step 131950 : 2.4601285457611084\n",
      "Loss at step 132000 : 3.9617021083831787\n",
      "Loss at step 132050 : 2.2773239612579346\n",
      "Loss at step 132100 : 3.1281211376190186\n",
      "Loss at step 132150 : 3.680185556411743\n",
      "Loss at step 132200 : 3.265387773513794\n",
      "Loss at step 132250 : 2.268871545791626\n",
      "Loss at step 132300 : 3.9517877101898193\n",
      "Loss at step 132350 : 2.6195778846740723\n",
      "Loss at step 132400 : 4.30573034286499\n",
      "Loss at step 132450 : 2.239427089691162\n",
      "Loss at step 132500 : 2.93794584274292\n",
      "Loss at step 132550 : 3.4881434440612793\n",
      "Loss at step 132600 : 3.4875869750976562\n",
      "Loss at step 132650 : 2.5823302268981934\n",
      "Loss at step 132700 : 3.0359487533569336\n",
      "Loss at step 132750 : 3.1084837913513184\n",
      "Loss at step 132800 : 2.6468148231506348\n",
      "Loss at step 132850 : 2.502711296081543\n",
      "Loss at step 132900 : 2.199159622192383\n",
      "Loss at step 132950 : 2.7011208534240723\n",
      "Loss at step 133000 : 3.811373233795166\n",
      "Loss at step 133050 : 3.750397205352783\n",
      "Loss at step 133100 : 2.634192705154419\n",
      "Loss at step 133150 : 2.7191455364227295\n",
      "Loss at step 133200 : 2.6457555294036865\n",
      "Loss at step 133250 : 2.9003710746765137\n",
      "Loss at step 133300 : 3.0045971870422363\n",
      "Loss at step 133350 : 3.368469715118408\n",
      "Loss at step 133400 : 2.4129130840301514\n",
      "Loss at step 133450 : 3.0843801498413086\n",
      "Loss at step 133500 : 3.9108176231384277\n",
      "Loss at step 133550 : 2.9271769523620605\n",
      "Loss at step 133600 : 3.1988179683685303\n",
      "Loss at step 133650 : 3.1378672122955322\n",
      "Loss at step 133700 : 2.7451744079589844\n",
      "Loss at step 133750 : 3.441067695617676\n",
      "Loss at step 133800 : 3.4374947547912598\n",
      "Loss at step 133850 : 2.314491033554077\n",
      "Loss at step 133900 : 4.599210739135742\n",
      "Loss at step 133950 : 2.454202175140381\n",
      "Loss at step 134000 : 3.1254873275756836\n",
      "Loss at step 134050 : 3.702340602874756\n",
      "Loss at step 134100 : 2.9238388538360596\n",
      "Loss at step 134150 : 3.098825216293335\n",
      "Loss at step 134200 : 2.756523609161377\n",
      "Loss at step 134250 : 2.985914707183838\n",
      "Loss at step 134300 : 2.199312210083008\n",
      "Loss at step 134350 : 3.6012685298919678\n",
      "Loss at step 134400 : 3.754206418991089\n",
      "Loss at step 134450 : 2.7732765674591064\n",
      "Loss at step 134500 : 3.716154098510742\n",
      "Loss at step 134550 : 3.4846208095550537\n",
      "Loss at step 134600 : 2.572598457336426\n",
      "Loss at step 134650 : 3.1834938526153564\n",
      "Loss at step 134700 : 2.0657448768615723\n",
      "Loss at step 134750 : 2.449841022491455\n",
      "Loss at step 134800 : 2.698495864868164\n",
      "Loss at step 134850 : 2.837185859680176\n",
      "Loss at step 134900 : 2.015341281890869\n",
      "Loss at step 134950 : 3.129892349243164\n",
      "Loss at step 135000 : 3.750849723815918\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 135050 : 3.975144863128662\n",
      "Loss at step 135100 : 2.8147060871124268\n",
      "Loss at step 135150 : 2.7057619094848633\n",
      "Loss at step 135200 : 3.2934601306915283\n",
      "Loss at step 135250 : 2.716078519821167\n",
      "Loss at step 135300 : 2.788709878921509\n",
      "Loss at step 135350 : 3.4574508666992188\n",
      "Loss at step 135400 : 2.55604887008667\n",
      "Loss at step 135450 : 3.0978612899780273\n",
      "Loss at step 135500 : 2.758816719055176\n",
      "Loss at step 135550 : 3.124417781829834\n",
      "Loss at step 135600 : 3.045976161956787\n",
      "Loss at step 135650 : 2.9517135620117188\n",
      "Loss at step 135700 : 3.525210380554199\n",
      "Loss at step 135750 : 2.9508941173553467\n",
      "Loss at step 135800 : 2.811173439025879\n",
      "Loss at step 135850 : 3.027585983276367\n",
      "Loss at step 135900 : 2.79911470413208\n",
      "Loss at step 135950 : 1.8945927619934082\n",
      "Loss at step 136000 : 3.6096010208129883\n",
      "Loss at step 136050 : 2.65924334526062\n",
      "Loss at step 136100 : 2.1749556064605713\n",
      "Loss at step 136150 : 3.585888385772705\n",
      "Loss at step 136200 : 3.046359062194824\n",
      "Loss at step 136250 : 2.6584534645080566\n",
      "Loss at step 136300 : 2.274569511413574\n",
      "Loss at step 136350 : 2.981895685195923\n",
      "Loss at step 136400 : 2.97039794921875\n",
      "Loss at step 136450 : 2.4083030223846436\n",
      "Loss at step 136500 : 2.4928178787231445\n",
      "Loss at step 136550 : 2.821136951446533\n",
      "Loss at step 136600 : 3.0633344650268555\n",
      "Loss at step 136650 : 4.080921173095703\n",
      "Loss at step 136700 : 2.9509902000427246\n",
      "Loss at step 136750 : 3.5285868644714355\n",
      "Loss at step 136800 : 2.6218934059143066\n",
      "Loss at step 136850 : 3.3954501152038574\n",
      "Loss at step 136900 : 3.712491035461426\n",
      "Loss at step 136950 : 2.874180316925049\n",
      "Loss at step 137000 : 3.2909226417541504\n",
      "Loss at step 137050 : 3.140132188796997\n",
      "Loss at step 137100 : 2.8233537673950195\n",
      "Loss at step 137150 : 2.7383246421813965\n",
      "Loss at step 137200 : 2.959923505783081\n",
      "Loss at step 137250 : 2.4583001136779785\n",
      "Loss at step 137300 : 2.032747745513916\n",
      "Loss at step 137350 : 3.3738787174224854\n",
      "Loss at step 137400 : 4.159585952758789\n",
      "Loss at step 137450 : 3.198890209197998\n",
      "Loss at step 137500 : 2.4692676067352295\n",
      "Loss at step 137550 : 2.913599967956543\n",
      "Loss at step 137600 : 3.168832540512085\n",
      "Loss at step 137650 : 3.1230850219726562\n",
      "Loss at step 137700 : 2.5006561279296875\n",
      "Loss at step 137750 : 2.099369525909424\n",
      "Loss at step 137800 : 2.998293399810791\n",
      "Loss at step 137850 : 3.0683696269989014\n",
      "Loss at step 137900 : 2.9845311641693115\n",
      "Loss at step 137950 : 2.030191659927368\n",
      "Loss at step 138000 : 2.700551748275757\n",
      "Loss at step 138050 : 3.3690128326416016\n",
      "Loss at step 138100 : 2.6330618858337402\n",
      "Loss at step 138150 : 4.030135154724121\n",
      "Loss at step 138200 : 3.158057689666748\n",
      "Loss at step 138250 : 2.894688606262207\n",
      "Loss at step 138300 : 3.3603625297546387\n",
      "Loss at step 138350 : 2.979646921157837\n",
      "Loss at step 138400 : 3.172586441040039\n",
      "Loss at step 138450 : 2.4396581649780273\n",
      "Loss at step 138500 : 2.393894672393799\n",
      "Loss at step 138550 : 3.424345016479492\n",
      "Loss at step 138600 : 1.848760724067688\n",
      "Loss at step 138650 : 2.7914624214172363\n",
      "Loss at step 138700 : 3.0719153881073\n",
      "Loss at step 138750 : 2.878697395324707\n",
      "Loss at step 138800 : 2.2480416297912598\n",
      "Loss at step 138850 : 2.777672290802002\n",
      "Loss at step 138900 : 3.259734630584717\n",
      "Loss at step 138950 : 1.9009499549865723\n",
      "Loss at step 139000 : 3.668915271759033\n",
      "Loss at step 139050 : 3.1262993812561035\n",
      "Loss at step 139100 : 3.5695595741271973\n",
      "Loss at step 139150 : 2.033353328704834\n",
      "Loss at step 139200 : 2.406571865081787\n",
      "Loss at step 139250 : 2.2964980602264404\n",
      "Loss at step 139300 : 3.4105911254882812\n",
      "Loss at step 139350 : 2.4815917015075684\n",
      "Loss at step 139400 : 3.22536301612854\n",
      "Loss at step 139450 : 2.7315244674682617\n",
      "Loss at step 139500 : 3.3586883544921875\n",
      "Loss at step 139550 : 2.458146095275879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 139600 : 2.831434726715088\n",
      "Loss at step 139650 : 2.307422161102295\n",
      "Loss at step 139700 : 3.000295877456665\n",
      "Loss at step 139750 : 3.061882495880127\n",
      "Loss at step 139800 : 2.9861021041870117\n",
      "Loss at step 139850 : 2.6449360847473145\n",
      "Loss at step 139900 : 2.8612546920776367\n",
      "Loss at step 139950 : 3.340554714202881\n",
      "Loss at step 140000 : 2.9186103343963623\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 140050 : 3.24385404586792\n",
      "Loss at step 140100 : 3.6802868843078613\n",
      "Loss at step 140150 : 2.632960557937622\n",
      "Loss at step 140200 : 2.1150007247924805\n",
      "Loss at step 140250 : 2.7900314331054688\n",
      "Loss at step 140300 : 2.662296772003174\n",
      "Loss at step 140350 : 3.3597636222839355\n",
      "Loss at step 140400 : 2.598073959350586\n",
      "Loss at step 140450 : 2.40181827545166\n",
      "Loss at step 140500 : 3.8594911098480225\n",
      "Loss at step 140550 : 2.915485382080078\n",
      "Loss at step 140600 : 2.3174824714660645\n",
      "Loss at step 140650 : 3.2162277698516846\n",
      "Loss at step 140700 : 3.8577983379364014\n",
      "Loss at step 140750 : 2.276611804962158\n",
      "Loss at step 140800 : 3.2532100677490234\n",
      "Loss at step 140850 : 2.5757620334625244\n",
      "Loss at step 140900 : 3.126218795776367\n",
      "Loss at step 140950 : 3.0141191482543945\n",
      "Loss at step 141000 : 2.8291873931884766\n",
      "Loss at step 141050 : 2.609365940093994\n",
      "Loss at step 141100 : 3.960087537765503\n",
      "Loss at step 141150 : 4.083730697631836\n",
      "Loss at step 141200 : 3.0260703563690186\n",
      "Loss at step 141250 : 2.3851823806762695\n",
      "Loss at step 141300 : 2.7990915775299072\n",
      "Loss at step 141350 : 3.257046699523926\n",
      "Loss at step 141400 : 2.4201672077178955\n",
      "Loss at step 141450 : 2.913410186767578\n",
      "Loss at step 141500 : 3.288479804992676\n",
      "Loss at step 141550 : 3.155795097351074\n",
      "Loss at step 141600 : 3.3751120567321777\n",
      "Loss at step 141650 : 3.834138870239258\n",
      "Loss at step 141700 : 2.790402889251709\n",
      "Loss at step 141750 : 3.081268787384033\n",
      "Loss at step 141800 : 3.4216713905334473\n",
      "Loss at step 141850 : 3.355943202972412\n",
      "Loss at step 141900 : 3.0716638565063477\n",
      "Loss at step 141950 : 2.8726859092712402\n",
      "Loss at step 142000 : 2.4037375450134277\n",
      "Loss at step 142050 : 2.569739818572998\n",
      "Loss at step 142100 : 2.7056422233581543\n",
      "Loss at step 142150 : 3.361039161682129\n",
      "Loss at step 142200 : 2.6553616523742676\n",
      "Loss at step 142250 : 2.5226030349731445\n",
      "Loss at step 142300 : 2.2830209732055664\n",
      "Loss at step 142350 : 2.5305073261260986\n",
      "Loss at step 142400 : 3.3046231269836426\n",
      "Loss at step 142450 : 3.1532626152038574\n",
      "Loss at step 142500 : 2.9909005165100098\n",
      "Loss at step 142550 : 3.144514799118042\n",
      "Loss at step 142600 : 3.8475399017333984\n",
      "Loss at step 142650 : 2.260340690612793\n",
      "Loss at step 142700 : 2.708554983139038\n",
      "Loss at step 142750 : 3.036360740661621\n",
      "Loss at step 142800 : 2.8410489559173584\n",
      "Loss at step 142850 : 2.508963108062744\n",
      "Loss at step 142900 : 3.3419911861419678\n",
      "Loss at step 142950 : 2.6693687438964844\n",
      "Loss at step 143000 : 2.9177815914154053\n",
      "Loss at step 143050 : 2.4250030517578125\n",
      "Loss at step 143100 : 3.1050360202789307\n",
      "Loss at step 143150 : 2.904597520828247\n",
      "Loss at step 143200 : 2.7080931663513184\n",
      "Loss at step 143250 : 3.312490940093994\n",
      "Loss at step 143300 : 1.395463466644287\n",
      "Loss at step 143350 : 3.587627410888672\n",
      "Loss at step 143400 : 3.110499858856201\n",
      "Loss at step 143450 : 2.8028202056884766\n",
      "Loss at step 143500 : 3.950066089630127\n",
      "Loss at step 143550 : 4.207633018493652\n",
      "Loss at step 143600 : 3.398749828338623\n",
      "Loss at step 143650 : 4.2715325355529785\n",
      "Loss at step 143700 : 2.7154345512390137\n",
      "Loss at step 143750 : 1.710408329963684\n",
      "Loss at step 143800 : 2.9159719944000244\n",
      "Loss at step 143850 : 3.616661548614502\n",
      "Loss at step 143900 : 2.7612552642822266\n",
      "Loss at step 143950 : 3.433267593383789\n",
      "Loss at step 144000 : 3.1483564376831055\n",
      "Loss at step 144050 : 3.6100964546203613\n",
      "Loss at step 144100 : 2.838897228240967\n",
      "Loss at step 144150 : 3.177647590637207\n",
      "Loss at step 144200 : 2.6308934688568115\n",
      "Loss at step 144250 : 2.8809452056884766\n",
      "Loss at step 144300 : 2.9231386184692383\n",
      "Loss at step 144350 : 3.2990174293518066\n",
      "Loss at step 144400 : 3.2312235832214355\n",
      "Loss at step 144450 : 2.7367024421691895\n",
      "Loss at step 144500 : 3.0649185180664062\n",
      "Loss at step 144550 : 2.808533191680908\n",
      "Loss at step 144600 : 2.6228928565979004\n",
      "Loss at step 144650 : 2.9952468872070312\n",
      "Loss at step 144700 : 2.771601438522339\n",
      "Loss at step 144750 : 2.8955512046813965\n",
      "Loss at step 144800 : 2.8005757331848145\n",
      "Loss at step 144850 : 2.8622751235961914\n",
      "Loss at step 144900 : 3.4901137351989746\n",
      "Loss at step 144950 : 3.7299249172210693\n",
      "Loss at step 145000 : 2.561795711517334\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 145050 : 2.6359610557556152\n",
      "Loss at step 145100 : 2.6371169090270996\n",
      "Loss at step 145150 : 3.081852912902832\n",
      "Loss at step 145200 : 3.525913715362549\n",
      "Loss at step 145250 : 3.5273330211639404\n",
      "Loss at step 145300 : 2.750373363494873\n",
      "Loss at step 145350 : 3.2577590942382812\n",
      "Loss at step 145400 : 2.4370243549346924\n",
      "Loss at step 145450 : 2.7080531120300293\n",
      "Loss at step 145500 : 3.284733772277832\n",
      "Loss at step 145550 : 3.5997018814086914\n",
      "Loss at step 145600 : 3.4498445987701416\n",
      "Loss at step 145650 : 2.39335298538208\n",
      "Loss at step 145700 : 2.8891921043395996\n",
      "Loss at step 145750 : 2.6938302516937256\n",
      "Loss at step 145800 : 3.1320977210998535\n",
      "Loss at step 145850 : 2.269092559814453\n",
      "Loss at step 145900 : 2.7745609283447266\n",
      "Loss at step 145950 : 2.5139784812927246\n",
      "Loss at step 146000 : 3.273608684539795\n",
      "Loss at step 146050 : 4.359338283538818\n",
      "Loss at step 146100 : 2.5937247276306152\n",
      "Loss at step 146150 : 3.020217180252075\n",
      "Loss at step 146200 : 2.7277228832244873\n",
      "Loss at step 146250 : 3.186783790588379\n",
      "Loss at step 146300 : 3.139291286468506\n",
      "Loss at step 146350 : 2.656205415725708\n",
      "Loss at step 146400 : 1.4959042072296143\n",
      "Loss at step 146450 : 2.439513683319092\n",
      "Loss at step 146500 : 3.721982002258301\n",
      "Loss at step 146550 : 5.515207290649414\n",
      "Loss at step 146600 : 2.393200159072876\n",
      "Loss at step 146650 : 2.5637097358703613\n",
      "Loss at step 146700 : 2.541329860687256\n",
      "Loss at step 146750 : 3.24291729927063\n",
      "Loss at step 146800 : 3.4590578079223633\n",
      "Loss at step 146850 : 2.492882490158081\n",
      "Loss at step 146900 : 2.4870376586914062\n",
      "Loss at step 146950 : 3.960880756378174\n",
      "Loss at step 147000 : 3.583811044692993\n",
      "Loss at step 147050 : 3.196896553039551\n",
      "Loss at step 147100 : 2.664079189300537\n",
      "Loss at step 147150 : 4.023286819458008\n",
      "Loss at step 147200 : 2.545797824859619\n",
      "Loss at step 147250 : 2.6383590698242188\n",
      "Loss at step 147300 : 2.7202510833740234\n",
      "Loss at step 147350 : 2.439805507659912\n",
      "Loss at step 147400 : 3.4712297916412354\n",
      "Loss at step 147450 : 2.5907793045043945\n",
      "Loss at step 147500 : 2.489952564239502\n",
      "Loss at step 147550 : 3.1022744178771973\n",
      "Loss at step 147600 : 3.3056914806365967\n",
      "Loss at step 147650 : 2.7500224113464355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 147700 : 1.9925295114517212\n",
      "Loss at step 147750 : 3.2544422149658203\n",
      "Loss at step 147800 : 2.945370674133301\n",
      "Loss at step 147850 : 3.5414998531341553\n",
      "Loss at step 147900 : 3.2913968563079834\n",
      "Loss at step 147950 : 3.123749256134033\n",
      "Loss at step 148000 : 3.064244031906128\n",
      "Loss at step 148050 : 3.0046403408050537\n",
      "Loss at step 148100 : 2.993462085723877\n",
      "Loss at step 148150 : 2.599231719970703\n",
      "Loss at step 148200 : 2.649953603744507\n",
      "Loss at step 148250 : 3.0390334129333496\n",
      "Loss at step 148300 : 2.4959628582000732\n",
      "Loss at step 148350 : 1.7622523307800293\n",
      "Loss at step 148400 : 2.7981088161468506\n",
      "Loss at step 148450 : 2.575040102005005\n",
      "Loss at step 148500 : 3.5521717071533203\n",
      "Loss at step 148550 : 2.0146701335906982\n",
      "Loss at step 148600 : 2.72477650642395\n",
      "Loss at step 148650 : 1.9041593074798584\n",
      "Loss at step 148700 : 3.827796459197998\n",
      "Loss at step 148750 : 2.1806375980377197\n",
      "Loss at step 148800 : 4.175812721252441\n",
      "Loss at step 148850 : 3.4207863807678223\n",
      "Loss at step 148900 : 2.214794158935547\n",
      "Loss at step 148950 : 2.803750514984131\n",
      "Loss at step 149000 : 2.157097578048706\n",
      "Loss at step 149050 : 3.247103691101074\n",
      "Loss at step 149100 : 2.966158866882324\n",
      "Loss at step 149150 : 3.315676212310791\n",
      "Loss at step 149200 : 2.928122043609619\n",
      "Loss at step 149250 : 3.358105421066284\n",
      "Loss at step 149300 : 2.741978406906128\n",
      "Loss at step 149350 : 2.417339324951172\n",
      "Loss at step 149400 : 2.174699544906616\n",
      "Loss at step 149450 : 3.582797050476074\n",
      "Loss at step 149500 : 2.3833625316619873\n",
      "Loss at step 149550 : 2.8931689262390137\n",
      "Loss at step 149600 : 3.2666895389556885\n",
      "Loss at step 149650 : 2.7730460166931152\n",
      "Loss at step 149700 : 2.990583896636963\n",
      "Loss at step 149750 : 2.615838050842285\n",
      "Loss at step 149800 : 4.033278465270996\n",
      "Loss at step 149850 : 2.2226622104644775\n",
      "Loss at step 149900 : 3.2189102172851562\n",
      "Loss at step 149950 : 2.462552070617676\n",
      "Loss at step 150000 : 2.634267568588257\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 150050 : 2.4408445358276367\n",
      "Loss at step 150100 : 2.7395215034484863\n",
      "Loss at step 150150 : 2.975191593170166\n",
      "Loss at step 150200 : 2.922698974609375\n",
      "Loss at step 150250 : 2.890964984893799\n",
      "Loss at step 150300 : 3.5747783184051514\n",
      "Loss at step 150350 : 3.2537941932678223\n",
      "Loss at step 150400 : 3.1272213459014893\n",
      "Loss at step 150450 : 2.1756041049957275\n",
      "Loss at step 150500 : 2.952859878540039\n",
      "Loss at step 150550 : 2.4248602390289307\n",
      "Loss at step 150600 : 3.239804744720459\n",
      "Loss at step 150650 : 3.4462809562683105\n",
      "Loss at step 150700 : 3.145538330078125\n",
      "Loss at step 150750 : 3.624798059463501\n",
      "Loss at step 150800 : 2.1003940105438232\n",
      "Loss at step 150850 : 2.449537754058838\n",
      "Loss at step 150900 : 3.5993926525115967\n",
      "Loss at step 150950 : 2.063680410385132\n",
      "Loss at step 151000 : 3.031689405441284\n",
      "Loss at step 151050 : 2.5230367183685303\n",
      "Loss at step 151100 : 1.9190788269042969\n",
      "Loss at step 151150 : 3.414422035217285\n",
      "Loss at step 151200 : 3.1476664543151855\n",
      "Loss at step 151250 : 3.050055742263794\n",
      "Loss at step 151300 : 2.6682674884796143\n",
      "Loss at step 151350 : 2.6978116035461426\n",
      "Loss at step 151400 : 3.318676471710205\n",
      "Loss at step 151450 : 2.3625588417053223\n",
      "Loss at step 151500 : 2.2031636238098145\n",
      "Loss at step 151550 : 3.3790664672851562\n",
      "Loss at step 151600 : 2.677844285964966\n",
      "Loss at step 151650 : 3.1129636764526367\n",
      "Loss at step 151700 : 2.432493209838867\n",
      "Loss at step 151750 : 4.297390937805176\n",
      "Loss at step 151800 : 3.1922597885131836\n",
      "Loss at step 151850 : 4.24832820892334\n",
      "Loss at step 151900 : 3.166794776916504\n",
      "Loss at step 151950 : 2.002528667449951\n",
      "Loss at step 152000 : 3.2699835300445557\n",
      "Loss at step 152050 : 3.435849666595459\n",
      "Loss at step 152100 : 2.483355760574341\n",
      "Loss at step 152150 : 2.4399640560150146\n",
      "Loss at step 152200 : 3.125685691833496\n",
      "Loss at step 152250 : 2.6254169940948486\n",
      "Loss at step 152300 : 3.3416550159454346\n",
      "Loss at step 152350 : 2.8519082069396973\n",
      "Loss at step 152400 : 2.5400538444519043\n",
      "Loss at step 152450 : 3.3440980911254883\n",
      "Loss at step 152500 : 3.7861099243164062\n",
      "Loss at step 152550 : 3.2663092613220215\n",
      "Loss at step 152600 : 4.353560447692871\n",
      "Loss at step 152650 : 2.47038197517395\n",
      "Loss at step 152700 : 3.0717720985412598\n",
      "Loss at step 152750 : 2.3227572441101074\n",
      "Loss at step 152800 : 2.561251640319824\n",
      "Loss at step 152850 : 4.0214433670043945\n",
      "Loss at step 152900 : 2.1029863357543945\n",
      "Loss at step 152950 : 3.003268241882324\n",
      "Loss at step 153000 : 2.858908176422119\n",
      "Loss at step 153050 : 3.1266088485717773\n",
      "Loss at step 153100 : 3.720506429672241\n",
      "Loss at step 153150 : 1.9978455305099487\n",
      "Loss at step 153200 : 3.0246074199676514\n",
      "Loss at step 153250 : 3.931562900543213\n",
      "Loss at step 153300 : 2.849337100982666\n",
      "Loss at step 153350 : 3.8228721618652344\n",
      "Loss at step 153400 : 3.1632513999938965\n",
      "Loss at step 153450 : 3.0884273052215576\n",
      "Loss at step 153500 : 3.4324052333831787\n",
      "Loss at step 153550 : 4.2544989585876465\n",
      "Loss at step 153600 : 2.763660430908203\n",
      "Loss at step 153650 : 3.0891685485839844\n",
      "Loss at step 153700 : 2.9233062267303467\n",
      "Loss at step 153750 : 1.9314900636672974\n",
      "Loss at step 153800 : 3.3124356269836426\n",
      "Loss at step 153850 : 3.2200913429260254\n",
      "Loss at step 153900 : 2.887401819229126\n",
      "Loss at step 153950 : 2.347836971282959\n",
      "Loss at step 154000 : 2.404510974884033\n",
      "Loss at step 154050 : 2.5356740951538086\n",
      "Loss at step 154100 : 3.095336437225342\n",
      "Loss at step 154150 : 4.497054576873779\n",
      "Loss at step 154200 : 3.0252790451049805\n",
      "Loss at step 154250 : 2.7667696475982666\n",
      "Loss at step 154300 : 3.2905335426330566\n",
      "Loss at step 154350 : 2.183049440383911\n",
      "Loss at step 154400 : 2.7378106117248535\n",
      "Loss at step 154450 : 3.218994617462158\n",
      "Loss at step 154500 : 2.3967297077178955\n",
      "Loss at step 154550 : 3.096982955932617\n",
      "Loss at step 154600 : 2.9806671142578125\n",
      "Loss at step 154650 : 2.319701910018921\n",
      "Loss at step 154700 : 2.6662774085998535\n",
      "Loss at step 154750 : 2.4478025436401367\n",
      "Loss at step 154800 : 3.4952173233032227\n",
      "Loss at step 154850 : 3.1667778491973877\n",
      "Loss at step 154900 : 2.1501622200012207\n",
      "Loss at step 154950 : 2.386868953704834\n",
      "Loss at step 155000 : 3.5298140048980713\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 155050 : 2.5509753227233887\n",
      "Loss at step 155100 : 2.8962838649749756\n",
      "Loss at step 155150 : 3.6032252311706543\n",
      "Loss at step 155200 : 2.8531150817871094\n",
      "Loss at step 155250 : 2.849816083908081\n",
      "Loss at step 155300 : 3.640043258666992\n",
      "Loss at step 155350 : 2.660983085632324\n",
      "Loss at step 155400 : 2.9918346405029297\n",
      "Loss at step 155450 : 3.1981494426727295\n",
      "Loss at step 155500 : 3.276665687561035\n",
      "Loss at step 155550 : 2.4949305057525635\n",
      "Loss at step 155600 : 3.1960372924804688\n",
      "Loss at step 155650 : 2.914921760559082\n",
      "Loss at step 155700 : 3.328594923019409\n",
      "Loss at step 155750 : 3.3342814445495605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 155800 : 4.753554344177246\n",
      "Loss at step 155850 : 2.9524624347686768\n",
      "Loss at step 155900 : 2.946901798248291\n",
      "Loss at step 155950 : 3.3063604831695557\n",
      "Loss at step 156000 : 2.1632256507873535\n",
      "Loss at step 156050 : 2.4762096405029297\n",
      "Loss at step 156100 : 2.3655595779418945\n",
      "Loss at step 156150 : 3.2554404735565186\n",
      "Loss at step 156200 : 2.079461097717285\n",
      "Loss at step 156250 : 2.618549346923828\n",
      "Loss at step 156300 : 3.726295232772827\n",
      "Loss at step 156350 : 3.4405641555786133\n",
      "Loss at step 156400 : 3.167638063430786\n",
      "Loss at step 156450 : 3.1504063606262207\n",
      "Loss at step 156500 : 1.9345051050186157\n",
      "Loss at step 156550 : 2.6353306770324707\n",
      "Loss at step 156600 : 4.039382457733154\n",
      "Loss at step 156650 : 2.7869715690612793\n",
      "Loss at step 156700 : 3.4208996295928955\n",
      "Loss at step 156750 : 2.573464870452881\n",
      "Loss at step 156800 : 2.6233670711517334\n",
      "Loss at step 156850 : 3.078625202178955\n",
      "Loss at step 156900 : 3.0101566314697266\n",
      "Loss at step 156950 : 3.6957008838653564\n",
      "Loss at step 157000 : 2.181732416152954\n",
      "Loss at step 157050 : 2.973597526550293\n",
      "Loss at step 157100 : 2.040264844894409\n",
      "Loss at step 157150 : 3.0754408836364746\n",
      "Loss at step 157200 : 2.500641345977783\n",
      "Loss at step 157250 : 1.991951584815979\n",
      "Loss at step 157300 : 3.4560976028442383\n",
      "Loss at step 157350 : 1.629638910293579\n",
      "Loss at step 157400 : 2.7760229110717773\n",
      "Loss at step 157450 : 2.175096035003662\n",
      "Loss at step 157500 : 2.712409496307373\n",
      "Loss at step 157550 : 3.1029186248779297\n",
      "Loss at step 157600 : 2.8367016315460205\n",
      "Loss at step 157650 : 2.95514178276062\n",
      "Loss at step 157700 : 2.6053783893585205\n",
      "Loss at step 157750 : 2.332252264022827\n",
      "Loss at step 157800 : 3.002753734588623\n",
      "Loss at step 157850 : 2.2335214614868164\n",
      "Loss at step 157900 : 2.3866424560546875\n",
      "Loss at step 157950 : 2.939558267593384\n",
      "Loss at step 158000 : 2.4531595706939697\n",
      "Loss at step 158050 : 2.5530600547790527\n",
      "Loss at step 158100 : 3.1475093364715576\n",
      "Loss at step 158150 : 3.5828664302825928\n",
      "Loss at step 158200 : 2.9723682403564453\n",
      "Loss at step 158250 : 2.7852985858917236\n",
      "Loss at step 158300 : 3.226595163345337\n",
      "Loss at step 158350 : 3.8950819969177246\n",
      "Loss at step 158400 : 2.301750421524048\n",
      "Loss at step 158450 : 2.4639055728912354\n",
      "Loss at step 158500 : 2.3273048400878906\n",
      "Loss at step 158550 : 3.9980666637420654\n",
      "Loss at step 158600 : 2.637382984161377\n",
      "Loss at step 158650 : 2.4407858848571777\n",
      "Loss at step 158700 : 2.6168854236602783\n",
      "Loss at step 158750 : 3.669279098510742\n",
      "Loss at step 158800 : 3.5199244022369385\n",
      "Loss at step 158850 : 2.8434174060821533\n",
      "Loss at step 158900 : 2.8618812561035156\n",
      "Loss at step 158950 : 2.1762874126434326\n",
      "Loss at step 159000 : 3.083026885986328\n",
      "Loss at step 159050 : 2.3442070484161377\n",
      "Loss at step 159100 : 3.111884593963623\n",
      "Loss at step 159150 : 2.772380828857422\n",
      "Loss at step 159200 : 2.8697593212127686\n",
      "Loss at step 159250 : 3.5318703651428223\n",
      "Loss at step 159300 : 2.7635815143585205\n",
      "Loss at step 159350 : 3.0957298278808594\n",
      "Loss at step 159400 : 2.9744081497192383\n",
      "Loss at step 159450 : 3.119406223297119\n",
      "Loss at step 159500 : 2.735671281814575\n",
      "Loss at step 159550 : 1.9787817001342773\n",
      "Loss at step 159600 : 2.2472081184387207\n",
      "Loss at step 159650 : 3.1086525917053223\n",
      "Loss at step 159700 : 2.262399673461914\n",
      "Loss at step 159750 : 5.160945415496826\n",
      "Loss at step 159800 : 3.2856552600860596\n",
      "Loss at step 159850 : 2.731039524078369\n",
      "Loss at step 159900 : 2.6206371784210205\n",
      "Loss at step 159950 : 3.312206745147705\n",
      "Loss at step 160000 : 2.0800540447235107\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 160050 : 3.736577033996582\n",
      "Loss at step 160100 : 2.722073793411255\n",
      "Loss at step 160150 : 3.2789950370788574\n",
      "Loss at step 160200 : 3.539264678955078\n",
      "Loss at step 160250 : 2.6550421714782715\n",
      "Loss at step 160300 : 3.2664170265197754\n",
      "Loss at step 160350 : 2.7274715900421143\n",
      "Loss at step 160400 : 2.464545249938965\n",
      "Loss at step 160450 : 3.30159068107605\n",
      "Loss at step 160500 : 2.8481273651123047\n",
      "Loss at step 160550 : 3.4791197776794434\n",
      "Loss at step 160600 : 2.757768154144287\n",
      "Loss at step 160650 : 2.692286491394043\n",
      "Loss at step 160700 : 2.6874592304229736\n",
      "Loss at step 160750 : 3.1004910469055176\n",
      "Loss at step 160800 : 3.3315162658691406\n",
      "Loss at step 160850 : 3.100369930267334\n",
      "Loss at step 160900 : 3.4983439445495605\n",
      "Loss at step 160950 : 2.6250338554382324\n",
      "Loss at step 161000 : 2.9573943614959717\n",
      "Loss at step 161050 : 3.1515719890594482\n",
      "Loss at step 161100 : 3.0696675777435303\n",
      "Loss at step 161150 : 2.0737006664276123\n",
      "Loss at step 161200 : 2.8454065322875977\n",
      "Loss at step 161250 : 3.179272174835205\n",
      "Loss at step 161300 : 2.0123438835144043\n",
      "Loss at step 161350 : 2.5859131813049316\n",
      "Loss at step 161400 : 3.401998519897461\n",
      "Loss at step 161450 : 2.5529181957244873\n",
      "Loss at step 161500 : 2.629187822341919\n",
      "Loss at step 161550 : 2.840782880783081\n",
      "Loss at step 161600 : 2.678676128387451\n",
      "Loss at step 161650 : 2.632523536682129\n",
      "Loss at step 161700 : 3.6727142333984375\n",
      "Loss at step 161750 : 2.347977638244629\n",
      "Loss at step 161800 : 3.182314395904541\n",
      "Loss at step 161850 : 2.5269625186920166\n",
      "Loss at step 161900 : 2.084282398223877\n",
      "Loss at step 161950 : 2.872972011566162\n",
      "Loss at step 162000 : 3.5270955562591553\n",
      "Loss at step 162050 : 2.7153573036193848\n",
      "Loss at step 162100 : 3.0138003826141357\n",
      "Loss at step 162150 : 3.4135332107543945\n",
      "Loss at step 162200 : 2.2463908195495605\n",
      "Loss at step 162250 : 3.630037307739258\n",
      "Loss at step 162300 : 2.435147762298584\n",
      "Loss at step 162350 : 3.1536989212036133\n",
      "Loss at step 162400 : 3.135909080505371\n",
      "Loss at step 162450 : 3.0390615463256836\n",
      "Loss at step 162500 : 3.3610289096832275\n",
      "Loss at step 162550 : 3.1315319538116455\n",
      "Loss at step 162600 : 3.5660510063171387\n",
      "Loss at step 162650 : 2.7542366981506348\n",
      "Loss at step 162700 : 3.2725906372070312\n",
      "Loss at step 162750 : 4.19382381439209\n",
      "Loss at step 162800 : 2.7743852138519287\n",
      "Loss at step 162850 : 2.568185329437256\n",
      "Loss at step 162900 : 2.19831919670105\n",
      "Loss at step 162950 : 2.9270834922790527\n",
      "Loss at step 163000 : 2.065460205078125\n",
      "Loss at step 163050 : 2.6654672622680664\n",
      "Loss at step 163100 : 2.8327267169952393\n",
      "Loss at step 163150 : 2.7868404388427734\n",
      "Loss at step 163200 : 2.9881012439727783\n",
      "Loss at step 163250 : 3.349544048309326\n",
      "Loss at step 163300 : 2.798015832901001\n",
      "Loss at step 163350 : 3.103404998779297\n",
      "Loss at step 163400 : 2.878278970718384\n",
      "Loss at step 163450 : 3.603339195251465\n",
      "Loss at step 163500 : 3.0876364707946777\n",
      "Loss at step 163550 : 2.442424774169922\n",
      "Loss at step 163600 : 2.607387065887451\n",
      "Loss at step 163650 : 2.3792130947113037\n",
      "Loss at step 163700 : 2.573269844055176\n",
      "Loss at step 163750 : 3.4506123065948486\n",
      "Loss at step 163800 : 2.466036796569824\n",
      "Loss at step 163850 : 1.9476985931396484\n",
      "Loss at step 163900 : 2.968486785888672\n",
      "Loss at step 163950 : 2.9835519790649414\n",
      "Loss at step 164000 : 2.881661891937256\n",
      "Loss at step 164050 : 2.8577489852905273\n",
      "Loss at step 164100 : 3.322648525238037\n",
      "Loss at step 164150 : 2.6744112968444824\n",
      "Loss at step 164200 : 2.476468801498413\n",
      "Loss at step 164250 : 2.346325159072876\n",
      "Loss at step 164300 : 2.833287000656128\n",
      "Loss at step 164350 : 3.232424020767212\n",
      "Loss at step 164400 : 2.929800033569336\n",
      "Loss at step 164450 : 2.335143566131592\n",
      "Loss at step 164500 : 3.051877975463867\n",
      "Loss at step 164550 : 2.4502739906311035\n",
      "Loss at step 164600 : 3.643714189529419\n",
      "Loss at step 164650 : 3.248352527618408\n",
      "Loss at step 164700 : 2.70310640335083\n",
      "Loss at step 164750 : 3.167898654937744\n",
      "Loss at step 164800 : 3.2507688999176025\n",
      "Loss at step 164850 : 3.0743303298950195\n",
      "Loss at step 164900 : 3.1726670265197754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 164950 : 3.106454849243164\n",
      "Loss at step 165000 : 3.5526018142700195\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 165050 : 2.114621162414551\n",
      "Loss at step 165100 : 2.7790424823760986\n",
      "Loss at step 165150 : 2.7160167694091797\n",
      "Loss at step 165200 : 2.441523790359497\n",
      "Loss at step 165250 : 2.7331721782684326\n",
      "Loss at step 165300 : 3.9287590980529785\n",
      "Loss at step 165350 : 2.766476631164551\n",
      "Loss at step 165400 : 3.3960318565368652\n",
      "Loss at step 165450 : 2.4024834632873535\n",
      "Loss at step 165500 : 2.704970598220825\n",
      "Loss at step 165550 : 3.0212559700012207\n",
      "Loss at step 165600 : 2.5667619705200195\n",
      "Loss at step 165650 : 2.335174560546875\n",
      "Loss at step 165700 : 2.8454489707946777\n",
      "Loss at step 165750 : 3.082595109939575\n",
      "Loss at step 165800 : 3.2893741130828857\n",
      "Loss at step 165850 : 2.7866973876953125\n",
      "Loss at step 165900 : 2.9827675819396973\n",
      "Loss at step 165950 : 3.739342212677002\n",
      "Loss at step 166000 : 2.2860453128814697\n",
      "Loss at step 166050 : 3.6210789680480957\n",
      "Loss at step 166100 : 2.576836347579956\n",
      "Loss at step 166150 : 3.0300374031066895\n",
      "Loss at step 166200 : 3.278698444366455\n",
      "Loss at step 166250 : 3.472431182861328\n",
      "Loss at step 166300 : 2.5541038513183594\n",
      "Loss at step 166350 : 2.8619184494018555\n",
      "Loss at step 166400 : 2.9157373905181885\n",
      "Loss at step 166450 : 2.730478286743164\n",
      "Loss at step 166500 : 2.7028427124023438\n",
      "Loss at step 166550 : 3.550291061401367\n",
      "Loss at step 166600 : 2.753648281097412\n",
      "Loss at step 166650 : 3.348243236541748\n",
      "Loss at step 166700 : 1.5109493732452393\n",
      "Loss at step 166750 : 3.1248791217803955\n",
      "Loss at step 166800 : 3.4664368629455566\n",
      "Loss at step 166850 : 3.107520341873169\n",
      "Loss at step 166900 : 3.5638763904571533\n",
      "Loss at step 166950 : 3.3764872550964355\n",
      "Loss at step 167000 : 3.3685736656188965\n",
      "Loss at step 167050 : 3.3261594772338867\n",
      "Loss at step 167100 : 2.5369884967803955\n",
      "Loss at step 167150 : 2.548093318939209\n",
      "Loss at step 167200 : 2.945645332336426\n",
      "Loss at step 167250 : 2.868166923522949\n",
      "Loss at step 167300 : 3.227557897567749\n",
      "Loss at step 167350 : 2.4231770038604736\n",
      "Loss at step 167400 : 2.539381265640259\n",
      "Loss at step 167450 : 3.124411106109619\n",
      "Loss at step 167500 : 2.5090088844299316\n",
      "Loss at step 167550 : 2.9244730472564697\n",
      "Loss at step 167600 : 2.223757743835449\n",
      "Loss at step 167650 : 2.9847025871276855\n",
      "Loss at step 167700 : 3.395397186279297\n",
      "Loss at step 167750 : 2.524082660675049\n",
      "Loss at step 167800 : 3.356017589569092\n",
      "Loss at step 167850 : 3.1744112968444824\n",
      "Loss at step 167900 : 3.136460304260254\n",
      "Loss at step 167950 : 2.3022780418395996\n",
      "Loss at step 168000 : 2.807502508163452\n",
      "Loss at step 168050 : 3.366924285888672\n",
      "Loss at step 168100 : 2.638916254043579\n",
      "Loss at step 168150 : 2.5298147201538086\n",
      "Loss at step 168200 : 2.4271175861358643\n",
      "Loss at step 168250 : 3.086740016937256\n",
      "Loss at step 168300 : 3.1429266929626465\n",
      "Loss at step 168350 : 3.4015870094299316\n",
      "Loss at step 168400 : 3.5629987716674805\n",
      "Loss at step 168450 : 2.678412675857544\n",
      "Loss at step 168500 : 3.193154811859131\n",
      "Loss at step 168550 : 3.1177778244018555\n",
      "Loss at step 168600 : 3.3048315048217773\n",
      "Loss at step 168650 : 2.4731554985046387\n",
      "Loss at step 168700 : 2.578462600708008\n",
      "Loss at step 168750 : 3.420273780822754\n",
      "Loss at step 168800 : 2.5325887203216553\n",
      "Loss at step 168850 : 2.955320358276367\n",
      "Loss at step 168900 : 3.6092703342437744\n",
      "Loss at step 168950 : 2.8719685077667236\n",
      "Loss at step 169000 : 2.3608789443969727\n",
      "Loss at step 169050 : 2.163233995437622\n",
      "Loss at step 169100 : 2.6484155654907227\n",
      "Loss at step 169150 : 2.464047431945801\n",
      "Loss at step 169200 : 2.7080183029174805\n",
      "Loss at step 169250 : 2.1911354064941406\n",
      "Loss at step 169300 : 2.9868927001953125\n",
      "Loss at step 169350 : 3.2786145210266113\n",
      "Loss at step 169400 : 3.062676429748535\n",
      "Loss at step 169450 : 2.2509100437164307\n",
      "Loss at step 169500 : 2.9236650466918945\n",
      "Loss at step 169550 : 1.829054355621338\n",
      "Loss at step 169600 : 4.317417144775391\n",
      "Loss at step 169650 : 3.4952335357666016\n",
      "Loss at step 169700 : 2.4849071502685547\n",
      "Loss at step 169750 : 2.64101243019104\n",
      "Loss at step 169800 : 2.145146131515503\n",
      "Loss at step 169850 : 2.5294125080108643\n",
      "Loss at step 169900 : 2.9953408241271973\n",
      "Loss at step 169950 : 2.3114662170410156\n",
      "Loss at step 170000 : 2.614778995513916\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 170050 : 1.8811800479888916\n",
      "Loss at step 170100 : 3.748777389526367\n",
      "Loss at step 170150 : 2.604067802429199\n",
      "Loss at step 170200 : 3.3931612968444824\n",
      "Loss at step 170250 : 2.5918941497802734\n",
      "Loss at step 170300 : 3.4903905391693115\n",
      "Loss at step 170350 : 3.0060524940490723\n",
      "Loss at step 170400 : 3.6538772583007812\n",
      "Loss at step 170450 : 2.9188499450683594\n",
      "Loss at step 170500 : 3.3772637844085693\n",
      "Loss at step 170550 : 2.684363842010498\n",
      "Loss at step 170600 : 3.403148651123047\n",
      "Loss at step 170650 : 2.434295177459717\n",
      "Loss at step 170700 : 2.2845215797424316\n",
      "Loss at step 170750 : 4.3856096267700195\n",
      "Loss at step 170800 : 3.4897847175598145\n",
      "Loss at step 170850 : 2.934441566467285\n",
      "Loss at step 170900 : 2.80517578125\n",
      "Loss at step 170950 : 3.3677141666412354\n",
      "Loss at step 171000 : 2.6407461166381836\n",
      "Loss at step 171050 : 3.23529314994812\n",
      "Loss at step 171100 : 3.1693179607391357\n",
      "Loss at step 171150 : 4.457420349121094\n",
      "Loss at step 171200 : 2.8951029777526855\n",
      "Loss at step 171250 : 2.176435947418213\n",
      "Loss at step 171300 : 3.1743760108947754\n",
      "Loss at step 171350 : 2.490408420562744\n",
      "Loss at step 171400 : 3.6048085689544678\n",
      "Loss at step 171450 : 3.252086639404297\n",
      "Loss at step 171500 : 3.037315845489502\n",
      "Loss at step 171550 : 2.813610553741455\n",
      "Loss at step 171600 : 2.190890312194824\n",
      "Loss at step 171650 : 3.0532007217407227\n",
      "Loss at step 171700 : 3.764171838760376\n",
      "Loss at step 171750 : 3.274331569671631\n",
      "Loss at step 171800 : 3.237257242202759\n",
      "Loss at step 171850 : 3.1966845989227295\n",
      "Loss at step 171900 : 2.831233501434326\n",
      "Loss at step 171950 : 3.5638723373413086\n",
      "Loss at step 172000 : 2.418966770172119\n",
      "Loss at step 172050 : 2.1091794967651367\n",
      "Loss at step 172100 : 2.2583484649658203\n",
      "Loss at step 172150 : 4.242101669311523\n",
      "Loss at step 172200 : 3.3372859954833984\n",
      "Loss at step 172250 : 2.979762554168701\n",
      "Loss at step 172300 : 2.8600950241088867\n",
      "Loss at step 172350 : 2.9971442222595215\n",
      "Loss at step 172400 : 2.2392802238464355\n",
      "Loss at step 172450 : 3.0346004962921143\n",
      "Loss at step 172500 : 1.893049716949463\n",
      "Loss at step 172550 : 3.014251470565796\n",
      "Loss at step 172600 : 3.883718729019165\n",
      "Loss at step 172650 : 3.1635470390319824\n",
      "Loss at step 172700 : 2.952864408493042\n",
      "Loss at step 172750 : 2.7821779251098633\n",
      "Loss at step 172800 : 3.055964708328247\n",
      "Loss at step 172850 : 2.3017287254333496\n",
      "Loss at step 172900 : 2.925076961517334\n",
      "Loss at step 172950 : 2.493499755859375\n",
      "Loss at step 173000 : 2.970323085784912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 173050 : 2.7165729999542236\n",
      "Loss at step 173100 : 2.7066116333007812\n",
      "Loss at step 173150 : 4.08202600479126\n",
      "Loss at step 173200 : 3.42217755317688\n",
      "Loss at step 173250 : 2.8906545639038086\n",
      "Loss at step 173300 : 2.244373083114624\n",
      "Loss at step 173350 : 2.066524028778076\n",
      "Loss at step 173400 : 2.4893829822540283\n",
      "Loss at step 173450 : 2.742922306060791\n",
      "Loss at step 173500 : 3.0356266498565674\n",
      "Loss at step 173550 : 3.0591681003570557\n",
      "Loss at step 173600 : 2.757065534591675\n",
      "Loss at step 173650 : 3.519181728363037\n",
      "Loss at step 173700 : 2.618668556213379\n",
      "Loss at step 173750 : 2.9896864891052246\n",
      "Loss at step 173800 : 1.9236602783203125\n",
      "Loss at step 173850 : 2.8907907009124756\n",
      "Loss at step 173900 : 2.618011713027954\n",
      "Loss at step 173950 : 3.4663078784942627\n",
      "Loss at step 174000 : 4.155165672302246\n",
      "Loss at step 174050 : 2.917320489883423\n",
      "Loss at step 174100 : 3.1401357650756836\n",
      "Loss at step 174150 : 2.511472702026367\n",
      "Loss at step 174200 : 3.7290937900543213\n",
      "Loss at step 174250 : 2.1544551849365234\n",
      "Loss at step 174300 : 2.904367208480835\n",
      "Loss at step 174350 : 2.6702308654785156\n",
      "Loss at step 174400 : 3.374680519104004\n",
      "Loss at step 174450 : 3.773432731628418\n",
      "Loss at step 174500 : 3.2004780769348145\n",
      "Loss at step 174550 : 3.5100207328796387\n",
      "Loss at step 174600 : 2.3317465782165527\n",
      "Loss at step 174650 : 2.482908248901367\n",
      "Loss at step 174700 : 3.0545239448547363\n",
      "Loss at step 174750 : 2.49239444732666\n",
      "Loss at step 174800 : 2.176316261291504\n",
      "Loss at step 174850 : 2.6154041290283203\n",
      "Loss at step 174900 : 3.425230026245117\n",
      "Loss at step 174950 : 2.5666255950927734\n",
      "Loss at step 175000 : 2.0539474487304688\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 175050 : 2.135988235473633\n",
      "Loss at step 175100 : 3.7609922885894775\n",
      "Loss at step 175150 : 2.878514289855957\n",
      "Loss at step 175200 : 2.6409664154052734\n",
      "Loss at step 175250 : 2.2343435287475586\n",
      "Loss at step 175300 : 3.2106168270111084\n",
      "Loss at step 175350 : 2.5212197303771973\n",
      "Loss at step 175400 : 3.886934280395508\n",
      "Loss at step 175450 : 4.392195701599121\n",
      "Loss at step 175500 : 3.7301926612854004\n",
      "Loss at step 175550 : 2.8646364212036133\n",
      "Loss at step 175600 : 2.7327611446380615\n",
      "Loss at step 175650 : 2.3472278118133545\n",
      "Loss at step 175700 : 2.532845973968506\n",
      "Loss at step 175750 : 2.575141191482544\n",
      "Loss at step 175800 : 2.983546257019043\n",
      "Loss at step 175850 : 1.2355701923370361\n",
      "Loss at step 175900 : 3.231691837310791\n",
      "Loss at step 175950 : 2.9556310176849365\n",
      "Loss at step 176000 : 3.0482349395751953\n",
      "Loss at step 176050 : 1.9531350135803223\n",
      "Loss at step 176100 : 3.0299978256225586\n",
      "Loss at step 176150 : 1.9382011890411377\n",
      "Loss at step 176200 : 2.7522659301757812\n",
      "Loss at step 176250 : 2.888315439224243\n",
      "Loss at step 176300 : 1.8048679828643799\n",
      "Loss at step 176350 : 3.050250768661499\n",
      "Loss at step 176400 : 3.3008131980895996\n",
      "Loss at step 176450 : 2.4816627502441406\n",
      "Loss at step 176500 : 3.024149179458618\n",
      "Loss at step 176550 : 2.269843578338623\n",
      "Loss at step 176600 : 3.134110927581787\n",
      "Loss at step 176650 : 2.1318540573120117\n",
      "Loss at step 176700 : 3.024561882019043\n",
      "Loss at step 176750 : 1.998853325843811\n",
      "Loss at step 176800 : 2.2019267082214355\n",
      "Loss at step 176850 : 3.0377871990203857\n",
      "Loss at step 176900 : 2.8375649452209473\n",
      "Loss at step 176950 : 2.5369820594787598\n",
      "Loss at step 177000 : 3.049140214920044\n",
      "Loss at step 177050 : 1.4872429370880127\n",
      "Loss at step 177100 : 3.625770330429077\n",
      "Loss at step 177150 : 2.6341652870178223\n",
      "Loss at step 177200 : 2.514587879180908\n",
      "Loss at step 177250 : 2.8500702381134033\n",
      "Loss at step 177300 : 2.2058873176574707\n",
      "Loss at step 177350 : 2.3948311805725098\n",
      "Loss at step 177400 : 3.0926547050476074\n",
      "Loss at step 177450 : 2.8761250972747803\n",
      "Loss at step 177500 : 4.111349105834961\n",
      "Loss at step 177550 : 2.689095973968506\n",
      "Loss at step 177600 : 3.217651128768921\n",
      "Loss at step 177650 : 3.129493236541748\n",
      "Loss at step 177700 : 3.2137231826782227\n",
      "Loss at step 177750 : 3.990901470184326\n",
      "Loss at step 177800 : 2.7230329513549805\n",
      "Loss at step 177850 : 3.2874608039855957\n",
      "Loss at step 177900 : 2.07688570022583\n",
      "Loss at step 177950 : 2.482351303100586\n",
      "Loss at step 178000 : 2.796346426010132\n",
      "Loss at step 178050 : 2.8229644298553467\n",
      "Loss at step 178100 : 2.9451680183410645\n",
      "Loss at step 178150 : 4.310733795166016\n",
      "Loss at step 178200 : 2.2073116302490234\n",
      "Loss at step 178250 : 2.8956170082092285\n",
      "Loss at step 178300 : 3.066152572631836\n",
      "Loss at step 178350 : 2.8532369136810303\n",
      "Loss at step 178400 : 2.8927512168884277\n",
      "Loss at step 178450 : 4.16429328918457\n",
      "Loss at step 178500 : 2.713524341583252\n",
      "Loss at step 178550 : 2.572047233581543\n",
      "Loss at step 178600 : 3.9661099910736084\n",
      "Loss at step 178650 : 3.718090534210205\n",
      "Loss at step 178700 : 2.4512393474578857\n",
      "Loss at step 178750 : 3.164092540740967\n",
      "Loss at step 178800 : 2.4847450256347656\n",
      "Loss at step 178850 : 2.1938185691833496\n",
      "Loss at step 178900 : 4.19305419921875\n",
      "Loss at step 178950 : 3.1650257110595703\n",
      "Loss at step 179000 : 2.8789281845092773\n",
      "Loss at step 179050 : 3.043271064758301\n",
      "Loss at step 179100 : 2.988123893737793\n",
      "Loss at step 179150 : 2.9169695377349854\n",
      "Loss at step 179200 : 3.0139126777648926\n",
      "Loss at step 179250 : 2.5579733848571777\n",
      "Loss at step 179300 : 2.7569990158081055\n",
      "Loss at step 179350 : 3.3077006340026855\n",
      "Loss at step 179400 : 3.569998264312744\n",
      "Loss at step 179450 : 3.0516693592071533\n",
      "Loss at step 179500 : 3.5738725662231445\n",
      "Loss at step 179550 : 2.545375347137451\n",
      "Loss at step 179600 : 2.7256507873535156\n",
      "Loss at step 179650 : 3.669328212738037\n",
      "Loss at step 179700 : 2.988304615020752\n",
      "Loss at step 179750 : 3.446657180786133\n",
      "Loss at step 179800 : 3.0300657749176025\n",
      "Loss at step 179850 : 2.232085704803467\n",
      "Loss at step 179900 : 2.8281779289245605\n",
      "Loss at step 179950 : 2.364590644836426\n",
      "Loss at step 180000 : 3.1327016353607178\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 180050 : 1.8919414281845093\n",
      "Loss at step 180100 : 2.871903896331787\n",
      "Loss at step 180150 : 3.0822713375091553\n",
      "Loss at step 180200 : 3.3790454864501953\n",
      "Loss at step 180250 : 3.4520578384399414\n",
      "Loss at step 180300 : 3.207468271255493\n",
      "Loss at step 180350 : 2.819108486175537\n",
      "Loss at step 180400 : 3.8399178981781006\n",
      "Loss at step 180450 : 2.6567373275756836\n",
      "Loss at step 180500 : 2.1077382564544678\n",
      "Loss at step 180550 : 2.739377498626709\n",
      "Loss at step 180600 : 2.9153473377227783\n",
      "Loss at step 180650 : 3.5511624813079834\n",
      "Loss at step 180700 : 1.5288314819335938\n",
      "Loss at step 180750 : 4.2335662841796875\n",
      "Loss at step 180800 : 2.7513556480407715\n",
      "Loss at step 180850 : 2.8526408672332764\n",
      "Loss at step 180900 : 3.186159610748291\n",
      "Loss at step 180950 : 2.9079413414001465\n",
      "Loss at step 181000 : 2.9092538356781006\n",
      "Loss at step 181050 : 2.627007007598877\n",
      "Loss at step 181100 : 2.6304893493652344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 181150 : 2.585817337036133\n",
      "Loss at step 181200 : 2.3295116424560547\n",
      "Loss at step 181250 : 2.430349349975586\n",
      "Loss at step 181300 : 2.091128349304199\n",
      "Loss at step 181350 : 2.454464912414551\n",
      "Loss at step 181400 : 2.41652250289917\n",
      "Loss at step 181450 : 2.7743120193481445\n",
      "Loss at step 181500 : 3.350148916244507\n",
      "Loss at step 181550 : 3.999124526977539\n",
      "Loss at step 181600 : 3.0804367065429688\n",
      "Loss at step 181650 : 3.120558738708496\n",
      "Loss at step 181700 : 3.770890712738037\n",
      "Loss at step 181750 : 3.6661205291748047\n",
      "Loss at step 181800 : 2.3058876991271973\n",
      "Loss at step 181850 : 3.052584171295166\n",
      "Loss at step 181900 : 3.568582534790039\n",
      "Loss at step 181950 : 3.1105847358703613\n",
      "Loss at step 182000 : 3.1838536262512207\n",
      "Loss at step 182050 : 3.504340887069702\n",
      "Loss at step 182100 : 2.2642781734466553\n",
      "Loss at step 182150 : 3.7928175926208496\n",
      "Loss at step 182200 : 3.5463552474975586\n",
      "Loss at step 182250 : 3.6363449096679688\n",
      "Loss at step 182300 : 3.0566935539245605\n",
      "Loss at step 182350 : 3.366199016571045\n",
      "Loss at step 182400 : 2.2584047317504883\n",
      "Loss at step 182450 : 2.4058756828308105\n",
      "Loss at step 182500 : 2.726388931274414\n",
      "Loss at step 182550 : 3.0056135654449463\n",
      "Loss at step 182600 : 3.0657036304473877\n",
      "Loss at step 182650 : 3.3705029487609863\n",
      "Loss at step 182700 : 3.4506235122680664\n",
      "Loss at step 182750 : 2.386268377304077\n",
      "Loss at step 182800 : 4.037389755249023\n",
      "Loss at step 182850 : 3.1648285388946533\n",
      "Loss at step 182900 : 2.927807569503784\n",
      "Loss at step 182950 : 2.73026180267334\n",
      "Loss at step 183000 : 2.3882369995117188\n",
      "Loss at step 183050 : 2.7349092960357666\n",
      "Loss at step 183100 : 2.817347288131714\n",
      "Loss at step 183150 : 2.185659885406494\n",
      "Loss at step 183200 : 2.2634456157684326\n",
      "Loss at step 183250 : 3.095545530319214\n",
      "Loss at step 183300 : 2.7698512077331543\n",
      "Loss at step 183350 : 2.4560675621032715\n",
      "Loss at step 183400 : 2.44132924079895\n",
      "Loss at step 183450 : 3.5331296920776367\n",
      "Loss at step 183500 : 5.006540775299072\n",
      "Loss at step 183550 : 2.5453715324401855\n",
      "Loss at step 183600 : 2.8425798416137695\n",
      "Loss at step 183650 : 2.1996564865112305\n",
      "Loss at step 183700 : 2.7137198448181152\n",
      "Loss at step 183750 : 2.25386905670166\n",
      "Loss at step 183800 : 3.015162944793701\n",
      "Loss at step 183850 : 2.838087320327759\n",
      "Loss at step 183900 : 2.957390785217285\n",
      "Loss at step 183950 : 3.0275793075561523\n",
      "Loss at step 184000 : 2.17820405960083\n",
      "Loss at step 184050 : 4.265689373016357\n",
      "Loss at step 184100 : 2.5808560848236084\n",
      "Loss at step 184150 : 2.3895466327667236\n",
      "Loss at step 184200 : 3.2450063228607178\n",
      "Loss at step 184250 : 3.132185459136963\n",
      "Loss at step 184300 : 3.857412338256836\n",
      "Loss at step 184350 : 2.833545446395874\n",
      "Loss at step 184400 : 4.641732215881348\n",
      "Loss at step 184450 : 2.9056079387664795\n",
      "Loss at step 184500 : 2.6422646045684814\n",
      "Loss at step 184550 : 2.753904342651367\n",
      "Loss at step 184600 : 3.4894392490386963\n",
      "Loss at step 184650 : 2.6249327659606934\n",
      "Loss at step 184700 : 2.666660785675049\n",
      "Loss at step 184750 : 2.4939002990722656\n",
      "Loss at step 184800 : 3.137221336364746\n",
      "Loss at step 184850 : 3.515028476715088\n",
      "Loss at step 184900 : 3.1733720302581787\n",
      "Loss at step 184950 : 3.2352209091186523\n",
      "Loss at step 185000 : 2.6600699424743652\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 185050 : 3.058070182800293\n",
      "Loss at step 185100 : 2.9185843467712402\n",
      "Loss at step 185150 : 3.431196689605713\n",
      "Loss at step 185200 : 4.102041721343994\n",
      "Loss at step 185250 : 2.6303277015686035\n",
      "Loss at step 185300 : 3.224705457687378\n",
      "Loss at step 185350 : 4.248044013977051\n",
      "Loss at step 185400 : 2.6069464683532715\n",
      "Loss at step 185450 : 2.9590611457824707\n",
      "Loss at step 185500 : 3.061408042907715\n",
      "Loss at step 185550 : 2.681624412536621\n",
      "Loss at step 185600 : 2.214195489883423\n",
      "Loss at step 185650 : 3.2888224124908447\n",
      "Loss at step 185700 : 1.5902793407440186\n",
      "Loss at step 185750 : 3.279329538345337\n",
      "Loss at step 185800 : 3.8134548664093018\n",
      "Loss at step 185850 : 3.52236270904541\n",
      "Loss at step 185900 : 2.8154220581054688\n",
      "Loss at step 185950 : 2.300098419189453\n",
      "Loss at step 186000 : 2.978917360305786\n",
      "Loss at step 186050 : 3.0745418071746826\n",
      "Loss at step 186100 : 3.273221731185913\n",
      "Loss at step 186150 : 2.5966544151306152\n",
      "Loss at step 186200 : 2.7689309120178223\n",
      "Loss at step 186250 : 2.819774866104126\n",
      "Loss at step 186300 : 3.858335494995117\n",
      "Loss at step 186350 : 3.421095848083496\n",
      "Loss at step 186400 : 2.6518876552581787\n",
      "Loss at step 186450 : 2.5847911834716797\n",
      "Loss at step 186500 : 4.654971122741699\n",
      "Loss at step 186550 : 2.785644054412842\n",
      "Loss at step 186600 : 3.6858887672424316\n",
      "Loss at step 186650 : 3.617823600769043\n",
      "Loss at step 186700 : 2.252593517303467\n",
      "Loss at step 186750 : 2.6772561073303223\n",
      "Loss at step 186800 : 3.483281135559082\n",
      "Loss at step 186850 : 2.4078915119171143\n",
      "Loss at step 186900 : 2.085887908935547\n",
      "Loss at step 186950 : 3.658071756362915\n",
      "Loss at step 187000 : 3.1268365383148193\n",
      "Loss at step 187050 : 3.4183433055877686\n",
      "Loss at step 187100 : 1.900597095489502\n",
      "Loss at step 187150 : 3.2385430335998535\n",
      "Loss at step 187200 : 2.473846435546875\n",
      "Loss at step 187250 : 3.8324897289276123\n",
      "Loss at step 187300 : 2.8806495666503906\n",
      "Loss at step 187350 : 2.6454830169677734\n",
      "Loss at step 187400 : 2.4033899307250977\n",
      "Loss at step 187450 : 3.500223159790039\n",
      "Loss at step 187500 : 2.5883986949920654\n",
      "Loss at step 187550 : 2.500723123550415\n",
      "Loss at step 187600 : 2.597838878631592\n",
      "Loss at step 187650 : 3.2972865104675293\n",
      "Loss at step 187700 : 3.0774128437042236\n",
      "Loss at step 187750 : 2.476260185241699\n",
      "Loss at step 187800 : 2.770792007446289\n",
      "Loss at step 187850 : 2.4978017807006836\n",
      "Loss at step 187900 : 4.024783134460449\n",
      "Loss at step 187950 : 3.1220884323120117\n",
      "Loss at step 188000 : 2.830860137939453\n",
      "Loss at step 188050 : 2.6580090522766113\n",
      "Loss at step 188100 : 2.680023670196533\n",
      "Loss at step 188150 : 2.10593581199646\n",
      "Loss at step 188200 : 2.906186580657959\n",
      "Loss at step 188250 : 3.5359981060028076\n",
      "Loss at step 188300 : 2.823859691619873\n",
      "Loss at step 188350 : 2.6274397373199463\n",
      "Loss at step 188400 : 2.5693295001983643\n",
      "Loss at step 188450 : 2.2658352851867676\n",
      "Loss at step 188500 : 3.088932991027832\n",
      "Loss at step 188550 : 3.3310022354125977\n",
      "Loss at step 188600 : 2.8580846786499023\n",
      "Loss at step 188650 : 3.207444667816162\n",
      "Loss at step 188700 : 2.268913745880127\n",
      "Loss at step 188750 : 2.2211246490478516\n",
      "Loss at step 188800 : 3.2536513805389404\n",
      "Loss at step 188850 : 2.448622703552246\n",
      "Loss at step 188900 : 2.8176660537719727\n",
      "Loss at step 188950 : 2.637451410293579\n",
      "Loss at step 189000 : 3.647213935852051\n",
      "Loss at step 189050 : 3.320301055908203\n",
      "Loss at step 189100 : 2.927366018295288\n",
      "Loss at step 189150 : 3.017550468444824\n",
      "Loss at step 189200 : 2.8026962280273438\n",
      "Loss at step 189250 : 2.972886085510254\n",
      "Loss at step 189300 : 3.5746281147003174\n",
      "Loss at step 189350 : 2.918610095977783\n",
      "Loss at step 189400 : 3.6043646335601807\n",
      "Loss at step 189450 : 1.7520462274551392\n",
      "Loss at step 189500 : 2.3755877017974854\n",
      "Loss at step 189550 : 2.544771671295166\n",
      "Loss at step 189600 : 4.124553203582764\n",
      "Loss at step 189650 : 3.5702743530273438\n",
      "Loss at step 189700 : 2.800898790359497\n",
      "Loss at step 189750 : 1.9278905391693115\n",
      "Loss at step 189800 : 2.3622097969055176\n",
      "Loss at step 189850 : 3.2060396671295166\n",
      "Loss at step 189900 : 3.6484780311584473\n",
      "Loss at step 189950 : 2.9743237495422363\n",
      "Loss at step 190000 : 2.773664951324463\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 190050 : 3.4357147216796875\n",
      "Loss at step 190100 : 1.83039128780365\n",
      "Loss at step 190150 : 3.575221538543701\n",
      "Loss at step 190200 : 3.0421972274780273\n",
      "Loss at step 190250 : 2.6529839038848877\n",
      "Loss at step 190300 : 2.8612523078918457\n",
      "Loss at step 190350 : 2.809826135635376\n",
      "Loss at step 190400 : 3.0876691341400146\n",
      "Loss at step 190450 : 3.0990548133850098\n",
      "Loss at step 190500 : 3.336799383163452\n",
      "Loss at step 190550 : 2.760427474975586\n",
      "Loss at step 190600 : 3.204167366027832\n",
      "Loss at step 190650 : 2.8181262016296387\n",
      "Loss at step 190700 : 3.9046783447265625\n",
      "Loss at step 190750 : 3.3070478439331055\n",
      "Loss at step 190800 : 2.8177614212036133\n",
      "Loss at step 190850 : 3.0169589519500732\n",
      "Loss at step 190900 : 3.211078643798828\n",
      "Loss at step 190950 : 2.4265642166137695\n",
      "Loss at step 191000 : 3.499279737472534\n",
      "Loss at step 191050 : 2.478121757507324\n",
      "Loss at step 191100 : 2.7809319496154785\n",
      "Loss at step 191150 : 2.956204414367676\n",
      "Loss at step 191200 : 2.1574721336364746\n",
      "Loss at step 191250 : 3.6538004875183105\n",
      "Loss at step 191300 : 4.708128929138184\n",
      "Loss at step 191350 : 2.40525484085083\n",
      "Loss at step 191400 : 2.8938612937927246\n",
      "Loss at step 191450 : 3.7243199348449707\n",
      "Loss at step 191500 : 3.1287238597869873\n",
      "Loss at step 191550 : 2.8415870666503906\n",
      "Loss at step 191600 : 2.677011013031006\n",
      "Loss at step 191650 : 2.741953134536743\n",
      "Loss at step 191700 : 2.934443235397339\n",
      "Loss at step 191750 : 4.27773904800415\n",
      "Loss at step 191800 : 3.240663528442383\n",
      "Loss at step 191850 : 2.9609181880950928\n",
      "Loss at step 191900 : 2.456017255783081\n",
      "Loss at step 191950 : 3.555424690246582\n",
      "Loss at step 192000 : 2.990248918533325\n",
      "Loss at step 192050 : 2.8715434074401855\n",
      "Loss at step 192100 : 3.788886070251465\n",
      "Loss at step 192150 : 2.64693546295166\n",
      "Loss at step 192200 : 2.228322982788086\n",
      "Loss at step 192250 : 3.105084180831909\n",
      "Loss at step 192300 : 3.0168254375457764\n",
      "Loss at step 192350 : 2.693955183029175\n",
      "Loss at step 192400 : 1.7973099946975708\n",
      "Loss at step 192450 : 2.6401195526123047\n",
      "Loss at step 192500 : 2.5970823764801025\n",
      "Loss at step 192550 : 3.126883029937744\n",
      "Loss at step 192600 : 3.149899959564209\n",
      "Loss at step 192650 : 2.9654345512390137\n",
      "Loss at step 192700 : 2.497953414916992\n",
      "Loss at step 192750 : 2.899191379547119\n",
      "Loss at step 192800 : 3.626157760620117\n",
      "Loss at step 192850 : 2.8879482746124268\n",
      "Loss at step 192900 : 3.355332851409912\n",
      "Loss at step 192950 : 3.4279913902282715\n",
      "Loss at step 193000 : 2.979832172393799\n",
      "Loss at step 193050 : 2.5381689071655273\n",
      "Loss at step 193100 : 3.546830892562866\n",
      "Loss at step 193150 : 3.085874557495117\n",
      "Loss at step 193200 : 2.879854917526245\n",
      "Loss at step 193250 : 2.3537442684173584\n",
      "Loss at step 193300 : 3.4155256748199463\n",
      "Loss at step 193350 : 2.901986598968506\n",
      "Loss at step 193400 : 3.801920175552368\n",
      "Loss at step 193450 : 2.6778550148010254\n",
      "Loss at step 193500 : 3.0542774200439453\n",
      "Loss at step 193550 : 2.8262736797332764\n",
      "Loss at step 193600 : 2.5875191688537598\n",
      "Loss at step 193650 : 2.485060930252075\n",
      "Loss at step 193700 : 3.4501047134399414\n",
      "Loss at step 193750 : 3.99352765083313\n",
      "Loss at step 193800 : 2.373319625854492\n",
      "Loss at step 193850 : 2.472646713256836\n",
      "Loss at step 193900 : 2.5798444747924805\n",
      "Loss at step 193950 : 3.0933938026428223\n",
      "Loss at step 194000 : 2.9288482666015625\n",
      "Loss at step 194050 : 2.7172486782073975\n",
      "Loss at step 194100 : 2.7645974159240723\n",
      "Loss at step 194150 : 3.7043235301971436\n",
      "Loss at step 194200 : 3.1849842071533203\n",
      "Loss at step 194250 : 3.032313346862793\n",
      "Loss at step 194300 : 3.044083595275879\n",
      "Loss at step 194350 : 2.6246867179870605\n",
      "Loss at step 194400 : 2.5282347202301025\n",
      "Loss at step 194450 : 2.592602014541626\n",
      "Loss at step 194500 : 3.3930883407592773\n",
      "Loss at step 194550 : 2.6618428230285645\n",
      "Loss at step 194600 : 3.2759289741516113\n",
      "Loss at step 194650 : 2.203507900238037\n",
      "Loss at step 194700 : 3.840874433517456\n",
      "Loss at step 194750 : 3.4202702045440674\n",
      "Loss at step 194800 : 2.8187737464904785\n",
      "Loss at step 194850 : 4.382305145263672\n",
      "Loss at step 194900 : 2.9539618492126465\n",
      "Loss at step 194950 : 2.3168954849243164\n",
      "Loss at step 195000 : 4.189064025878906\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 195050 : 2.206930637359619\n",
      "Loss at step 195100 : 2.975163459777832\n",
      "Loss at step 195150 : 3.022333860397339\n",
      "Loss at step 195200 : 2.8960583209991455\n",
      "Loss at step 195250 : 2.854339838027954\n",
      "Loss at step 195300 : 3.448978900909424\n",
      "Loss at step 195350 : 3.319520950317383\n",
      "Loss at step 195400 : 3.3922436237335205\n",
      "Loss at step 195450 : 3.0940818786621094\n",
      "Loss at step 195500 : 2.985969066619873\n",
      "Loss at step 195550 : 1.673915982246399\n",
      "Loss at step 195600 : 2.678645372390747\n",
      "Loss at step 195650 : 2.47658109664917\n",
      "Loss at step 195700 : 2.924957752227783\n",
      "Loss at step 195750 : 3.8995609283447266\n",
      "Loss at step 195800 : 1.4874885082244873\n",
      "Loss at step 195850 : 2.623689651489258\n",
      "Loss at step 195900 : 2.4833569526672363\n",
      "Loss at step 195950 : 2.461167812347412\n",
      "Loss at step 196000 : 2.529233932495117\n",
      "Loss at step 196050 : 3.080687999725342\n",
      "Loss at step 196100 : 3.205874443054199\n",
      "Loss at step 196150 : 2.3840768337249756\n",
      "Loss at step 196200 : 3.275847911834717\n",
      "Loss at step 196250 : 2.783487319946289\n",
      "Loss at step 196300 : 3.697542190551758\n",
      "Loss at step 196350 : 2.978472948074341\n",
      "Loss at step 196400 : 2.5573198795318604\n",
      "Loss at step 196450 : 2.7139134407043457\n",
      "Loss at step 196500 : 2.7006306648254395\n",
      "Loss at step 196550 : 2.4881057739257812\n",
      "Loss at step 196600 : 1.9722418785095215\n",
      "Loss at step 196650 : 2.72487211227417\n",
      "Loss at step 196700 : 2.4978561401367188\n",
      "Loss at step 196750 : 3.075075149536133\n",
      "Loss at step 196800 : 3.6485705375671387\n",
      "Loss at step 196850 : 3.108128547668457\n",
      "Loss at step 196900 : 2.918583393096924\n",
      "Loss at step 196950 : 2.5669033527374268\n",
      "Loss at step 197000 : 2.182562828063965\n",
      "Loss at step 197050 : 2.4607770442962646\n",
      "Loss at step 197100 : 2.553558111190796\n",
      "Loss at step 197150 : 3.4394075870513916\n",
      "Loss at step 197200 : 2.9661765098571777\n",
      "Loss at step 197250 : 3.425743818283081\n",
      "Loss at step 197300 : 2.956655979156494\n",
      "Loss at step 197350 : 2.692467212677002\n",
      "Loss at step 197400 : 2.2615156173706055\n",
      "Loss at step 197450 : 1.759277105331421\n",
      "Loss at step 197500 : 3.309223175048828\n",
      "Loss at step 197550 : 2.2740349769592285\n",
      "Loss at step 197600 : 2.4300477504730225\n",
      "Loss at step 197650 : 2.7327771186828613\n",
      "Loss at step 197700 : 4.071828365325928\n",
      "Loss at step 197750 : 3.61112380027771\n",
      "Loss at step 197800 : 3.516420841217041\n",
      "Loss at step 197850 : 3.230130672454834\n",
      "Loss at step 197900 : 2.893115997314453\n",
      "Loss at step 197950 : 3.6184277534484863\n",
      "Loss at step 198000 : 2.684860944747925\n",
      "Loss at step 198050 : 2.3787741661071777\n",
      "Loss at step 198100 : 2.277568817138672\n",
      "Loss at step 198150 : 2.600510358810425\n",
      "Loss at step 198200 : 2.802483558654785\n",
      "Loss at step 198250 : 2.7371621131896973\n",
      "Loss at step 198300 : 2.4892735481262207\n",
      "Loss at step 198350 : 1.725840449333191\n",
      "Loss at step 198400 : 2.5294806957244873\n",
      "Loss at step 198450 : 2.7992658615112305\n",
      "Loss at step 198500 : 3.0924246311187744\n",
      "Loss at step 198550 : 3.2642359733581543\n",
      "Loss at step 198600 : 2.83807635307312\n",
      "Loss at step 198650 : 1.95344877243042\n",
      "Loss at step 198700 : 2.121778964996338\n",
      "Loss at step 198750 : 2.6395955085754395\n",
      "Loss at step 198800 : 2.830249786376953\n",
      "Loss at step 198850 : 2.050602674484253\n",
      "Loss at step 198900 : 3.0396106243133545\n",
      "Loss at step 198950 : 2.7863404750823975\n",
      "Loss at step 199000 : 3.0502567291259766\n",
      "Loss at step 199050 : 4.0189924240112305\n",
      "Loss at step 199100 : 2.383000373840332\n",
      "Loss at step 199150 : 2.330720901489258\n",
      "Loss at step 199200 : 3.446368455886841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 199250 : 3.577129364013672\n",
      "Loss at step 199300 : 3.6276893615722656\n",
      "Loss at step 199350 : 2.6090211868286133\n",
      "Loss at step 199400 : 2.7248284816741943\n",
      "Loss at step 199450 : 3.24519681930542\n",
      "Loss at step 199500 : 2.1846117973327637\n",
      "Loss at step 199550 : 2.3045291900634766\n",
      "Loss at step 199600 : 2.9513297080993652\n",
      "Loss at step 199650 : 3.098628520965576\n",
      "Loss at step 199700 : 3.2195024490356445\n",
      "Loss at step 199750 : 2.6985244750976562\n",
      "Loss at step 199800 : 3.075005292892456\n",
      "Loss at step 199850 : 2.0871458053588867\n",
      "Loss at step 199900 : 3.2238855361938477\n",
      "Loss at step 199950 : 2.687626838684082\n",
      "Loss at step 200000 : 2.971001386642456\n",
      "Nearest to tuna: parmesan, metal, eye, position, mat,\n",
      "Nearest to rice: thigh, lime, mandu, simmer, spinach,\n",
      "Nearest to sushi: piece, sashimi, sesame, rare, bundt,\n",
      "Nearest to roll: meal, sheet, ricotta, salt, oil,\n",
      "Nearest to sashimi: fryer, seaweed, sushi, ring, gyoza,\n",
      "Nearest to steak: maple, grate, eye, meat, marinade,\n",
      "Nearest to grill: soy, spring, teriyaki, steak, cider,\n",
      "Nearest to sauce: appetizer, jack, wasabi, oregano, soy,\n",
      "Nearest to cream: chocolate, cherry, extract, cake, oven,\n",
      "Nearest to cheesecake: cashew, nutmeg, flake, tin, chill,\n",
      "Nearest to pizza: flake, white, baguette, mixture, run,\n",
      "Nearest to lasagna: fry, noodle, rare, lemon, teriyaki,\n",
      "Nearest to hamburger: herb, meat, pine, tomato, ½,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/10/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "\n",
    "doc_embeddings_test_initial_tensor = tf.random_uniform([len(preprocessed_texts_test), doc_embedding_size], -1.0, 1.0)\n",
    "doc_embeddings_all_initial_tensor=tf.concat([doc_embeddings, doc_embeddings_test_initial_tensor],0,\"concat\")\n",
    "doc_embeddings_all=tf.Variable(doc_embeddings_all_initial_tensor,name = \"doc_embeddings_all\")\n",
    "sess.run(tf.initialize_variables([doc_embeddings_all]))\n",
    "\n",
    "print(doc_embeddings.shape)\n",
    "print(doc_embeddings_all.shape)\n",
    "\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[None, 2]) #2:  1 for word index and 1 for doc index\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embed= tf.nn.embedding_lookup(embeddings, x_inputs[:, 0])\n",
    "    \n",
    "doc_indices = tf.slice(x_inputs, [0,1],[batch_size,1])\n",
    "doc_embed = tf.nn.embedding_lookup(doc_embeddings_all,doc_indices)\n",
    "final_embed = tf.concat([embed, tf.squeeze(doc_embed)],1)\n",
    "\n",
    "logits = tf.matmul(final_embed, tf.transpose(decoder_weights)) + decoder_biases\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_target))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=model_learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "#cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True, name=\"cosine_similarity\")\n",
    "\n",
    "with tf.name_scope(\"performance\"):\n",
    "    loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "    loss_summary = tf.summary.scalar('loss', loss_ph)\n",
    "performance_summaries = tf.summary.merge([loss_summary])\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summ_writer = tf.summary.FileWriter(summaries_folder_name, sess.graph)\n",
    "\n",
    "sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "\n",
    "\n",
    "\n",
    "print('Starting Training')\n",
    "\n",
    "\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = create_batch_data(text_data)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        summ = sess.run(performance_summaries, feed_dict={loss_ph:loss_val})\n",
    "        summ_writer.add_summary(summ, i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "        \n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    #save dictionary + embeddings\n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        #save vocabulary\n",
    "        with open(os.path.join(models_folder_name_test,'doc2vec_recipes_dict_words_integers.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        #save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),models_folder_name_test,'doc2vec_recipes_checkpoint.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))\n",
    "        \n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
