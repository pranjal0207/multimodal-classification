{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steak': 1, 'sashimi': 0, 'tiramisu': 3, 'sushi': 2}\n"
     ]
    }
   ],
   "source": [
    "models_folder_name = os.path.join(os.getcwd(),'models')\n",
    "summaries_folder_name = os.path.join(os.getcwd(),'summaries')\n",
    "path_to_preprocessed_texts = os.path.join(os.getcwd(),\n",
    "                                          'texts','preprocessed_texts_for_doc2vec.pkl')\n",
    "\n",
    "df_preprocessed_texts = pd.read_pickle(path_to_preprocessed_texts)\n",
    "\n",
    "preprocessed_texts = df_preprocessed_texts.preprocessed_texts.values.tolist()\n",
    "labels = df_preprocessed_texts['labels'].values.tolist()\n",
    "\n",
    "unique_labels=sorted(set(labels))\n",
    "number_categories=len(unique_labels)\n",
    "categories_indices=np.linspace(0,number_categories-1,number_categories,dtype=int)\n",
    "labels2integers=dict(zip(unique_labels,categories_indices))\n",
    "\n",
    "print(labels2integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "generations = 100000\n",
    "model_learning_rate = 0.001\n",
    "\n",
    "embedding_size = 24   #word embedding size\n",
    "doc_embedding_size = 12  #document embedding size\n",
    "concatenated_size = embedding_size + doc_embedding_size\n",
    "\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(preprocessed_texts):\n",
    "    words=[w for words_in_recipe in preprocessed_texts for w in words_in_recipe]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words))\n",
    "    count=sorted(count)\n",
    "    word_dict = {}\n",
    "    for word in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return (word_dict)\n",
    "\n",
    "#replace each word in texts with integer value\n",
    "def text_to_numbers(preprocessed_texts, word_dict):\n",
    "    data = []\n",
    "    for prepr_text in preprocessed_texts:\n",
    "        text_data = []\n",
    "        for word in prepr_text:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            text_data.append(word_ix)\n",
    "        data.append(text_data)\n",
    "    return (data)\n",
    "\n",
    "\n",
    "def create_batch_data(text_with_words_conv_to_numbers, batch_size=batch_size):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    rand_text_ix = int(np.random.choice(len(text_with_words_conv_to_numbers), size=1))\n",
    "    rand_text = text_with_words_conv_to_numbers[rand_text_ix]\n",
    "    word_to_predict_label=np.random.choice(list(set(rand_text)), size=1,replace=False)\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        item_in_batch=[]        \n",
    "        \n",
    "        label_words=np.random.choice(rand_text, size=1,replace=False)\n",
    "\n",
    "        item_in_batch.extend(word_to_predict_label)\n",
    "        item_in_batch.append(rand_text_ix)     \n",
    "        label_data.extend(label_words)\n",
    "        batch_data.append(item_in_batch)\n",
    "\n",
    "        \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array(label_data))\n",
    "\n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'press': 86, 'cutting': 35, 'pressure': 87, 'mixture': 70, 'shrimp': 103, 'angle': 1, 'carrot': 18, 'tomato': 129, 'chive': 21, 'position': 82, 'avocado': 3, 'air': 0, 'salt': 95, 'soy': 108, 'stick': 117, 'truffle': 134, 'sesame': 101, 'torch': 132, 'vegetable': 137, 'cheese': 19, 'towel': 133, 'brush': 15, 'sheet': 102, 'oil': 74, 'spread': 112, 'seed': 100, 'bamboo': 6, 'strawberry': 119, 'blade': 11, 'raspberry': 89, 'mushroom': 71, 'leg': 58, 'cucumber': 32, 'butter': 16, 'matchstick': 64, 'yolk': 147, 'stone': 118, 'mascarpone': 62, 'mixer': 69, 'beat': 8, 'peak': 78, 'skewer': 106, 'sushi': 123, 'sea': 99, 'sauce': 97, 'strip': 120, 'cling': 24, 'starch': 115, 'guacamole': 53, 'cream': 31, 'lime': 59, 'rice': 90, 'tempura': 124, 'whisk': 143, 'espresso': 40, 'powder': 84, 'sirloin': 105, 'meat': 66, 'water': 141, 'tuna': 135, 'mat': 63, 'filling': 42, 'slice': 107, 'cut': 34, 'nori': 73, 'wine': 145, 'wasabi': 140, 'sprinkle': 114, 'pinch': 81, 'quantity': 88, 'fryer': 46, 'space': 109, 'steak': 116, 'chicken': 20, 'coriander': 29, 'speed': 110, 'wafer': 139, 'breast': 14, 'tobikko': 128, 'egg': 39, 'cake': 17, 'coffee': 26, 'confectioner': 28, 'berry': 9, 'noodle': 72, 'spring': 113, 'vinegar': 138, 'marinade': 61, 'roll': 92, 'chocolate': 22, 'pour': 83, 'cone': 27, 'mayonnaise': 65, 'medium': 67, 'spicy': 111, 'surface': 122, 'cocoa': 25, 'pepper': 79, 'teriyaki': 125, 'block': 12, 'worcestershire': 146, 'salmon': 94, 'bit': 10, 'bottom': 13, 'sashimi': 96, 'juice': 54, 'topping': 131, 'white': 144, 'leaf': 57, 'piece': 80, 'bag': 4, 'saucepan': 98, 'grate': 50, 'crab': 30, 'row': 93, 'daikon': 36, 'chopstick': 23, 'paper': 76, 'sieve': 104, 'metal': 68, 'drain': 38, 'curl': 33, 'preheat': 85, 'flesh': 45, 'garnish': 47, 'grill': 51, 'ginger': 48, 'part': 77, 'ladyfinger': 55, 'zip': 148, 'sugar': 121, 'thickness': 126, 'ball': 5, 'fish': 44, 'finger': 43, 'ground': 52, 'roe': 91, 'vanilla': 136, 'liqueur': 60, 'tongs': 130, 'whip': 142, 'thumb': 127, 'onion': 75, 'dipping': 37, 'filet': 41, 'batter': 7, 'asparagus': 2, 'layer': 56, 'grain': 49}\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "word_dictionary=build_dictionary(preprocessed_texts)\n",
    "vocabulary_size=len(word_dictionary)\n",
    "print(word_dictionary)\n",
    "print(vocabulary_size)\n",
    "\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94, 3, 75, 140, 100, 90, 72, 80, 94, 95, 94, 141, 44, 53, 75, 129, 95, 95, 95, 95, 95, 95, 95, 95, 34, 3, 3, 3, 3, 80, 75, 70, 3, 70, 53, 92, 59, 54, 59, 59, 54, 59, 54, 3, 54, 59, 53, 70, 140, 140, 10, 140, 53, 10, 10, 101, 100, 108, 97, 129, 129, 129, 3, 29, 57, 57, 70, 29, 53, 24, 94, 95, 94, 94, 44, 96, 34, 107, 24, 10, 115, 24, 107, 94, 115, 24, 115, 94, 94, 115, 94, 94, 94, 46, 107, 46, 94, 74, 94, 94, 117, 46, 94, 94, 94, 94, 94, 94, 46, 74, 94, 74, 94, 47, 94, 90, 72, 72, 94, 53, 29, 90, 72, 53], [135, 140, 108, 97, 48, 49, 82, 43, 49, 107], [96, 94, 41, 101, 100, 74, 101, 100, 97, 48, 41, 101, 100, 74, 122, 101, 100, 74, 43, 94, 41, 101, 100, 122, 43, 41, 41, 44, 101, 100, 74, 117, 41, 41, 44, 82, 41, 43, 41, 41, 34, 41, 97, 122, 11, 94, 107, 43, 82, 94, 97, 48, 94, 96, 107, 97], [123, 41, 12, 123, 41, 12, 29, 57, 101, 100, 74, 135, 135, 41, 107, 107, 41, 34, 135, 107, 12, 12, 135, 100, 74, 43, 74, 135, 29, 41, 122, 41, 12, 11, 44, 135, 12, 44, 44, 82, 41, 12, 1, 34, 1, 11, 107, 44, 107, 135, 12, 11, 34, 107, 47, 47, 43, 18, 5, 82, 5, 107, 123, 18, 5, 11, 82, 107, 80, 107, 107, 94, 43, 43, 123, 5, 82, 5, 32, 18, 5, 135, 107, 123, 107, 1, 82, 1, 107, 47, 107, 43, 107, 135, 107, 107, 47, 107, 57, 32, 5, 135, 107, 32, 47, 3, 107, 3, 43, 3, 27, 82, 107, 123, 27, 5, 48, 135, 107, 120, 96, 107, 108, 97, 140], [29, 57, 101, 100, 74, 123, 102, 29, 29, 135, 74, 29, 80, 123, 135, 74, 135, 29, 135, 120, 29, 120, 135, 135, 101, 100, 74, 120, 29, 135, 44, 135, 135, 96, 44, 135, 102, 102, 29, 135, 102, 73, 92, 135, 73, 92, 92, 92, 92, 80, 34, 57, 18, 135, 108, 97, 37], [123, 41, 49, 141, 95, 121, 74, 41, 123, 41, 49, 94, 41, 141, 95, 121, 41, 44, 94, 41, 94, 41, 4, 74, 4, 4, 0, 74, 4, 141, 44, 4, 4, 122, 94, 4, 44, 120, 94, 82, 120, 41, 120, 96, 96, 3, 108, 0, 108, 97, 0, 108, 97, 0, 82, 107, 3, 108, 97, 43, 94, 107, 3, 94], [123, 29, 135, 96, 80, 135, 44, 135, 44, 135, 135, 12, 29, 135, 29, 135, 117, 74, 107], [135, 3, 34, 107, 74, 59, 54, 59, 75, 108, 97, 135, 107, 107, 3, 74, 59, 54, 3, 135, 75, 95, 108, 97, 37, 59], [135, 80, 80, 48, 108, 97, 97, 37, 97, 108, 97, 108, 97, 123, 108, 97, 97, 36, 102, 102, 107, 117, 141, 36, 48, 27, 27, 48, 135, 135, 12, 107, 49, 49, 12, 135, 120, 135, 12, 135, 44, 36, 57, 57, 75, 32, 48, 108, 97, 44, 135, 37, 97, 36, 108, 97, 96], [123, 90, 90, 138, 138, 121, 95, 94, 3, 80, 73, 120, 97, 108, 140, 101, 90, 90, 141, 90, 90, 138, 121, 95, 121, 95, 90, 138, 70, 90, 94, 3, 73, 108, 97, 140, 97, 140, 140, 108, 97, 97], [138, 108, 97, 74, 52, 79, 146, 97, 75, 84, 95, 79, 116, 67, 138, 108, 97, 74, 52, 79, 146, 97, 75, 84, 95, 79, 116, 61, 66, 85, 51, 67, 74, 51, 50, 61], [75, 74, 138, 108, 97, 79, 116, 75, 74, 138, 108, 97, 95, 79, 116, 4, 83, 61, 116, 51, 51, 50, 74, 61, 116, 51], [116, 95, 52, 79, 74, 116, 95, 79, 51, 85, 51, 116, 51, 116, 74, 51, 116, 116], [48, 75, 108, 97, 74, 66, 121, 116, 48, 75, 97, 74, 146, 97, 66, 121, 61, 4, 116, 61, 51, 67, 51, 116, 51, 67], [16, 79, 85, 51, 16, 84, 116, 95, 79, 116, 16], [108, 97, 138, 52, 48, 84, 74, 108, 97, 138, 48, 84, 74, 116, 116, 83, 61, 116, 85, 51, 50, 74, 116, 51, 61, 116], [74, 146, 97, 67, 75, 79, 97, 74, 146, 97, 108, 97, 75, 95, 79, 116, 97, 116, 83, 61, 116, 51, 67, 116, 61, 61, 74, 116, 51, 116], [116, 121, 79, 84, 85, 51, 116, 116, 97, 121, 83, 97, 116, 95, 79, 84, 116, 95, 79, 84, 116, 61, 83, 61, 74, 51, 50, 116, 116, 61], [138, 79, 95, 74, 116, 16, 21, 79, 138, 79, 95, 74, 116, 4, 61, 4, 51, 67, 74, 50, 116, 61, 61, 116, 51, 16, 21, 79, 116, 51, 67, 116, 16], [16, 21, 79, 95, 75, 84, 52, 79, 116, 74, 85, 51, 16, 21, 79, 95, 75, 84, 79, 116, 74, 66, 74, 51, 50, 66, 51, 51, 51, 16], [123, 90, 138, 121, 95, 90, 141, 102, 42, 32, 32, 64, 64, 79, 64, 75, 64, 97, 80, 36, 64, 36, 3, 54, 6, 123, 63, 90, 138, 121, 95, 90, 104, 38, 90, 141, 67, 98, 133, 98, 133, 141, 90, 90, 70, 90, 76, 90, 90, 133, 92, 123, 63, 109, 102, 63, 43, 141, 86, 90, 73, 42, 90, 90, 63, 73, 90, 42, 63, 92, 63, 92, 63, 73, 10, 141, 123, 123, 92, 90, 42, 123, 92, 80, 108, 97], [108, 97, 90, 145, 138, 48, 100, 100, 74, 65, 134, 74, 97, 108, 97, 90, 138, 48, 70, 54, 97, 123, 92, 101, 100, 70, 101, 100, 74, 123, 92, 97, 131, 65, 74, 123, 92, 116, 105, 116, 43, 116, 43, 116, 126, 34, 56, 116, 126, 101, 100, 74, 105, 99, 95, 114, 122, 116, 68, 130, 41, 137, 101, 100, 74, 67, 2, 71, 2, 71, 99, 95, 130, 71, 74, 123, 92, 74, 123, 92, 102, 43, 123, 90, 102, 90, 102, 6, 63, 148, 4, 90, 63, 102, 43, 90, 63, 82, 102, 68, 130, 56, 71, 2, 102, 92, 63, 43, 43, 87, 6, 63, 102, 127, 43, 92, 87, 105, 105, 82, 45, 107, 126, 116, 80, 107, 123, 92, 123, 92, 43, 107, 105, 82, 92, 123, 92, 92, 43, 63, 123, 92, 43, 141, 11, 34, 123, 92, 34, 80, 123, 24, 80, 123, 92, 82, 80, 123, 1, 88, 82, 5, 48, 5, 140, 97, 123, 92, 107, 132, 97, 132, 122, 122, 107, 81, 113, 75, 134, 74, 65, 81, 101, 100, 107], [123, 94, 89, 36, 94, 91, 3, 3, 118, 3, 118, 118, 118, 3, 127, 3, 80, 3, 82, 3, 1, 3, 43, 107, 126, 11, 43, 3, 3, 107, 93, 24, 93, 107, 93, 107, 93, 123, 92, 92, 82, 80, 123, 94, 41, 93, 93, 24, 3, 41, 24, 87, 43, 123, 92, 123, 92, 80, 89, 82, 89, 104, 68, 89, 104, 54, 104, 54, 15, 15, 89, 122, 23, 3, 92, 80, 89, 15, 82, 107, 23, 94, 34, 80, 36, 88, 11, 88, 3, 92, 23, 94, 91, 107, 23, 87], [123, 90, 102, 32, 107, 3, 134, 42, 102, 123, 90, 73, 43, 90, 90, 102, 73, 122, 133, 90, 102, 41, 102, 32, 32, 102, 92, 123, 92, 6, 63, 127, 102, 92, 102, 42, 92, 6, 63, 148, 4, 92, 63, 92, 86, 43, 92, 92, 92, 107, 3, 123, 92, 109, 107, 92, 24, 43, 24, 141, 92, 92, 92, 34, 6, 63, 24, 131, 16, 107, 123, 107, 132, 134, 132, 122, 107, 107, 91, 107, 107, 77, 74, 77, 74, 10, 70, 123, 107, 82, 107, 123, 1, 109, 107, 137, 32, 18, 107, 80, 140], [14, 101, 74, 74, 99, 95, 125, 97, 102, 90, 20, 97, 123, 90, 74, 74, 80, 20, 14, 107, 80, 92, 80, 81, 99, 95, 81, 95, 20, 14, 20, 66, 20, 14, 106, 77, 54, 77, 20, 66, 20, 125, 97, 66, 20, 14, 125, 125, 20, 92, 92, 123, 92, 6, 63, 148, 4, 90, 63, 102, 73, 63, 141, 90, 138, 90, 123, 90, 122, 102, 90, 102, 90, 63, 80, 125, 20, 14, 102, 32, 20, 92, 10, 125, 97, 32, 20, 92, 123, 92, 6, 63, 42, 92, 87, 6, 63, 87, 63, 92, 92, 10, 92, 90, 3, 3, 107, 107, 3, 34, 3, 3, 10, 107, 3, 92, 123, 92, 3, 107, 107, 123, 92, 92, 102, 92, 6, 63, 4, 87, 92, 63, 24, 90, 11, 92, 80, 80, 92, 34, 35, 24, 123, 123, 48, 125, 97, 123, 80, 101, 100, 92, 125, 20, 123, 92], [30, 117, 90, 102, 107, 107, 101, 100, 30, 30, 117, 117, 92, 65, 30, 117, 65, 90, 102, 102, 123, 90, 102, 43, 90, 102, 114, 100, 90, 90, 90, 6, 63, 148, 4, 102, 63, 30, 70, 102, 107, 32, 107, 109, 102, 92, 63, 42, 42, 127, 92, 87, 63, 92, 63, 10, 92, 92, 63, 123, 92, 92, 24, 92, 92, 24, 63, 24, 92, 24, 92, 90, 92, 123, 80, 123, 92, 90, 11, 92, 92, 107, 123, 92, 11, 87, 123, 92, 92, 123, 80, 48, 140, 108, 97], [123, 90, 102, 2, 117, 91, 124, 7, 123, 102, 73, 4, 63, 123, 90, 102, 43, 90, 102, 117, 2, 102, 56, 2, 44, 123, 92, 2, 120, 96, 94, 123, 92, 102, 6, 63, 43, 102, 117, 92, 123, 123, 92, 123, 92, 124, 7, 7, 123, 92, 7, 123, 92, 90, 124, 7, 92, 92, 7, 92, 46, 94, 130, 76, 74, 123, 92, 123, 92, 141, 11, 90, 7, 34, 92, 80, 123, 123, 92, 123, 92, 123, 92, 140, 48, 97], [67, 90, 141, 80, 90, 138, 121, 95, 65, 79, 74, 95, 79, 102, 123, 90, 101, 100, 135, 113, 75, 90, 141, 141, 38, 90, 67, 98, 141, 70, 141, 90, 123, 90, 138, 121, 95, 98, 121, 70, 90, 90, 70, 90, 133, 143, 65, 74, 95, 79, 123, 63, 122, 102, 73, 63, 102, 63, 43, 90, 73, 56, 90, 101, 100, 73, 90, 73, 56, 32, 111, 65, 70, 32, 135, 114, 113, 75, 92, 123, 63, 80, 92], [90, 90, 138, 121, 95, 90, 141, 102, 42, 32, 32, 64, 3, 54, 108, 97, 6, 123, 63, 92, 90, 138, 121, 95, 90, 141, 38, 90, 141, 67, 98, 90, 133, 98, 133, 90, 141, 90, 90, 70, 90, 76, 102, 90, 90, 133, 92, 123, 63, 109, 102, 63, 43, 141, 86, 90, 73, 42, 90, 90, 63, 73, 90, 42, 63, 92, 63, 92, 63, 73, 10, 141, 123, 123, 76, 133, 90, 42, 123, 92, 80, 108, 97], [103, 124, 101, 100, 95, 74, 103, 111, 97, 65, 97, 90, 145, 138, 54, 103, 124, 143, 7, 143, 101, 100, 95, 54, 7, 43, 46, 74, 74, 98, 74, 103, 7, 46, 38, 76, 133, 102, 114, 95, 103, 46, 111, 97, 143, 65, 90, 145, 138, 54, 103, 111, 97], [90, 123, 90, 90, 145, 138, 121, 65, 65, 90, 145, 138, 108, 97, 123, 4, 102, 42, 120, 94, 135, 79, 3, 113, 75, 123, 140, 48, 108, 97, 92, 90, 102, 63, 141, 90, 56, 65, 56, 65, 90, 42, 65, 42, 135, 32, 92, 63, 90, 87, 92, 90, 15, 141, 92, 63, 92, 123, 107, 24, 123, 56, 94, 24, 56, 24, 90, 86, 86, 90, 44, 24, 123, 43, 24, 5, 131, 131, 80, 94, 5, 90, 131, 5, 24, 5], [102, 90, 123, 94, 44, 120, 97, 123, 56, 123, 90, 90, 90, 90, 123, 80, 94, 80, 44, 92, 44, 44, 123, 94, 123, 92, 41, 80, 120, 90, 3, 92, 92, 6, 63, 76, 123, 92, 80, 123, 92, 108, 97, 48], [90, 96, 94, 3, 18, 2, 36, 91, 34, 80, 36, 102, 34, 34, 133, 68, 106, 133, 106, 137, 106, 137, 35, 102, 34, 2, 141, 99, 95, 2, 141, 18, 67, 18, 34, 80, 80, 120, 122, 107, 107, 107, 120, 34, 107, 107, 120, 3, 3, 45, 3, 80, 96, 94, 34, 80, 94, 80, 34, 92, 80, 73, 63, 141, 138, 90, 73, 10, 140, 2, 10, 107, 18, 91, 92, 92, 10, 77, 90, 102, 92, 102, 102, 80, 92, 80, 92, 77, 80], [3, 73, 90, 90, 128, 91, 48, 140, 108, 97, 3, 100, 100, 100, 77, 3, 100, 45, 3, 45, 3, 107, 107, 100, 100, 77, 3, 100, 100, 3, 100, 3, 80, 96, 94, 107, 120, 94, 92, 92, 80, 92, 6, 63, 102, 73, 63, 90, 73, 102, 10, 140, 102, 43, 43, 102, 120, 102, 107, 3, 94, 92, 92, 123, 92, 128, 92, 128, 56, 122, 92, 128, 92, 128, 140, 92, 92, 90, 11, 34, 92, 123, 123, 35, 6, 63, 107, 92, 123, 10, 48, 140, 108, 97], [30, 16, 48, 3, 123, 90, 102, 30, 30, 58, 43, 30, 66, 58, 66, 30, 58, 80, 30, 66, 58, 99, 95, 58, 95, 79, 16, 16, 30, 66, 130, 130, 30, 48, 104, 70, 3, 100, 100, 3, 107, 34, 11, 54, 107, 123, 92, 102, 123, 90, 102, 43, 90, 6, 63, 148, 4, 102, 63, 90, 48, 120, 102, 30, 58, 48, 127, 6, 63, 102, 87, 43, 123, 92, 63, 82, 34, 107, 123, 92, 82, 43, 3, 107, 123, 92, 11, 43, 92, 35, 123, 92, 63, 24, 123, 92, 63, 43, 3, 92, 63, 3, 92, 87, 3, 92, 63, 24, 123, 92, 80, 92, 80, 63, 107, 24, 107, 1, 88, 111, 65, 107, 91], [39, 147, 121, 62, 39, 144, 31, 26, 55, 25, 84, 67, 8, 39, 147, 121, 62, 19, 39, 144, 31, 121, 26, 55, 26, 70, 55, 56, 56, 70, 55, 56, 70, 114, 25], [119, 28, 121, 62, 31, 60, 55, 84, 119, 9, 9, 119, 28, 67, 19, 31, 121, 60, 8, 69, 67, 110, 55, 40, 26, 112, 70, 55, 56, 119, 55, 40, 70, 119, 55, 31, 121, 8, 69, 67, 110, 31, 56, 55, 25, 31, 119, 83, 119], [39, 147, 121, 31, 136, 62, 26, 25, 84, 67, 39, 147, 121, 67, 70, 67, 31, 136, 78, 62, 70, 26, 55, 26, 70, 55, 13, 62, 70, 55, 31, 56, 114, 25], [39, 147, 121, 31, 55, 26, 25, 84, 22, 39, 147, 121, 141, 142, 147, 62, 147, 8, 142, 31, 78, 70, 13, 26, 60, 31, 55, 26, 60, 56, 25, 22, 33, 22, 33, 22], [121, 39, 40, 121, 55, 84, 8, 121, 39, 147, 69, 39, 70, 8, 8, 39, 144, 121, 69, 78, 39, 144, 62, 70, 83, 40, 55, 40, 55, 112, 62, 70, 55, 56, 25, 84, 55, 40, 62, 70, 25, 84, 55, 40, 62, 70], [26, 60, 31, 19, 31, 55, 25, 84, 67, 26, 31, 19, 26, 70, 67, 31, 136, 78, 31, 31, 19, 55, 13, 26, 70, 31, 70, 56, 114, 25], [39, 147, 121, 62, 55, 26, 84, 67, 147, 121, 136, 62, 70, 55, 26, 13, 112, 62, 70, 55, 62, 114, 25], [119, 28, 121, 62, 31, 60, 55, 84, 119, 9, 9, 119, 28, 67, 19, 31, 121, 60, 8, 69, 67, 110, 55, 40, 26, 112, 70, 55, 56, 119, 55, 40, 70, 119, 55, 31, 121, 8, 69, 67, 110, 31, 56, 55, 25, 31, 119, 83, 119], [17, 17, 26, 84, 26, 26, 28, 121, 26, 31, 28, 121, 26, 25, 84, 22, 17, 7, 26, 7, 17, 26, 26, 60, 69, 110, 62, 28, 121, 60, 8, 67, 69, 67, 110, 31, 28, 121, 60, 31, 70, 70, 17, 17, 56, 17, 26, 70, 17, 70, 26, 17, 56, 17, 26, 70, 56, 112, 17, 56, 17, 83, 26, 70, 112, 17, 25, 17, 33, 22, 33, 22], [136, 139, 26, 141, 31, 19, 121, 31, 39, 142, 25, 84, 112, 139, 13, 26, 141, 139, 26, 8, 31, 19, 121, 69, 31, 39, 110, 7, 67, 26, 141, 7, 139, 139, 26, 7, 142, 25, 84]]\n"
     ]
    }
   ],
   "source": [
    "text_data = text_to_numbers(preprocessed_texts, word_dictionary)\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135, 90, 123, 92, 96, 116, 51, 97, 31]\n"
     ]
    }
   ],
   "source": [
    "valid_words = ['tuna', 'rice', 'sushi', 'roll', 'sashimi','steak','grill', 'sauce', 'cream']\n",
    "\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]\n",
    "print(valid_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['oil' '13']\n",
      " ['oil' '13']]\n",
      "['ginger' 'steak']\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "batch_data, label_data = create_batch_data(preprocessed_texts)\n",
    "print(batch_data)\n",
    "print(label_data)\n",
    "print(np.shape(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "WARNING:tensorflow:From <ipython-input-10-82854ef2b125>:30: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Starting Training\n",
      "Loss at step 50 : 5.785186767578125\n",
      "Loss at step 100 : 5.244414329528809\n",
      "Loss at step 150 : 5.202678203582764\n",
      "Loss at step 200 : 4.671710014343262\n",
      "Loss at step 250 : 4.7154998779296875\n",
      "Loss at step 300 : 4.962398529052734\n",
      "Loss at step 350 : 4.250115871429443\n",
      "Loss at step 400 : 5.452340602874756\n",
      "Loss at step 450 : 4.718405246734619\n",
      "Loss at step 500 : 4.806320667266846\n",
      "Loss at step 550 : 4.470209121704102\n",
      "Loss at step 600 : 4.7410712242126465\n",
      "Loss at step 650 : 5.1305036544799805\n",
      "Loss at step 700 : 4.800546169281006\n",
      "Loss at step 750 : 4.510857582092285\n",
      "Loss at step 800 : 4.665602207183838\n",
      "Loss at step 850 : 4.712397575378418\n",
      "Loss at step 900 : 4.3110198974609375\n",
      "Loss at step 950 : 5.345371246337891\n",
      "Loss at step 1000 : 4.690176010131836\n",
      "Loss at step 1050 : 5.059058666229248\n",
      "Loss at step 1100 : 4.370900630950928\n",
      "Loss at step 1150 : 4.13517951965332\n",
      "Loss at step 1200 : 5.246269226074219\n",
      "Loss at step 1250 : 4.778396129608154\n",
      "Loss at step 1300 : 3.5612359046936035\n",
      "Loss at step 1350 : 4.761995792388916\n",
      "Loss at step 1400 : 4.687037467956543\n",
      "Loss at step 1450 : 4.502780914306641\n",
      "Loss at step 1500 : 5.005115509033203\n",
      "Loss at step 1550 : 4.509173393249512\n",
      "Loss at step 1600 : 4.546994209289551\n",
      "Loss at step 1650 : 5.016635417938232\n",
      "Loss at step 1700 : 5.04274845123291\n",
      "Loss at step 1750 : 4.209221839904785\n",
      "Loss at step 1800 : 4.298395156860352\n",
      "Loss at step 1850 : 3.885075569152832\n",
      "Loss at step 1900 : 4.712485313415527\n",
      "Loss at step 1950 : 4.677505970001221\n",
      "Loss at step 2000 : 3.1352624893188477\n",
      "Loss at step 2050 : 4.171247482299805\n",
      "Loss at step 2100 : 4.402332305908203\n",
      "Loss at step 2150 : 5.315061569213867\n",
      "Loss at step 2200 : 4.514245986938477\n",
      "Loss at step 2250 : 3.752328634262085\n",
      "Loss at step 2300 : 4.709686279296875\n",
      "Loss at step 2350 : 4.319090843200684\n",
      "Loss at step 2400 : 4.578157424926758\n",
      "Loss at step 2450 : 4.131357192993164\n",
      "Loss at step 2500 : 4.93834114074707\n",
      "Loss at step 2550 : 5.327744483947754\n",
      "Loss at step 2600 : 3.4703712463378906\n",
      "Loss at step 2650 : 4.0294060707092285\n",
      "Loss at step 2700 : 3.2748525142669678\n",
      "Loss at step 2750 : 4.771474838256836\n",
      "Loss at step 2800 : 3.5377564430236816\n",
      "Loss at step 2850 : 3.247044324874878\n",
      "Loss at step 2900 : 4.360203266143799\n",
      "Loss at step 2950 : 2.9483110904693604\n",
      "Loss at step 3000 : 4.798584938049316\n",
      "Loss at step 3050 : 4.249152183532715\n",
      "Loss at step 3100 : 3.4030685424804688\n",
      "Loss at step 3150 : 4.064779758453369\n",
      "Loss at step 3200 : 2.6604936122894287\n",
      "Loss at step 3250 : 6.114832878112793\n",
      "Loss at step 3300 : 4.883185386657715\n",
      "Loss at step 3350 : 4.126192569732666\n",
      "Loss at step 3400 : 2.8151302337646484\n",
      "Loss at step 3450 : 3.729207992553711\n",
      "Loss at step 3500 : 4.461637496948242\n",
      "Loss at step 3550 : 3.7627227306365967\n",
      "Loss at step 3600 : 4.428886413574219\n",
      "Loss at step 3650 : 2.108380079269409\n",
      "Loss at step 3700 : 2.433124542236328\n",
      "Loss at step 3750 : 1.8737947940826416\n",
      "Loss at step 3800 : 3.8771533966064453\n",
      "Loss at step 3850 : 3.2296345233917236\n",
      "Loss at step 3900 : 4.454775333404541\n",
      "Loss at step 3950 : 3.756258726119995\n",
      "Loss at step 4000 : 4.72408390045166\n",
      "Loss at step 4050 : 2.922031879425049\n",
      "Loss at step 4100 : 5.495339870452881\n",
      "Loss at step 4150 : 3.4082939624786377\n",
      "Loss at step 4200 : 3.8455147743225098\n",
      "Loss at step 4250 : 2.5424296855926514\n",
      "Loss at step 4300 : 3.435157537460327\n",
      "Loss at step 4350 : 3.9292776584625244\n",
      "Loss at step 4400 : 4.255851745605469\n",
      "Loss at step 4450 : 3.1140642166137695\n",
      "Loss at step 4500 : 2.697803020477295\n",
      "Loss at step 4550 : 4.492975234985352\n",
      "Loss at step 4600 : 4.790773868560791\n",
      "Loss at step 4650 : 5.120169639587402\n",
      "Loss at step 4700 : 4.362067222595215\n",
      "Loss at step 4750 : 4.08140754699707\n",
      "Loss at step 4800 : 3.8535332679748535\n",
      "Loss at step 4850 : 3.5337212085723877\n",
      "Loss at step 4900 : 4.286477565765381\n",
      "Loss at step 4950 : 3.4668335914611816\n",
      "Loss at step 5000 : 6.002950668334961\n",
      "Nearest to tuna: coriander, tobikko, wasabi, whisk, butter,\n",
      "Nearest to rice: drain, soy, mayonnaise, chive, stick,\n",
      "Nearest to sushi: carrot, salmon, onion, water, raspberry,\n",
      "Nearest to roll: liqueur, dipping, mat, mayonnaise, thickness,\n",
      "Nearest to sashimi: truffle, chicken, strawberry, white, roe,\n",
      "Nearest to steak: pour, mixture, pepper, white, preheat,\n",
      "Nearest to grill: ginger, water, mixture, onion, sauce,\n",
      "Nearest to sauce: grill, oil, torch, vinegar, peak,\n",
      "Nearest to cream: position, skewer, bottom, batter, mixture,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 5050 : 1.8578195571899414\n",
      "Loss at step 5100 : 3.5653316974639893\n",
      "Loss at step 5150 : 3.3020873069763184\n",
      "Loss at step 5200 : 4.419093608856201\n",
      "Loss at step 5250 : 3.2118382453918457\n",
      "Loss at step 5300 : 3.050539016723633\n",
      "Loss at step 5350 : 3.817992925643921\n",
      "Loss at step 5400 : 3.022576332092285\n",
      "Loss at step 5450 : 3.0533924102783203\n",
      "Loss at step 5500 : 3.2446651458740234\n",
      "Loss at step 5550 : 1.5909994840621948\n",
      "Loss at step 5600 : 5.025744438171387\n",
      "Loss at step 5650 : 2.9459614753723145\n",
      "Loss at step 5700 : 3.962562322616577\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-82854ef2b125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#run the train step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#return the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name=\"word_embeddings\")\n",
    "doc_embeddings = tf.Variable(tf.random_uniform([len(preprocessed_texts), doc_embedding_size], -1.0, 1.0), name=\"doc_embeddings\")\n",
    "\n",
    "decoder_weights = tf.Variable(tf.truncated_normal([vocabulary_size, concatenated_size],\n",
    "                                               stddev=1.0 / np.sqrt(concatenated_size)),\n",
    "                                               name=\"decoder_weights\")\n",
    "decoder_biases = tf.Variable(tf.zeros([vocabulary_size]), name=\"decoder_biases\")\n",
    "\n",
    "\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[None, 2]) #1 for word index and 1 for doc index\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embed= tf.nn.embedding_lookup(embeddings, x_inputs[:, 0])\n",
    "    \n",
    "doc_indices = tf.slice(x_inputs, [0,1],[batch_size,1])\n",
    "doc_embed = tf.nn.embedding_lookup(doc_embeddings,doc_indices)\n",
    "final_embed = tf.concat([embed, tf.squeeze(doc_embed)],1)\n",
    "\n",
    "logits = tf.matmul(final_embed, tf.transpose(decoder_weights)) + decoder_biases\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_target))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=model_learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "#cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True, name=\"cosine_similarity\")\n",
    "\n",
    "with tf.name_scope(\"performance\"):\n",
    "    loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "    loss_summary = tf.summary.scalar('loss', loss_ph)\n",
    "performance_summaries = tf.summary.merge([loss_summary])\n",
    "\n",
    "saver = tf.train.Saver({\"embeddings\": embeddings, \"doc_embeddings\": doc_embeddings})\n",
    "summ_writer = tf.summary.FileWriter(summaries_folder_name, sess.graph)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "print('Starting Training')\n",
    "\n",
    "#loss_vec = []\n",
    "#loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = create_batch_data(text_data)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    #run the train step\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    #return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        summ = sess.run(performance_summaries, feed_dict={loss_ph:loss_val})\n",
    "        summ_writer.add_summary(summ, i+1)\n",
    "        #loss_vec.append(loss_val)\n",
    "        #loss_x_vec.append(i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "        \n",
    "    #validation\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    #save dictionary + embeddings\n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        #save vocabulary dictionary\n",
    "        with open(os.path.join(models_folder_name,'doc2vec_recipes_dict_words_integers.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        #save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),models_folder_name,'doc2vec_recipes_checkpoint.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))\n",
    "        \n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
